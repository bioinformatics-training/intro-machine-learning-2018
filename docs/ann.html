<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>An Introduction to Machine Learning</title>
  <meta name="description" content="Course materials for An Introduction to Machine Learning">
  <meta name="generator" content="bookdown 0.7 and GitBook 2.6.7">

  <meta property="og:title" content="An Introduction to Machine Learning" />
  <meta property="og:type" content="book" />
  
  <meta property="og:image" content="figures/cover_image.png" />
  <meta property="og:description" content="Course materials for An Introduction to Machine Learning" />
  <meta name="github-repo" content="bioinformatics-training/intro-machine-learning-2018" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="An Introduction to Machine Learning" />
  
  <meta name="twitter:description" content="Course materials for An Introduction to Machine Learning" />
  <meta name="twitter:image" content="figures/cover_image.png" />

<meta name="author" content="Sudhakaran Prabakaran, Matt Wayland and Chris Penfold">


<meta name="date" content="2018-09-25">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="decision-trees.html">
<link rel="next" href="mlnn.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">An Introduction to Machine Learning</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> About the course</a><ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#overview"><i class="fa fa-check"></i><b>1.1</b> Overview</a></li>
<li class="chapter" data-level="1.2" data-path="index.html"><a href="index.html#registration"><i class="fa fa-check"></i><b>1.2</b> Registration</a></li>
<li class="chapter" data-level="1.3" data-path="index.html"><a href="index.html#prerequisites"><i class="fa fa-check"></i><b>1.3</b> Prerequisites</a></li>
<li class="chapter" data-level="1.4" data-path="index.html"><a href="index.html#github"><i class="fa fa-check"></i><b>1.4</b> Github</a></li>
<li class="chapter" data-level="1.5" data-path="index.html"><a href="index.html#license"><i class="fa fa-check"></i><b>1.5</b> License</a></li>
<li class="chapter" data-level="1.6" data-path="index.html"><a href="index.html#contact"><i class="fa fa-check"></i><b>1.6</b> Contact</a></li>
<li class="chapter" data-level="1.7" data-path="index.html"><a href="index.html#colophon"><i class="fa fa-check"></i><b>1.7</b> Colophon</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>2</b> Introduction</a><ul>
<li class="chapter" data-level="2.1" data-path="intro.html"><a href="intro.html#what-is-machine-learning"><i class="fa fa-check"></i><b>2.1</b> What is machine learning?</a></li>
<li class="chapter" data-level="2.2" data-path="intro.html"><a href="intro.html#aspects-of-ml"><i class="fa fa-check"></i><b>2.2</b> Aspects of ML</a></li>
<li class="chapter" data-level="2.3" data-path="intro.html"><a href="intro.html#what-actually-happened-under-the-hood"><i class="fa fa-check"></i><b>2.3</b> What actually happened under the hood</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="linear-models.html"><a href="linear-models.html"><i class="fa fa-check"></i><b>3</b> Linear models and matrix algebra</a><ul>
<li class="chapter" data-level="3.1" data-path="linear-models.html"><a href="linear-models.html#linear-models"><i class="fa fa-check"></i><b>3.1</b> Linear models</a></li>
<li class="chapter" data-level="3.2" data-path="linear-models.html"><a href="linear-models.html#matrix-algebra"><i class="fa fa-check"></i><b>3.2</b> Matrix algebra</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="logistic-regression.html"><a href="logistic-regression.html"><i class="fa fa-check"></i><b>4</b> Supervised learning</a><ul>
<li class="chapter" data-level="4.1" data-path="logistic-regression.html"><a href="logistic-regression.html#regression"><i class="fa fa-check"></i><b>4.1</b> Regression</a><ul>
<li class="chapter" data-level="4.1.1" data-path="logistic-regression.html"><a href="logistic-regression.html#linear-regression"><i class="fa fa-check"></i><b>4.1.1</b> Linear regression</a></li>
<li class="chapter" data-level="4.1.2" data-path="logistic-regression.html"><a href="logistic-regression.html#polynomial-regression"><i class="fa fa-check"></i><b>4.1.2</b> Polynomial regression</a></li>
<li class="chapter" data-level="4.1.3" data-path="logistic-regression.html"><a href="logistic-regression.html#distributions-of-fits"><i class="fa fa-check"></i><b>4.1.3</b> Distributions of fits</a></li>
<li class="chapter" data-level="4.1.4" data-path="logistic-regression.html"><a href="logistic-regression.html#gaussian-process-regression"><i class="fa fa-check"></i><b>4.1.4</b> Gaussian process regression</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="logistic-regression.html"><a href="logistic-regression.html#classification"><i class="fa fa-check"></i><b>4.2</b> Classification</a><ul>
<li class="chapter" data-level="4.2.1" data-path="logistic-regression.html"><a href="logistic-regression.html#logistic-regression"><i class="fa fa-check"></i><b>4.2.1</b> Logistic regression</a></li>
<li class="chapter" data-level="4.2.2" data-path="logistic-regression.html"><a href="logistic-regression.html#gp-classification"><i class="fa fa-check"></i><b>4.2.2</b> GP classification</a></li>
<li class="chapter" data-level="4.2.3" data-path="logistic-regression.html"><a href="logistic-regression.html#other-classification-approaches."><i class="fa fa-check"></i><b>4.2.3</b> Other classification approaches.</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="logistic-regression.html"><a href="logistic-regression.html#resources"><i class="fa fa-check"></i><b>4.3</b> Resources</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="dimensionality-reduction.html"><a href="dimensionality-reduction.html"><i class="fa fa-check"></i><b>5</b> Dimensionality reduction</a><ul>
<li class="chapter" data-level="5.1" data-path="dimensionality-reduction.html"><a href="dimensionality-reduction.html#linear-dimensionality-reduction"><i class="fa fa-check"></i><b>5.1</b> Linear Dimensionality Reduction</a><ul>
<li class="chapter" data-level="5.1.1" data-path="dimensionality-reduction.html"><a href="dimensionality-reduction.html#interpreting-the-principle-component-axes"><i class="fa fa-check"></i><b>5.1.1</b> Interpreting the Principle Component Axes</a></li>
<li class="chapter" data-level="5.1.2" data-path="dimensionality-reduction.html"><a href="dimensionality-reduction.html#horseshoe-effect"><i class="fa fa-check"></i><b>5.1.2</b> Horseshoe effect</a></li>
<li class="chapter" data-level="5.1.3" data-path="dimensionality-reduction.html"><a href="dimensionality-reduction.html#pca-analysis-of-mammalian-development"><i class="fa fa-check"></i><b>5.1.3</b> PCA analysis of mammalian development</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="dimensionality-reduction.html"><a href="dimensionality-reduction.html#nonlinear-dimensionality-reduction"><i class="fa fa-check"></i><b>5.2</b> Nonlinear Dimensionality Reduction</a><ul>
<li class="chapter" data-level="5.2.1" data-path="dimensionality-reduction.html"><a href="dimensionality-reduction.html#nonlinear-warping"><i class="fa fa-check"></i><b>5.2.1</b> Nonlinear warping</a></li>
<li class="chapter" data-level="5.2.2" data-path="dimensionality-reduction.html"><a href="dimensionality-reduction.html#stochasticity"><i class="fa fa-check"></i><b>5.2.2</b> Stochasticity</a></li>
<li class="chapter" data-level="5.2.3" data-path="dimensionality-reduction.html"><a href="dimensionality-reduction.html#analysis-of-mammalian-development"><i class="fa fa-check"></i><b>5.2.3</b> Analysis of mammalian development</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="dimensionality-reduction.html"><a href="dimensionality-reduction.html#other-dimensionality-reduction-techniques"><i class="fa fa-check"></i><b>5.3</b> Other dimensionality reduction techniques</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="clustering.html"><a href="clustering.html"><i class="fa fa-check"></i><b>6</b> Clustering</a><ul>
<li class="chapter" data-level="6.1" data-path="clustering.html"><a href="clustering.html#introduction"><i class="fa fa-check"></i><b>6.1</b> Introduction</a></li>
<li class="chapter" data-level="6.2" data-path="clustering.html"><a href="clustering.html#distance-metrics"><i class="fa fa-check"></i><b>6.2</b> Distance metrics</a></li>
<li class="chapter" data-level="6.3" data-path="clustering.html"><a href="clustering.html#hierarchic-agglomerative"><i class="fa fa-check"></i><b>6.3</b> Hierarchic agglomerative</a><ul>
<li class="chapter" data-level="6.3.1" data-path="clustering.html"><a href="clustering.html#linkage-algorithms"><i class="fa fa-check"></i><b>6.3.1</b> Linkage algorithms</a></li>
<li class="chapter" data-level="6.3.2" data-path="clustering.html"><a href="clustering.html#example-clustering-synthetic-data-sets"><i class="fa fa-check"></i><b>6.3.2</b> Example: clustering synthetic data sets</a></li>
<li class="chapter" data-level="6.3.3" data-path="clustering.html"><a href="clustering.html#example-gene-expression-profiling-of-human-tissues"><i class="fa fa-check"></i><b>6.3.3</b> Example: gene expression profiling of human tissues</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="clustering.html"><a href="clustering.html#k-means"><i class="fa fa-check"></i><b>6.4</b> K-means</a><ul>
<li class="chapter" data-level="6.4.1" data-path="clustering.html"><a href="clustering.html#algorithm"><i class="fa fa-check"></i><b>6.4.1</b> Algorithm</a></li>
<li class="chapter" data-level="6.4.2" data-path="clustering.html"><a href="clustering.html#choosing-initial-cluster-centres"><i class="fa fa-check"></i><b>6.4.2</b> Choosing initial cluster centres</a></li>
<li class="chapter" data-level="6.4.3" data-path="clustering.html"><a href="clustering.html#choosingK"><i class="fa fa-check"></i><b>6.4.3</b> Choosing k</a></li>
<li class="chapter" data-level="6.4.4" data-path="clustering.html"><a href="clustering.html#example-clustering-synthetic-data-sets-1"><i class="fa fa-check"></i><b>6.4.4</b> Example: clustering synthetic data sets</a></li>
<li class="chapter" data-level="6.4.5" data-path="clustering.html"><a href="clustering.html#example-gene-expression-profiling-of-human-tissues-1"><i class="fa fa-check"></i><b>6.4.5</b> Example: gene expression profiling of human tissues</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="clustering.html"><a href="clustering.html#dbscan"><i class="fa fa-check"></i><b>6.5</b> DBSCAN</a><ul>
<li class="chapter" data-level="6.5.1" data-path="clustering.html"><a href="clustering.html#algorithm-1"><i class="fa fa-check"></i><b>6.5.1</b> Algorithm</a></li>
<li class="chapter" data-level="6.5.2" data-path="clustering.html"><a href="clustering.html#implementation-in-r"><i class="fa fa-check"></i><b>6.5.2</b> Implementation in R</a></li>
<li class="chapter" data-level="6.5.3" data-path="clustering.html"><a href="clustering.html#choosing-parameters"><i class="fa fa-check"></i><b>6.5.3</b> Choosing parameters</a></li>
<li class="chapter" data-level="6.5.4" data-path="clustering.html"><a href="clustering.html#example-clustering-synthetic-data-sets-2"><i class="fa fa-check"></i><b>6.5.4</b> Example: clustering synthetic data sets</a></li>
<li class="chapter" data-level="6.5.5" data-path="clustering.html"><a href="clustering.html#example-gene-expression-profiling-of-human-tissues-2"><i class="fa fa-check"></i><b>6.5.5</b> Example: gene expression profiling of human tissues</a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="clustering.html"><a href="clustering.html#evaluating-cluster-quality"><i class="fa fa-check"></i><b>6.6</b> Evaluating cluster quality</a><ul>
<li class="chapter" data-level="6.6.1" data-path="clustering.html"><a href="clustering.html#silhouetteMethod"><i class="fa fa-check"></i><b>6.6.1</b> Silhouette method</a></li>
<li class="chapter" data-level="6.6.2" data-path="clustering.html"><a href="clustering.html#example---k-means-clustering-of-blobs-data-set"><i class="fa fa-check"></i><b>6.6.2</b> Example - k-means clustering of blobs data set</a></li>
<li class="chapter" data-level="6.6.3" data-path="clustering.html"><a href="clustering.html#example---dbscan-clustering-of-noisy-moons"><i class="fa fa-check"></i><b>6.6.3</b> Example - DBSCAN clustering of noisy moons</a></li>
</ul></li>
<li class="chapter" data-level="6.7" data-path="clustering.html"><a href="clustering.html#exercises"><i class="fa fa-check"></i><b>6.7</b> Exercises</a><ul>
<li class="chapter" data-level="6.7.1" data-path="clustering.html"><a href="clustering.html#clusteringEx1"><i class="fa fa-check"></i><b>6.7.1</b> Exercise 1</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="nearest-neighbours.html"><a href="nearest-neighbours.html"><i class="fa fa-check"></i><b>7</b> Nearest neighbours</a><ul>
<li class="chapter" data-level="7.1" data-path="nearest-neighbours.html"><a href="nearest-neighbours.html#introduction-1"><i class="fa fa-check"></i><b>7.1</b> Introduction</a></li>
<li class="chapter" data-level="7.2" data-path="nearest-neighbours.html"><a href="nearest-neighbours.html#classification-simulated-data"><i class="fa fa-check"></i><b>7.2</b> Classification: simulated data</a><ul>
<li class="chapter" data-level="7.2.1" data-path="nearest-neighbours.html"><a href="nearest-neighbours.html#knn-function"><i class="fa fa-check"></i><b>7.2.1</b> knn function</a></li>
<li class="chapter" data-level="7.2.2" data-path="nearest-neighbours.html"><a href="nearest-neighbours.html#plotting-decision-boundaries"><i class="fa fa-check"></i><b>7.2.2</b> Plotting decision boundaries</a></li>
<li class="chapter" data-level="7.2.3" data-path="nearest-neighbours.html"><a href="nearest-neighbours.html#bias-variance-tradeoff"><i class="fa fa-check"></i><b>7.2.3</b> Bias-variance tradeoff</a></li>
<li class="chapter" data-level="7.2.4" data-path="nearest-neighbours.html"><a href="nearest-neighbours.html#choosing-k"><i class="fa fa-check"></i><b>7.2.4</b> Choosing <em>k</em></a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="nearest-neighbours.html"><a href="nearest-neighbours.html#knn-cell-segmentation"><i class="fa fa-check"></i><b>7.3</b> Classification: cell segmentation</a><ul>
<li class="chapter" data-level="7.3.1" data-path="nearest-neighbours.html"><a href="nearest-neighbours.html#cell-segmentation-data-set"><i class="fa fa-check"></i><b>7.3.1</b> Cell segmentation data set</a></li>
<li class="chapter" data-level="7.3.2" data-path="nearest-neighbours.html"><a href="nearest-neighbours.html#data-splitting"><i class="fa fa-check"></i><b>7.3.2</b> Data splitting</a></li>
<li class="chapter" data-level="7.3.3" data-path="nearest-neighbours.html"><a href="nearest-neighbours.html#identification-of-data-quality-issues"><i class="fa fa-check"></i><b>7.3.3</b> Identification of data quality issues</a></li>
<li class="chapter" data-level="7.3.4" data-path="nearest-neighbours.html"><a href="nearest-neighbours.html#fit-model"><i class="fa fa-check"></i><b>7.3.4</b> Fit model</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="nearest-neighbours.html"><a href="nearest-neighbours.html#knn-regression"><i class="fa fa-check"></i><b>7.4</b> Regression</a><ul>
<li class="chapter" data-level="7.4.1" data-path="nearest-neighbours.html"><a href="nearest-neighbours.html#partition-data"><i class="fa fa-check"></i><b>7.4.1</b> Partition data</a></li>
<li class="chapter" data-level="7.4.2" data-path="nearest-neighbours.html"><a href="nearest-neighbours.html#data-pre-processing"><i class="fa fa-check"></i><b>7.4.2</b> Data pre-processing</a></li>
<li class="chapter" data-level="7.4.3" data-path="nearest-neighbours.html"><a href="nearest-neighbours.html#search-for-optimum-k"><i class="fa fa-check"></i><b>7.4.3</b> Search for optimum <em>k</em></a></li>
<li class="chapter" data-level="7.4.4" data-path="nearest-neighbours.html"><a href="nearest-neighbours.html#use-model-to-make-predictions"><i class="fa fa-check"></i><b>7.4.4</b> Use model to make predictions</a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="nearest-neighbours.html"><a href="nearest-neighbours.html#exercises-1"><i class="fa fa-check"></i><b>7.5</b> Exercises</a><ul>
<li class="chapter" data-level="7.5.1" data-path="nearest-neighbours.html"><a href="nearest-neighbours.html#knnEx1"><i class="fa fa-check"></i><b>7.5.1</b> Exercise 1</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="svm.html"><a href="svm.html"><i class="fa fa-check"></i><b>8</b> Support vector machines</a><ul>
<li class="chapter" data-level="8.1" data-path="svm.html"><a href="svm.html#introduction-2"><i class="fa fa-check"></i><b>8.1</b> Introduction</a><ul>
<li class="chapter" data-level="8.1.1" data-path="svm.html"><a href="svm.html#maximum-margin-classifier"><i class="fa fa-check"></i><b>8.1.1</b> Maximum margin classifier</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="svm.html"><a href="svm.html#support-vector-classifier"><i class="fa fa-check"></i><b>8.2</b> Support vector classifier</a></li>
<li class="chapter" data-level="8.3" data-path="svm.html"><a href="svm.html#support-vector-machine"><i class="fa fa-check"></i><b>8.3</b> Support Vector Machine</a></li>
<li class="chapter" data-level="8.4" data-path="svm.html"><a href="svm.html#example---training-a-classifier"><i class="fa fa-check"></i><b>8.4</b> Example - training a classifier</a><ul>
<li class="chapter" data-level="8.4.1" data-path="svm.html"><a href="svm.html#setup-environment"><i class="fa fa-check"></i><b>8.4.1</b> Setup environment</a></li>
<li class="chapter" data-level="8.4.2" data-path="svm.html"><a href="svm.html#partition-data-1"><i class="fa fa-check"></i><b>8.4.2</b> Partition data</a></li>
<li class="chapter" data-level="8.4.3" data-path="svm.html"><a href="svm.html#visualize-training-data"><i class="fa fa-check"></i><b>8.4.3</b> Visualize training data</a></li>
<li class="chapter" data-level="8.4.4" data-path="svm.html"><a href="svm.html#define-a-custom-model"><i class="fa fa-check"></i><b>8.4.4</b> Define a custom model</a></li>
<li class="chapter" data-level="8.4.5" data-path="svm.html"><a href="svm.html#model-cross-validation-and-tuning"><i class="fa fa-check"></i><b>8.4.5</b> Model cross-validation and tuning</a></li>
<li class="chapter" data-level="8.4.6" data-path="svm.html"><a href="svm.html#prediction-performance-measures"><i class="fa fa-check"></i><b>8.4.6</b> Prediction performance measures</a></li>
<li class="chapter" data-level="8.4.7" data-path="svm.html"><a href="svm.html#plot-decision-boundary"><i class="fa fa-check"></i><b>8.4.7</b> Plot decision boundary</a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="svm.html"><a href="svm.html#example---regression"><i class="fa fa-check"></i><b>8.5</b> Example - regression</a></li>
<li class="chapter" data-level="8.6" data-path="svm.html"><a href="svm.html#further-reading"><i class="fa fa-check"></i><b>8.6</b> Further reading</a></li>
<li class="chapter" data-level="8.7" data-path="svm.html"><a href="svm.html#exercises-2"><i class="fa fa-check"></i><b>8.7</b> Exercises</a><ul>
<li class="chapter" data-level="8.7.1" data-path="svm.html"><a href="svm.html#exercise-1"><i class="fa fa-check"></i><b>8.7.1</b> Exercise 1</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="decision-trees.html"><a href="decision-trees.html"><i class="fa fa-check"></i><b>9</b> Decision trees and random forests</a><ul>
<li class="chapter" data-level="9.1" data-path="decision-trees.html"><a href="decision-trees.html#decision-trees"><i class="fa fa-check"></i><b>9.1</b> Decision Trees</a></li>
<li class="chapter" data-level="9.2" data-path="decision-trees.html"><a href="decision-trees.html#random-forest"><i class="fa fa-check"></i><b>9.2</b> Random Forest</a></li>
<li class="chapter" data-level="9.3" data-path="decision-trees.html"><a href="decision-trees.html#exercises-3"><i class="fa fa-check"></i><b>9.3</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="ann.html"><a href="ann.html"><i class="fa fa-check"></i><b>10</b> Artificial neural networks</a><ul>
<li class="chapter" data-level="10.1" data-path="ann.html"><a href="ann.html#neural-networks"><i class="fa fa-check"></i><b>10.1</b> Neural Networks</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="mlnn.html"><a href="mlnn.html"><i class="fa fa-check"></i><b>11</b> Deep Learning</a><ul>
<li class="chapter" data-level="11.1" data-path="mlnn.html"><a href="mlnn.html#multilayer-neural-networks"><i class="fa fa-check"></i><b>11.1</b> Multilayer Neural Networks</a><ul>
<li class="chapter" data-level="11.1.1" data-path="mlnn.html"><a href="mlnn.html#constructing-layers-in-kerasr"><i class="fa fa-check"></i><b>11.1.1</b> Constructing layers in kerasR</a></li>
<li class="chapter" data-level="11.1.2" data-path="mlnn.html"><a href="mlnn.html#reading-in-images"><i class="fa fa-check"></i><b>11.1.2</b> Reading in images</a></li>
<li class="chapter" data-level="11.1.3" data-path="mlnn.html"><a href="mlnn.html#rick-and-morty-classifier-using-deep-learning"><i class="fa fa-check"></i><b>11.1.3</b> Rick and Morty classifier using Deep Learning</a></li>
</ul></li>
<li class="chapter" data-level="11.2" data-path="mlnn.html"><a href="mlnn.html#convolutional-neural-networks"><i class="fa fa-check"></i><b>11.2</b> Convolutional neural networks</a><ul>
<li class="chapter" data-level="11.2.1" data-path="mlnn.html"><a href="mlnn.html#data-augmentation"><i class="fa fa-check"></i><b>11.2.1</b> Data augmentation</a></li>
<li class="chapter" data-level="11.2.2" data-path="mlnn.html"><a href="mlnn.html#asking-more-precise-questions"><i class="fa fa-check"></i><b>11.2.2</b> Asking more precise questions</a></li>
<li class="chapter" data-level="11.2.3" data-path="mlnn.html"><a href="mlnn.html#more-complex-networks"><i class="fa fa-check"></i><b>11.2.3</b> More complex networks</a></li>
</ul></li>
<li class="chapter" data-level="11.3" data-path="mlnn.html"><a href="mlnn.html#further-reading-1"><i class="fa fa-check"></i><b>11.3</b> Further reading</a></li>
</ul></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path="logistic-regression.html"><a href="logistic-regression.html#resources"><i class="fa fa-check"></i><b>A</b> Resources</a><ul>
<li class="chapter" data-level="A.1" data-path="resources.html"><a href="resources.html"><i class="fa fa-check"></i><b>A.1</b> Python</a></li>
<li class="chapter" data-level="A.2" data-path="resources.html"><a href="resources.html#machine-learning-data-set-repositories"><i class="fa fa-check"></i><b>A.2</b> Machine learning data set repositories</a><ul>
<li class="chapter" data-level="A.2.1" data-path="resources.html"><a href="resources.html#mldata"><i class="fa fa-check"></i><b>A.2.1</b> MLDATA</a></li>
<li class="chapter" data-level="A.2.2" data-path="resources.html"><a href="resources.html#uci-machine-learning-repository"><i class="fa fa-check"></i><b>A.2.2</b> UCI Machine Learning Repository</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="B" data-path="solutions-linear-models.html"><a href="solutions-linear-models.html"><i class="fa fa-check"></i><b>B</b> Solutions ch. 3 - Linear models and matrix algebra</a><ul>
<li class="chapter" data-level="B.1" data-path="solutions-linear-models.html"><a href="solutions-linear-models.html#example-2"><i class="fa fa-check"></i><b>B.1</b> Example 2</a></li>
<li class="chapter" data-level="B.2" data-path="solutions-linear-models.html"><a href="solutions-linear-models.html#example-2-1"><i class="fa fa-check"></i><b>B.2</b> Example 2</a></li>
</ul></li>
<li class="chapter" data-level="C" data-path="solutions-logistic-regression.html"><a href="solutions-logistic-regression.html"><i class="fa fa-check"></i><b>C</b> Solutions ch. 4 - Linear and non-linear (logistic) regression</a></li>
<li class="chapter" data-level="D" data-path="solutions-dimensionality-reduction.html"><a href="solutions-dimensionality-reduction.html"><i class="fa fa-check"></i><b>D</b> Solutions ch. 5 - Dimensionality reduction</a><ul>
<li class="chapter" data-level="D.1" data-path="solutions-dimensionality-reduction.html"><a href="solutions-dimensionality-reduction.html#exercise-5.1"><i class="fa fa-check"></i><b>D.1</b> Exercise 5.1</a></li>
<li class="chapter" data-level="D.2" data-path="solutions-dimensionality-reduction.html"><a href="solutions-dimensionality-reduction.html#exercise-5.2"><i class="fa fa-check"></i><b>D.2</b> Exercise 5.2</a></li>
<li class="chapter" data-level="D.3" data-path="solutions-dimensionality-reduction.html"><a href="solutions-dimensionality-reduction.html#exercise-5.3."><i class="fa fa-check"></i><b>D.3</b> Exercise 5.3.</a></li>
<li class="chapter" data-level="D.4" data-path="solutions-dimensionality-reduction.html"><a href="solutions-dimensionality-reduction.html#exercise-5.4."><i class="fa fa-check"></i><b>D.4</b> Exercise 5.4.</a></li>
<li class="chapter" data-level="D.5" data-path="solutions-dimensionality-reduction.html"><a href="solutions-dimensionality-reduction.html#exercise-5.5"><i class="fa fa-check"></i><b>D.5</b> Exercise 5.5</a></li>
<li class="chapter" data-level="D.6" data-path="solutions-dimensionality-reduction.html"><a href="solutions-dimensionality-reduction.html#exercise-5.6."><i class="fa fa-check"></i><b>D.6</b> Exercise 5.6.</a></li>
<li class="chapter" data-level="D.7" data-path="solutions-dimensionality-reduction.html"><a href="solutions-dimensionality-reduction.html#exercise-5.7."><i class="fa fa-check"></i><b>D.7</b> Exercise 5.7.</a></li>
<li class="chapter" data-level="D.8" data-path="solutions-dimensionality-reduction.html"><a href="solutions-dimensionality-reduction.html#exercise-5.8."><i class="fa fa-check"></i><b>D.8</b> Exercise 5.8.</a></li>
</ul></li>
<li class="chapter" data-level="E" data-path="solutions-clustering.html"><a href="solutions-clustering.html"><i class="fa fa-check"></i><b>E</b> Solutions ch. 6 - Clustering</a><ul>
<li class="chapter" data-level="E.1" data-path="solutions-clustering.html"><a href="solutions-clustering.html#exercise-1-1"><i class="fa fa-check"></i><b>E.1</b> Exercise 1</a></li>
</ul></li>
<li class="chapter" data-level="F" data-path="solutions-nearest-neighbours.html"><a href="solutions-nearest-neighbours.html"><i class="fa fa-check"></i><b>F</b> Solutions ch. 7 - Nearest neighbours</a><ul>
<li class="chapter" data-level="F.1" data-path="solutions-nearest-neighbours.html"><a href="solutions-nearest-neighbours.html#exercise-1-2"><i class="fa fa-check"></i><b>F.1</b> Exercise 1</a></li>
</ul></li>
<li class="chapter" data-level="G" data-path="solutions-svm.html"><a href="solutions-svm.html"><i class="fa fa-check"></i><b>G</b> Solutions ch. 8 - Support vector machines</a><ul>
<li class="chapter" data-level="G.1" data-path="solutions-svm.html"><a href="solutions-svm.html#exercise-1-3"><i class="fa fa-check"></i><b>G.1</b> Exercise 1</a></li>
</ul></li>
<li class="chapter" data-level="H" data-path="solutions-decision-trees.html"><a href="solutions-decision-trees.html"><i class="fa fa-check"></i><b>H</b> Solutions ch. 9 - Decision trees and random forests</a><ul>
<li class="chapter" data-level="H.1" data-path="solutions-decision-trees.html"><a href="solutions-decision-trees.html#exercise-1-4"><i class="fa fa-check"></i><b>H.1</b> Exercise 1</a></li>
</ul></li>
<li class="chapter" data-level="I" data-path="solutions-ann.html"><a href="solutions-ann.html"><i class="fa fa-check"></i><b>I</b> Solutions ch. 10 - Artificial neural networks</a><ul>
<li class="chapter" data-level="I.1" data-path="solutions-ann.html"><a href="solutions-ann.html#exercise-1-5"><i class="fa fa-check"></i><b>I.1</b> Exercise 1</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">An Introduction to Machine Learning</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="ann" class="section level1">
<h1><span class="header-section-number">10</span> Artificial neural networks</h1>
<!-- Sudhakaran -->
<div id="neural-networks" class="section level2">
<h2><span class="header-section-number">10.1</span> Neural Networks</h2>
<p><strong>What are artificial neural networks (ANNs)?</strong></p>
<p><em>ANN</em> is actually an old idea but it came back into vogue recently and it is the state of the art technique for machine learning. The goal of ANN algorithms is to mimmick the functions of a neuron (Figure <a href="ann.html#fig:neuronalComputation">10.1</a>) and neuronal networks.</p>
<div class="figure" style="text-align: center"><span id="fig:neuronalComputation"></span>
<img src="images/neuronal_computation.png" alt="Neuronal computation" width="65%" />
<p class="caption">
Figure 10.1: Neuronal computation
</p>
</div>
<p>Computational representation of a neuron (Figure <a href="ann.html#fig:perceptron">10.2</a>) aims to mimmick the biological input-and-activation architecture of a neuron (Figure <a href="ann.html#fig:neuronalComputation">10.1</a>). A single unit of a computational neuron is also called a <strong>perceptron or ptrons</strong>. Ptrons have a nonlinear activation function (e.g a logistic function) which determines their output value based upon the values of their inputs.</p>
<div class="figure" style="text-align: center"><span id="fig:perceptron"></span>
<img src="images/Perceptron.png" alt="Perceptron" width="65%" />
<p class="caption">
Figure 10.2: Perceptron
</p>
</div>
<p><strong>Architecture of ANNs</strong></p>
<p>ANNs are built from ptrons. Ptrons have one or more inputs, an activation function and an output (Figure Perceptron). An ANN model is built up by combining ptrons in structured layers. Ptrons in a given layer are independent of each other, but each of them connect to all the ptrons in the next layer (Figure Neural Network Modeling).</p>
<p>The input layer contains a ptron for each input variable. One or more hidden layers contain a user defined number of ptrons. Each ptron in the first hidden layer receives an input from the each ptron in the input layer. If there is a second hidden layer, each ptron in this layer receives an input from each ptron in the first hidden layer, and so on with additional layers. The output layer contains a ptron for each response variable (usually one, sometimes more in multivariate response situations). Each output ptron receives one input from each ptron in the final hidden layer</p>
<p><em>Important</em>: The connections between ptrons are weighted. The magnitude of the weight controls the strength of the influence of that input on the receiving ptron. The sign of the weight controls whether the influence is stimulating or inhibiting the signal to the next layer.</p>
<p>The weights are somewhat analogous to the parameters of a linear model. There is also a bias adjustment that represents the base value of a ptron and is analogous to the intercept in a linear model. If the inputs are near zero, the bias ensures that the output is near average. Due to the network-like nature of the ANN a complex, non-linear relationship exist between the predictors and response.</p>
<p><em>Acknowledgement: aspects of the above discussion are from</em>: <a href="https://rpubs.com/julianhatwell/annr" class="uri">https://rpubs.com/julianhatwell/annr</a></p>
<p><strong>Forward propagation</strong></p>
<p>Figure <a href="ann.html#fig:neuralNetworkModeling">10.3</a> represents a simple ANN, where we have an input later (layer 1) with three ptrons and a base unit, one hidden layer (layer 2) again with three prtons and a base unit, and an output layer (layer 3) where the <span class="math inline">\(h_{\theta}\)</span>(x) is computed.</p>
<p>This method of computing <span class="math inline">\(h_{\theta}\)</span>(x) is called <em>Forward Propagation</em>.</p>
<div class="figure" style="text-align: center"><span id="fig:neuralNetworkModeling"></span>
<img src="images/NN3.png" alt="Neural Network Modeling" width="85%" />
<p class="caption">
Figure 10.3: Neural Network Modeling
</p>
</div>
<p><em>where</em></p>
<p><span class="math inline">\(a_i^{(j)}\)</span>= activation of i in layer j<br />
<span class="math inline">\(\theta^i\)</span> = matrix of weights controlling function mapping from layer j to layer j+1</p>
<p><span class="math display">\[\begin{align}
a_1^{(2)} &amp;= g (\theta_{10}^{(1)}x_0 + \theta_{11}^{(1)}x_1 + \theta_{12}^{(1)}x_2+ \theta_{13}^{(1)}x_3)\\
a_2^{(2)} &amp;= g (\theta_{20}^{(1)}x_0 + \theta_{21}^{(1)}x_1 + \theta_{22}^{(1)}x_2+ \theta_{23}^{(1)}x_3)\\
a_3^{(2)} &amp;= g (\theta_{30}^{(1)}x_0 + \theta_{31}^{(1)}x_1 + \theta_{32}^{(1)}x_2+ \theta_{33}^{(1)}x_3)\\
h_{\theta}(x) &amp;= a_1^{(3)}=g (\theta_{10}^{(2)}a_0^{(2)} + \theta_{11}^{(2)}a_1^{(2)} + \theta_{12}^{(1)}a_2^{(2)} + \theta_{13}^{(2)}a_3^{(2)})\\
\end{align}\]</span></p>
<p>Vectorized notations of inputs and activations.</p>
<p><span class="math display">\[\begin{align}
x &amp;= \begin{bmatrix}x_o \\
x_1\\
x_2\\
x_3
\end{bmatrix}\\
z^{(2)} &amp;= \begin{bmatrix} z_1^{(2)}\\
z_2^{(2)}\\
z_3^{(2)}
\end{bmatrix}
\end{align}\]</span></p>
<p>Vectorized representation of activation of hidden layer and activation layer.</p>
<p><span class="math display">\[\begin{align}
z^{(2)} &amp;= \Theta^{(1)} a^{(1)}\\ 
a^{(2)} &amp;= g(z^{(2)})
\end{align}\]</span></p>
<p><span class="math display">\[\begin{align}
z^{(3)} &amp;= \Theta^{(2)} a^{(2)}\\ 
h_\Theta(x) &amp;= a^{(3)} = g(z^{(3)})
\end{align}\]</span></p>
<p><strong>Why ANN?</strong></p>
Consider the supervised learning problems below (Figure <a href="ann.html#fig:supervisedLearningProbs">10.4</a>). The first two are straight forward cases.
<div class="figure" style="text-align: center"><span id="fig:supervisedLearningProbs"></span>
<img src="images/NN4.PNG" alt="Why ANN" width="85%" />
<p class="caption">
Figure 10.4: Why ANN
</p>
</div>
<p>Whereas for the third we could probably apply a logistic regression with a lot of nonlinear features like this</p>
<p><span class="math display">\[ Y_i = g(\theta_0 + \theta_1 x_1 + \theta_2 x_2+ \theta_3 x_1x_2+ \theta_4 x_i^2x_2 + \theta_5 x_i^3x_2+ \theta_6 x_i^3x_2^2 \dots)\]</span></p>
<p>i.e. if we include enough polynomials we could arrive at an hypothesis that will separate the two classes.</p>
<p>This could perfectly work well if we just have two features, such as x<sub>1</sub> and x<sub>2</sub> but for almost all machine learning problems we usually have more than two features. Importantly if the number of features increase the number of quadratic terms increase as a function of <span class="math inline">\(n^2/2\)</span>; where n is the number of features.</p>
<p>This would result in overfitting if the number of features increase.</p>
<p>Because ANN has felixibility to derive complex features from each layer of ptrons, it can be applied to any complex functional relationship and more importantly unlike generalized linear models (GLMs) it is not necessary to prespecify the type of relationship between covariates and response variables as for instance as linear combination. This makes ANN a valuable statistical tool.</p>
<p>Observed data are used to train the neural network and the neural network learns an approximation of the relationship by iteratively adapting its parameters.</p>
<p>Figure <a href="ann.html#fig:simpleLogicalANDANN">10.5</a> shows an example of a simple logical AND ptron architecture.</p>
<div class="figure" style="text-align: center"><span id="fig:simpleLogicalANDANN"></span>
<img src="images/NN5.png" alt="Simple Logical AND ANN" width="85%" />
<p class="caption">
Figure 10.5: Simple Logical AND ANN
</p>
</div>
<p><strong>Cost function and back propagation</strong></p>
<p>Cost function of linear models</p>
<p><span class="math display">\[
\begin{equation*} 
 CF(\theta_{0},\theta_{1}) = 1/2n\sum_{i=1}^{n} (h_{\theta}(x^i) - y^i)^2
\end{equation*} 
\]</span> Matrix solution to minimise the cost function in linear models</p>
<p><span class="math display">\[ \hat{\boldsymbol{\beta}} = (\mathbf{X}^\top \mathbf{X})^{-1} \mathbf{X}^\top \mathbf{Y} \]</span></p>
<p>Cost funciton in ANN (it is a pretty scarry equation)</p>
<p><span class="math display">\[
\begin{equation*} 
 CF(\Theta) = -1/m \Bigg[\sum_{i=1}^{m}\sum_{k=1}^{K} y_k^{(i)}log(h_\Theta(x^{(i)}))_k+(1-y_k^{(i)})log(1-(h_\Theta(x^{(i)}))_k)\Bigg]
\end{equation*} 
\]</span> <span class="math display">\[
\begin{equation*} 
  +\lambda/2m\sum_{l=1}^{L-1}\sum_{i=1}^{s_l}\sum_{j=1}^{S_{l+1}}(\Theta_{ji}^{(l)})^2
\end{equation*} 
\]</span> We have to minimise this ANN cost function and it is done by back propogation. It is termed back propogation because of the fact that we compute the error from the outer most layer and go backwards.</p>
<p><em>Acknowledgement: The above example and discussion is from Prof. Andrew Ng’s coursera session on ANN</em></p>
<p><em>Example model</em></p>
<p>This example uses the Boston data from the MASS package which contains a number of predictors of median property values in suburbs of Boston, MA, USA. The code used is based on Alice, (2015)</p>
<p>The Boston dataset is a collection of data about housing values in the suburbs of Boston. Our goal is to predict the median value of owner-occupied homes (medv) using all the other continuous variables available.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(neuralnet)
<span class="kw">library</span>(nnet)
<span class="kw">library</span>(NeuralNetTools)
<span class="kw">library</span>(MASS)
<span class="kw">library</span>(ISLR)
<span class="kw">library</span>(caTools) <span class="co"># sample.split</span>
<span class="kw">library</span>(boot) <span class="co"># cv.glm</span>
<span class="kw">library</span>(faraway) <span class="co"># compact lm summary &quot;sumary&quot; function</span></code></pre></div>
<pre><code>## 
## Attaching package: &#39;faraway&#39;</code></pre>
<pre><code>## The following objects are masked from &#39;package:boot&#39;:
## 
##     logit, melanoma</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(caret) <span class="co"># useful tools for machine learning</span></code></pre></div>
<pre><code>## Loading required package: lattice</code></pre>
<pre><code>## 
## Attaching package: &#39;lattice&#39;</code></pre>
<pre><code>## The following object is masked from &#39;package:faraway&#39;:
## 
##     melanoma</code></pre>
<pre><code>## The following object is masked from &#39;package:boot&#39;:
## 
##     melanoma</code></pre>
<pre><code>## Loading required package: ggplot2</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(corrplot)</code></pre></div>
<pre><code>## corrplot 0.84 loaded</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">500</span>)
<span class="kw">library</span>(MASS)
data &lt;-<span class="st"> </span>Boston</code></pre></div>
<p>Checking whether there are any missing data.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">apply</span>(data,<span class="dv">2</span>,<span class="cf">function</span>(x) <span class="kw">sum</span>(<span class="kw">is.na</span>(x)))</code></pre></div>
<pre><code>##    crim      zn   indus    chas     nox      rm     age     dis     rad 
##       0       0       0       0       0       0       0       0       0 
##     tax ptratio   black   lstat    medv 
##       0       0       0       0       0</code></pre>
<p>We randomly splitt the data into a train and a test set and then we fit a linear regression model and test it on the test set.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">index &lt;-<span class="st"> </span><span class="kw">sample</span>(<span class="dv">1</span><span class="op">:</span><span class="kw">nrow</span>(data),<span class="kw">round</span>(<span class="fl">0.75</span><span class="op">*</span><span class="kw">nrow</span>(data)))
train &lt;-<span class="st"> </span>data[index,]
test &lt;-<span class="st"> </span>data[<span class="op">-</span>index,]
lm.fit &lt;-<span class="st"> </span><span class="kw">glm</span>(medv<span class="op">~</span>., <span class="dt">data=</span>train)
<span class="kw">summary</span>(lm.fit)</code></pre></div>
<pre><code>## 
## Call:
## glm(formula = medv ~ ., data = train)
## 
## Deviance Residuals: 
##      Min        1Q    Median        3Q       Max  
## -14.9143   -2.8607   -0.5244    1.5242   25.0004  
## 
## Coefficients:
##               Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  43.469681   6.099347   7.127 5.50e-12 ***
## crim         -0.105439   0.057095  -1.847 0.065596 .  
## zn            0.044347   0.015974   2.776 0.005782 ** 
## indus         0.024034   0.071107   0.338 0.735556    
## chas          2.596028   1.089369   2.383 0.017679 *  
## nox         -22.336623   4.572254  -4.885 1.55e-06 ***
## rm            3.538957   0.472374   7.492 5.15e-13 ***
## age           0.016976   0.015088   1.125 0.261291    
## dis          -1.570970   0.235280  -6.677 9.07e-11 ***
## rad           0.400502   0.085475   4.686 3.94e-06 ***
## tax          -0.015165   0.004599  -3.297 0.001072 ** 
## ptratio      -1.147046   0.155702  -7.367 1.17e-12 ***
## black         0.010338   0.003077   3.360 0.000862 ***
## lstat        -0.524957   0.056899  -9.226  &lt; 2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for gaussian family taken to be 23.26491)
## 
##     Null deviance: 33642  on 379  degrees of freedom
## Residual deviance:  8515  on 366  degrees of freedom
## AIC: 2290
## 
## Number of Fisher Scoring iterations: 2</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">pr.lm &lt;-<span class="st"> </span><span class="kw">predict</span>(lm.fit,test)
MSE.lm &lt;-<span class="st"> </span><span class="kw">sum</span>((pr.lm <span class="op">-</span><span class="st"> </span>test<span class="op">$</span>medv)<span class="op">^</span><span class="dv">2</span>)<span class="op">/</span><span class="kw">nrow</span>(test)</code></pre></div>
<p>The sample(x,size) function simply outputs a vector of the specified size of randomly selected samples from the vector x. By default the sampling is without replacement: index is essentially a random vector of indeces.</p>
<p>Since we are dealing with a regression problem, we are going to use the mean squared error (MSE) as a measure of how much our predictions are far away from the real data.</p>
<p>Before fitting a neural network, we need to be prepare them to train and tune.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">maxs &lt;-<span class="st"> </span><span class="kw">apply</span>(data, <span class="dv">2</span>, max) 
mins &lt;-<span class="st"> </span><span class="kw">apply</span>(data, <span class="dv">2</span>, min)</code></pre></div>
<p>It is important to normalize the data before training a neural network. Avoiding normalization may lead to useless results or to a very difficult training process (most of the times the algorithm will not converge before the number of maximum iterations are allowed). There are different methods to choose to scale the data (z-normalization, min-max scale, etc…).</p>
<p>Here we have chosen to use the min-max method and scale the data in the interval [0,1]. Usually scaling in the intervals [0,1] or [-1,1] tends to give better results. We therefore scale and split the data before moving on:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">scaled &lt;-<span class="st"> </span><span class="kw">as.data.frame</span>(<span class="kw">scale</span>(data, <span class="dt">center =</span> mins, <span class="dt">scale =</span> maxs <span class="op">-</span><span class="st"> </span>mins))

train_ &lt;-<span class="st"> </span>scaled[index,]
test_ &lt;-<span class="st"> </span>scaled[<span class="op">-</span>index,]</code></pre></div>
<p>Scale returns a matrix that needs to be coerced into a data.frame.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(neuralnet)
n &lt;-<span class="st"> </span><span class="kw">names</span>(train_)
f &lt;-<span class="st"> </span><span class="kw">as.formula</span>(<span class="kw">paste</span>(<span class="st">&quot;medv ~&quot;</span>, <span class="kw">paste</span>(n[<span class="op">!</span>n <span class="op">%in%</span><span class="st"> &quot;medv&quot;</span>], <span class="dt">collapse =</span> <span class="st">&quot; + &quot;</span>)))
nn &lt;-<span class="st"> </span><span class="kw">neuralnet</span>(f,<span class="dt">data=</span>train_,<span class="dt">hidden=</span><span class="kw">c</span>(<span class="dv">5</span>,<span class="dv">3</span>),<span class="dt">linear.output=</span>T)</code></pre></div>
<p>There are no fixed rules as to how many layers and neurons to use although there are several more or less accepted rules of thumb. Usually, one hidden layer is enough for a vast numbers of applications. As far as the number of neurons is concerned, it should be between the input layer size and the output layer size, usually 2/3 of the input size.</p>
<p>Since this is a toy example, we are going to use 2 hidden layers with this configuration: 13:5:3:1. The input layer has 13 inputs, the two hidden layers have 5 and 3 neurons and the output layer has a single output since we are doing regression.</p>
<p>The formula y~. is not accepted in the neuralnet() function. You need to first write the formula and then pass it as an argument in the fitting function. The hidden argument accepts a vector with the number of neurons for each hidden layer, while the argument linear.output is used to specify whether we want to do regression linear.output=TRUE or classification linear.output=FALSE</p>
<p>This is the graphical representation of the model with the weights on each connection:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(nn)</code></pre></div>
<p>The black lines show the connections between each layer and the weights on each connection while the blue lines show the bias term added in each step. The bias can be thought as the intercept of a linear model.</p>
<p>The net is essentially a black box so we cannot say that much about the fitting, the weights and the model. Suffice to say that the training algorithm has converged and therefore the model is ready to be used.</p>
<p>Now we can try to predict the values for the test set and calculate the MSE. Remember that the net will output a normalized prediction, so we need to scale it back in order to make a meaningful comparison (or just a simple prediction).</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">pr.nn &lt;-<span class="st"> </span><span class="kw">compute</span>(nn,test_[,<span class="dv">1</span><span class="op">:</span><span class="dv">13</span>])

pr.nn_ &lt;-<span class="st"> </span>pr.nn<span class="op">$</span>net.result<span class="op">*</span>(<span class="kw">max</span>(data<span class="op">$</span>medv)<span class="op">-</span><span class="kw">min</span>(data<span class="op">$</span>medv))<span class="op">+</span><span class="kw">min</span>(data<span class="op">$</span>medv)
test.r &lt;-<span class="st"> </span>(test_<span class="op">$</span>medv)<span class="op">*</span>(<span class="kw">max</span>(data<span class="op">$</span>medv)<span class="op">-</span><span class="kw">min</span>(data<span class="op">$</span>medv))<span class="op">+</span><span class="kw">min</span>(data<span class="op">$</span>medv)

MSE.nn &lt;-<span class="st"> </span><span class="kw">sum</span>((test.r <span class="op">-</span><span class="st"> </span>pr.nn_)<span class="op">^</span><span class="dv">2</span>)<span class="op">/</span><span class="kw">nrow</span>(test_)</code></pre></div>
<p>we then compare the two MSEs</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">print</span>(<span class="kw">paste</span>(MSE.lm,MSE.nn))</code></pre></div>
<pre><code>## [1] &quot;21.6297593507225 15.7518370200153&quot;</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">2</span>))

<span class="kw">plot</span>(test<span class="op">$</span>medv,pr.nn_,<span class="dt">col=</span><span class="st">&#39;red&#39;</span>,<span class="dt">main=</span><span class="st">&#39;Real vs predicted NN&#39;</span>,<span class="dt">pch=</span><span class="dv">18</span>,<span class="dt">cex=</span><span class="fl">0.7</span>)
<span class="kw">abline</span>(<span class="dv">0</span>,<span class="dv">1</span>,<span class="dt">lwd=</span><span class="dv">2</span>)
<span class="kw">legend</span>(<span class="st">&#39;bottomright&#39;</span>,<span class="dt">legend=</span><span class="st">&#39;NN&#39;</span>,<span class="dt">pch=</span><span class="dv">18</span>,<span class="dt">col=</span><span class="st">&#39;red&#39;</span>, <span class="dt">bty=</span><span class="st">&#39;n&#39;</span>)

<span class="kw">plot</span>(test<span class="op">$</span>medv,pr.lm,<span class="dt">col=</span><span class="st">&#39;blue&#39;</span>,<span class="dt">main=</span><span class="st">&#39;Real vs predicted lm&#39;</span>,<span class="dt">pch=</span><span class="dv">18</span>, <span class="dt">cex=</span><span class="fl">0.7</span>)
<span class="kw">abline</span>(<span class="dv">0</span>,<span class="dv">1</span>,<span class="dt">lwd=</span><span class="dv">2</span>)
<span class="kw">legend</span>(<span class="st">&#39;bottomright&#39;</span>,<span class="dt">legend=</span><span class="st">&#39;LM&#39;</span>,<span class="dt">pch=</span><span class="dv">18</span>,<span class="dt">col=</span><span class="st">&#39;blue&#39;</span>, <span class="dt">bty=</span><span class="st">&#39;n&#39;</span>, <span class="dt">cex=</span>.<span class="dv">95</span>)</code></pre></div>
<p><img src="10-ann_files/figure-html/unnamed-chunk-11-1.png" width="672" /></p>
<p>The net is doing a better work than the linear model at predicting medv. Once again, be cautious because this result depends on the train-test split performed above. Below, after the visual plot, we are going to perform a fast cross validation in order to be more confident about the results.</p>
<p>A first visual approach to the performance of the network and the linear model on the test set is plotted.</p>
<p>By visually inspecting the plot we can see that the predictions made by the neural network are (in general) more concetrated around the line (a perfect alignment with the line would indicate a MSE of 0 and thus an ideal perfect prediction) than those made by the linear model.</p>
<p>A perhaps more useful visual comparison is plotted.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(test<span class="op">$</span>medv,pr.nn_,<span class="dt">col=</span><span class="st">&#39;red&#39;</span>,<span class="dt">main=</span><span class="st">&#39;Real vs predicted NN&#39;</span>,<span class="dt">pch=</span><span class="dv">18</span>,<span class="dt">cex=</span><span class="fl">0.7</span>)
<span class="kw">points</span>(test<span class="op">$</span>medv,pr.lm,<span class="dt">col=</span><span class="st">&#39;blue&#39;</span>,<span class="dt">pch=</span><span class="dv">18</span>,<span class="dt">cex=</span><span class="fl">0.7</span>)
<span class="kw">abline</span>(<span class="dv">0</span>,<span class="dv">1</span>,<span class="dt">lwd=</span><span class="dv">2</span>)
<span class="kw">legend</span>(<span class="st">&#39;bottomright&#39;</span>,<span class="dt">legend=</span><span class="kw">c</span>(<span class="st">&#39;NN&#39;</span>,<span class="st">&#39;LM&#39;</span>),<span class="dt">pch=</span><span class="dv">18</span>,<span class="dt">col=</span><span class="kw">c</span>(<span class="st">&#39;red&#39;</span>,<span class="st">&#39;blue&#39;</span>))</code></pre></div>
<p><img src="10-ann_files/figure-html/unnamed-chunk-12-1.png" width="672" /></p>
<p><em>Cross validation</em> is another very important step of building predictive models. While there are different kind of cross validation methods, the basic idea is repeating the following process a number of time:</p>
<p><em>Train-test split</em></p>
<ol style="list-style-type: decimal">
<li>Do the train-test split<br />
</li>
<li>Fit the model to the train set<br />
</li>
<li>Test the model on the test set<br />
</li>
<li>Calculate the prediction error<br />
</li>
<li>Repeat the process K times<br />
Then by calculating the average error we can get a grasp of how the model is doing.</li>
</ol>
<p>We are going to implement a fast cross validation using a for loop for the neural network and the cv.glm() function in the boot package for the linear model.</p>
<p>Here is the 10 fold cross validated MSE for the linear model</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(boot)
<span class="kw">set.seed</span>(<span class="dv">200</span>)
lm.fit &lt;-<span class="st"> </span><span class="kw">glm</span>(medv<span class="op">~</span>.,<span class="dt">data=</span>data)
<span class="kw">cv.glm</span>(data,lm.fit,<span class="dt">K=</span><span class="dv">10</span>)<span class="op">$</span>delta[<span class="dv">1</span>]</code></pre></div>
<pre><code>## [1] 23.83560156</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">450</span>)
cv.error &lt;-<span class="st"> </span><span class="ot">NULL</span>
k &lt;-<span class="st"> </span><span class="dv">10</span></code></pre></div>
<p>Note that we are splitting the data in this way: 90% train set and 10% test set in a random way for 10 times. We are also initializing a progress bar using the plyr library.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(plyr) </code></pre></div>
<pre><code>## 
## Attaching package: &#39;plyr&#39;</code></pre>
<pre><code>## The following object is masked from &#39;package:faraway&#39;:
## 
##     ozone</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">pbar &lt;-<span class="st"> </span><span class="kw">create_progress_bar</span>(<span class="st">&#39;text&#39;</span>)
pbar<span class="op">$</span><span class="kw">init</span>(k)</code></pre></div>
<pre><code>## 
  |                                                                       
  |                                                                 |   0%</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>k){
    index &lt;-<span class="st"> </span><span class="kw">sample</span>(<span class="dv">1</span><span class="op">:</span><span class="kw">nrow</span>(data),<span class="kw">round</span>(<span class="fl">0.9</span><span class="op">*</span><span class="kw">nrow</span>(data)))
    train.cv &lt;-<span class="st"> </span>scaled[index,]
    test.cv &lt;-<span class="st"> </span>scaled[<span class="op">-</span>index,]
    
    nn &lt;-<span class="st"> </span><span class="kw">neuralnet</span>(f,<span class="dt">data=</span>train.cv,<span class="dt">hidden=</span><span class="kw">c</span>(<span class="dv">5</span>,<span class="dv">2</span>),<span class="dt">linear.output=</span>T)
    
    pr.nn &lt;-<span class="st"> </span><span class="kw">compute</span>(nn,test.cv[,<span class="dv">1</span><span class="op">:</span><span class="dv">13</span>])
    pr.nn &lt;-<span class="st"> </span>pr.nn<span class="op">$</span>net.result<span class="op">*</span>(<span class="kw">max</span>(data<span class="op">$</span>medv)<span class="op">-</span><span class="kw">min</span>(data<span class="op">$</span>medv))<span class="op">+</span><span class="kw">min</span>(data<span class="op">$</span>medv)
    
    test.cv.r &lt;-<span class="st"> </span>(test.cv<span class="op">$</span>medv)<span class="op">*</span>(<span class="kw">max</span>(data<span class="op">$</span>medv)<span class="op">-</span><span class="kw">min</span>(data<span class="op">$</span>medv))<span class="op">+</span><span class="kw">min</span>(data<span class="op">$</span>medv)
    
    cv.error[i] &lt;-<span class="st"> </span><span class="kw">sum</span>((test.cv.r <span class="op">-</span><span class="st"> </span>pr.nn)<span class="op">^</span><span class="dv">2</span>)<span class="op">/</span><span class="kw">nrow</span>(test.cv)
    
    pbar<span class="op">$</span><span class="kw">step</span>()
}</code></pre></div>
<pre><code>## 
  |                                                                       
  |======                                                           |  10%
  |                                                                       
  |=============                                                    |  20%
  |                                                                       
  |====================                                             |  30%
  |                                                                       
  |==========================                                       |  40%
  |                                                                       
  |================================                                 |  50%
  |                                                                       
  |=======================================                          |  60%
  |                                                                       
  |==============================================                   |  70%
  |                                                                       
  |====================================================             |  80%
  |                                                                       
  |==========================================================       |  90%
  |                                                                       
  |=================================================================| 100%</code></pre>
<p>We calculate the average MSE and plot the results as a boxplot.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">mean</span>(cv.error)</code></pre></div>
<pre><code>## [1] 10.32697995</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">boxplot</span>(cv.error,<span class="dt">xlab=</span><span class="st">&#39;MSE CV&#39;</span>,<span class="dt">col=</span><span class="st">&#39;cyan&#39;</span>,
        <span class="dt">border=</span><span class="st">&#39;blue&#39;</span>,<span class="dt">names=</span><span class="st">&#39;CV error (MSE)&#39;</span>,
        <span class="dt">main=</span><span class="st">&#39;CV error (MSE) for NN&#39;</span>,<span class="dt">horizontal=</span><span class="ot">TRUE</span>)</code></pre></div>
<p><img src="10-ann_files/figure-html/unnamed-chunk-17-1.png" width="672" /></p>
<p>The average MSE for the neural network (10.33) is lower than the one of the linear model although there seems to be a certain degree of variation in the MSEs of the cross validation. This may depend on the splitting of the data or the random initialization of the weights in the net. By running the simulation different times with different seeds you can get a more precise point estimate for the average MSE.</p>
<p><em>Acknowledgement: the above example is from</em> <a href="https://www.r-bloggers.com/fitting-a-neural-network-in-r-neuralnet-package/" class="uri">https://www.r-bloggers.com/fitting-a-neural-network-in-r-neuralnet-package/</a> ## Exercises</p>
<p>Using a ANN, take a number and calculate its square root.</p>
<p>Solutions to exercises can be found in appendix <a href="solutions-ann.html#solutions-ann">I</a>.</p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="decision-trees.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="mlnn.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/bioinformatics-training/intro-machine-learning/edit/master/10-ann.Rmd",
"text": "Edit"
},
"download": ["intro-machine-learning.pdf"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
