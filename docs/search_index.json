[
["index.html", "An Introduction to Machine Learning 1 About the course 1.1 Overview 1.2 Registration 1.3 Prerequisites 1.4 Github 1.5 License 1.6 Contact 1.7 Colophon", " An Introduction to Machine Learning Sudhakaran Prabakaran, Matt Wayland and Chris Penfold 2018-09-25 1 About the course 1.1 Overview Machine learning gives computers the ability to learn without being explicitly programmed. It encompasses a broad range of approaches to data analysis with applicability across the biological sciences. Lectures will introduce commonly used algorithms and provide insight into their theoretical underpinnings. In the practicals students will apply these algorithms to real biological data-sets using the R language and environment. During this course you will learn about: Some of the core mathematical concepts underpinning machine learning algorithms: matrices and linear algebra; Bayes’ theorem. Classification (supervised learning): partitioning data into training and test sets; feature selection; logistic regression; support vector machines; artificial neural networks; decision trees; nearest neighbours, cross-validation. Exploratory data analysis (unsupervised learning): dimensionality reduction, anomaly detection, clustering. After this course you should be able to: Understand the concepts of machine learning. Understand the strengths and limitations of the various machine learning algorithms presented in this course. Select appropriate machine learning methods for your data. Perform machine learning in R. 1.2 Registration Bioinformatics Training: An Introduction to Machine Learning 1.3 Prerequisites Some familiarity with R would be helpful. For an introduction to R see An Introduction to Solving Biological Problems with R course. 1.4 Github bioinformatics-training/intro-machine-learning-2018 1.5 License GPL-3 1.6 Contact If you have any comments, questions or suggestions about the material, please contact the authors: Sudhakaran Prabakaran, Matt Wayland and Chris Penfold. 1.7 Colophon This book was produced using the bookdown package (Xie 2017), which was built on top of R Markdown and knitr (Xie 2015). References "],
["intro.html", "2 Introduction 2.1 What is machine learning? 2.2 Aspects of ML 2.3 What actually happened under the hood", " 2 Introduction In the era of large scale data collection we are trying to make meaningful intepretation of data. There are two ways to meaningfully intepret data and they are Mechanistic or mathematical modeling based Descriptive or Data Driven We are here to discuss the later approach using machine learning (ML) approaches. 2.1 What is machine learning? We use - computers - more precisely - algorithms to see patterns and learn concepts from data - without being explicitly programmed. For example Google ranking web pages Facebook or Gmail classifying Spams Biological research projects that we are doing - we use ML approaches to interpret effects of mutations in the noncoding regions. We are given a set of Predictors Features or Inputs that we call ‘Explanatory Variables’ and we ask different statistical methods, such as Linear Regression Logistic Regression Neural Networks to formulate an hypothesis i.e. Describe associations Search for patterns Make predictions for the Outcome Variables A bit of a background: ML grew out of AI and Neural Networks 2.2 Aspects of ML There are two aspects of ML Unsupervised learning Supervised learning Unsupervised learning: When we ask an algorithm to find patterns or structure in the data without any specific outcome variables e.g. clustering. We have little or no idea how the results should look like. Supervised learning: When we give both input and outcome variables and we ask the algorithm to formulate an hypothesis that closely captures the relationship. 2.3 What actually happened under the hood The algorithms take a subset of observations called as the training data and tests them on a different subset of data called as the test data. The error between the prediction of the outcome variable the actual data is evaulated as test error. The objective function of the algorithm is to minimise these test errors by tuning the parameters of the hypothesis. Models that successfully capture these desired outcomes are further evaluated for Bias and Variance (overfitting and underfitting). All the above concepts will be discussed in detail in the following lectures. "],
["linear-models.html", "3 Linear models and matrix algebra 3.1 Linear models 3.2 Matrix algebra", " 3 Linear models and matrix algebra 3.1 Linear models We will start with simple linear functions. First Example Height and Weight correlation people &lt;- read.csv(&quot;data/Linear_models/people.csv&quot;, header=F) caption = &#39;Height and weight correlation of people in USA.&#39; plot(people$V2 ~ people$V3, ylab=&quot;weight&quot;, xlab=&quot;height&quot;, xlim=c(0,200), ylim=c(0,100)) knitr::kable( head(people[, 1:3], 15), booktabs = TRUE, caption = &#39;A table of height and weight correlation.&#39; ) Table 3.1: A table of height and weight correlation. V1 V2 V3 1 58 115 2 59 117 3 60 120 4 61 123 5 62 126 6 63 129 7 64 132 8 65 135 9 66 139 10 67 142 11 68 146 12 69 150 13 70 154 14 71 159 15 72 164 Hypothesis \\[ h_{\\theta}(x) = \\theta_{(0)} + \\theta_{(1)}x \\] How do we evaluate? \\[\\theta_{(i&#39;s)}\\] Let us assume \\(\\theta_{(0)}\\) = 25 and \\(\\theta_{(1)}\\) = 0 or Let us assume \\(\\theta_{(0)}\\) = 0 and \\(\\theta_{(1)}\\) = -100 Our hope is that the hypothesis \\(h(x)\\) accounts for all of the data with minimal error. Mathematically: We are tying to minimise \\(\\theta_{(0)}\\) and \\(\\theta_{(1)}\\) or minimisation of \\[ (h_{\\theta}(x) - y)^2 \\] When done for all elements in the matrix it is called the Cost Function. \\[ \\begin{equation*} 1/2n\\sum_{i=1}^{n} (h_{\\theta}(x^i) - y^i)^2 \\end{equation*} \\] where \\(h_{\\theta}(x^i)\\) = \\(\\theta_{0} + \\theta_{1x^i}\\) The Cost Function \\[ \\begin{equation*} CF(\\theta_{0},\\theta_{1}) = 1/2n\\sum_{i=1}^{n} (h_{\\theta}(x^i) - y^i)^2 \\end{equation*} \\] Cost Function is also called the Squard Error Function Our training data set is a scatter plot in the x-y plane and the straight line or hypothesis defined by \\(hθ(x)\\) has to pass through most of these points and the best possible line will have the least average squared vertical distances from the line. When that happens the value of \\(CF(\\theta_{0},\\theta_{1})\\) will be 0. people &lt;- read.csv(&quot;data/Linear_models/people.csv&quot;, header=F) caption = &#39;Height and weight correlation of people in USA.&#39; plot(people$V2 ~ people$V3, ylab=&quot;weight&quot;, xlab=&quot;height&quot;, xlim=c(0,200), ylim=c(0,100)) fit &lt;- lm(people$V2 ~ people$V3) abline(fit$coef,lwd=2) b &lt;- round(fit$coef,4) text(10, 80, paste(&quot;y =&quot;, b[1], &quot;+&quot;, b[2], &quot;x&quot;), adj=c(0,0.5)) Second Example Falling of an object library(UsingR) ## Loading required package: MASS ## Loading required package: HistData ## Loading required package: Hmisc ## Loading required package: lattice ## Loading required package: survival ## Loading required package: Formula ## Loading required package: ggplot2 ## ## Attaching package: &#39;Hmisc&#39; ## The following objects are masked from &#39;package:base&#39;: ## ## format.pval, units ## ## Attaching package: &#39;UsingR&#39; ## The following object is masked from &#39;package:survival&#39;: ## ## cancer library(rafalib) set.seed(1) g &lt;- 9.8 ##meters per second n &lt;- 25 tt &lt;- seq(0,3.4,len=n) ##time in secs, note: we use tt because t is a base function d &lt;- 56.67 - 0.5*g*tt^2 + rnorm(n,sd=1) ##meters library(rafalib) mypar() plot(tt,d,ylab=&quot;Distance in meters&quot;,xlab=&quot;Time in seconds&quot;) This data looks like it might follow the equation \\[ Y_i = \\theta_0 + \\theta_1 x_i + \\theta_2 x_i^2 + \\varepsilon_i, i=1,\\dots,n \\] With \\(Y_i\\) representing location, \\(x_i\\) representing the time, and \\(\\varepsilon_i\\) accounting for measurement error. This is still a linear model because it is a linear combination of known quantities (the \\(x\\)’s) referred to as predictors or covariates and unknown parameters (the \\(\\theta\\)’s). Third Example Father and son height correlation data(father.son,package=&quot;UsingR&quot;) x=father.son$fheight y=father.son$sheight plot(x,y,xlab=&quot;Father&#39;s height&quot;,ylab=&quot;Son&#39;s height&quot;) \\[ Y_i = \\theta_0 + \\theta_1 x_i + \\varepsilon_i, i=1,\\dots,N \\] This is also a linear model with \\(x_i\\) and \\(Y_i\\), the father and son heights respectively, for the \\(i\\)-th pair and \\(\\varepsilon_i\\) a term to account for the extra variability. Here we think of the fathers’ heights as the predictor and being fixed (not random) so we use lower case. Measurement error alone can’t explain all the variability seen in \\(\\varepsilon_i\\). This makes sense as there are other variables not in the model, for example, mothers’ heights, genetic randomness, and environmental factors Fourth Example Mouse diet data Here we read-in mouse body weight data from mice that were fed two different diets: high fat and control (chow). We have a random sample of 12 mice for each. We are interested in determining if the diet has an effect on weight. library(downloader) url &lt;- &quot;https://raw.githubusercontent.com/genomicsclass/dagdata/master/inst/extdata/femaleMiceWeights.csv&quot; filename &lt;- &quot;femaleMiceWeights.csv&quot; if (!file.exists(filename)) download(url,destfile=filename) dat &lt;- read.csv(&quot;femaleMiceWeights.csv&quot;) mypar(1,1) stripchart(Bodyweight~Diet,data=dat,vertical=TRUE,method=&quot;jitter&quot;,pch=1,main=&quot;Mice weights&quot;) We can estimate the difference in average weight between populations using a linear model of the form. \\[ Y_i = \\theta_0 + \\theta_1 x_{i} + \\varepsilon_i\\] with \\(\\theta_0\\) the chow diet average weight, \\(\\theta_i\\) the difference between averages, \\(x_i = 1\\) when mouse \\(i\\) gets the high fat (hf) diet, \\(x_i = 0\\) when it gets the chow diet, and \\(\\varepsilon_i\\) explains the differences between mice of the same population. Linear models in general We have seen four very different examples in which linear models can be used. A general model that encompasses all of the above examples is the following: \\[ h_{\\theta}(x) = \\theta_0 + \\theta_1 x_{i,1} + \\theta_2 x_{i,2} + \\dots + \\theta_2 x_{i,p} + \\varepsilon_i, i=1,\\dots,n \\] \\[ h_{\\theta}(x) = \\theta_0 + \\sum_{j=1}^p \\theta_j x_{i,j} + \\varepsilon_i, i=1,\\dots,n \\] Note that we have a general number of predictors \\(p\\). Matrix algebra provides a compact language and mathematical framework to compute and make derivations with any linear model that fits into the above framework. Therefore most inear models are typically described in matrix algebra framework. 3.2 Matrix algebra The function matrix creates matrices matrix (data, nrow, ncol, byrow) Matrix fills values by columns seq1 &lt;- seq(1:6) m1 &lt;- matrix(seq1, 2) m1 ## [,1] [,2] [,3] ## [1,] 1 3 5 ## [2,] 2 4 6 You can also fill it by rows m2 &lt;- matrix(seq1, 2, byrow = T) m2 ## [,1] [,2] [,3] ## [1,] 1 2 3 ## [2,] 4 5 6 Creating a matrix of 20 numbers from a standard normal distribution m3 &lt;- matrix(rnorm(20), 4) m3 ## [,1] [,2] [,3] [,4] [,5] ## [1,] -0.0593134 -0.2533617 -0.7074952 0.8811077 -1.1293631 ## [2,] 1.1000254 0.6969634 0.3645820 0.3981059 1.4330237 ## [3,] 0.7631757 0.5566632 0.7685329 -0.6120264 1.9803999 ## [4,] -0.1645236 -0.6887557 -0.1123462 0.3411197 -0.3672215 appending a vector to a matrix v1 &lt;- c(1, 5, 7, 8) m4 &lt;- cbind(m3, v1) m4 ## v1 ## [1,] -0.0593134 -0.2533617 -0.7074952 0.8811077 -1.1293631 1 ## [2,] 1.1000254 0.6969634 0.3645820 0.3981059 1.4330237 5 ## [3,] 0.7631757 0.5566632 0.7685329 -0.6120264 1.9803999 7 ## [4,] -0.1645236 -0.6887557 -0.1123462 0.3411197 -0.3672215 8 v2 &lt;- c(1:6) m5 &lt;- rbind(m4, v2) m5 ## v1 ## -0.0593134 -0.2533617 -0.7074952 0.8811077 -1.1293631 1 ## 1.1000254 0.6969634 0.3645820 0.3981059 1.4330237 5 ## 0.7631757 0.5566632 0.7685329 -0.6120264 1.9803999 7 ## -0.1645236 -0.6887557 -0.1123462 0.3411197 -0.3672215 8 ## v2 1.0000000 2.0000000 3.0000000 4.0000000 5.0000000 6 Determining the dimension of a matrix dim(m5) ## [1] 5 6 m6 &lt;- matrix(1:6, 2) m6 ## [,1] [,2] [,3] ## [1,] 1 3 5 ## [2,] 2 4 6 m7 &lt;- matrix(c(rep(1, 3), rep(2, 3)), 2, byrow = T) m7 ## [,1] [,2] [,3] ## [1,] 1 1 1 ## [2,] 2 2 2 Matrix addition m6+m7 ## [,1] [,2] [,3] ## [1,] 2 4 6 ## [2,] 4 6 8 Matrix subtraction m6-m7 ## [,1] [,2] [,3] ## [1,] 0 2 4 ## [2,] 0 2 4 Matrix inverse \\[ \\begin{align} X &amp;= \\begin{bmatrix}a&amp; b \\\\ c &amp; d\\\\ \\end{bmatrix}\\\\\\\\ X^{-1} &amp;= \\dfrac{1}{(ad-bc)}\\begin{bmatrix} d&amp; -b\\\\ -c &amp; a\\\\ \\end{bmatrix} \\end{align} \\] m8 &lt;- matrix(1:4, 2) m8 ## [,1] [,2] ## [1,] 1 3 ## [2,] 2 4 R function for inv matrix solve(m8) ## [,1] [,2] ## [1,] -2 1.5 ## [2,] 1 -0.5 Matrix transpose \\[ \\begin{align} X &amp;= \\begin{bmatrix}a&amp; b \\\\ c &amp; d\\\\ \\end{bmatrix}\\\\\\\\ X^{T} &amp;= \\begin{bmatrix} a&amp; c\\\\ b &amp; d\\\\ \\end{bmatrix} \\end{align} \\] R function for matrix transpose t(m7) ## [,1] [,2] ## [1,] 1 2 ## [2,] 1 2 ## [3,] 1 2 Matrix multiplication Element-wise multiplication m6 * m7 ## [,1] [,2] [,3] ## [1,] 1 3 5 ## [2,] 4 8 12 Cross product m6 %*% t(m7) ## [,1] [,2] ## [1,] 9 18 ## [2,] 12 24 Matrices are not commutative: A∗B≠B∗A Matrices are associative: (A∗B)∗C=A∗(B∗C) Identity matrix (I) The identity matrix, when multiplied by any matrix of the same dimensions, results in the original matrix. It’s just like multiplying numbers by 1. The identity matrix simply has 1’s on the diagonal (upper left to lower right diagonal) and 0’s elsewhere. \\[ \\begin{bmatrix}1&amp; 0&amp; 0 \\\\ 0&amp; 1&amp; 0 \\\\ 0&amp; 0&amp; 1 \\end{bmatrix}\\\\\\\\ \\] \\(AA^{-1} = I\\) Estimating parameters \\(\\theta\\) For the models above to be useful we have to estimate the unknown \\(\\theta\\)s. In the second example, we want to describe a physical process for which we can’t have unknown parameters. In the third example, we better understand inheritance by estimating how much, on average, the father’s height affects the son’s height. In the fourth example, we want to determine if there is in fact a difference: if \\(\\theta_1 \\neq 0\\). As explained above, we have to find the values that minimize the distance of the fitted model to the data. We come back to Cost Function. \\[ \\sum_{i=1}^n \\left( Y_i - \\left(\\theta_0 + \\sum_{j=1}^p \\theta_j x_{i,j}\\right)\\right)^2 \\] Once we find the minimum, we will call the values the least squares estimates (LSE) and denote them with \\(\\hat{\\theta}\\). The quantity obtained when evaluating the least squares equation at the estimates is called the residual sum of squares (RSS). Since all these quantities depend on \\(Y\\), they are random variables. The \\(\\hat{\\theta}\\) s are random variables and we will eventually perform inference on them. What actually happens when we invoke lm? Inside of lm, we will form a design matrix \\(\\mathbf{X}\\) and calculate the Cost function: \\(\\boldsymbol{\\beta}\\), which minimizes the sum of squares. The formula for this solution is: \\[ \\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^\\top \\mathbf{X})^{-1} \\mathbf{X}^\\top \\mathbf{Y} \\] We can calculate this in R using matrix multiplication operator %*%, the inverse function solve, and the transpose function t. Getting back to the mice example set.seed(1) #same jitter in stripchart dat &lt;- read.csv(&quot;femaleMiceWeights.csv&quot;) ##previously downloaded stripchart(dat$Bodyweight ~ dat$Diet, vertical=TRUE, method=&quot;jitter&quot;, main=&quot;Bodyweight over Diet&quot;) We can see that the high fat diet group appears to have higher weights on average, although there is overlap between the two samples. levels(dat$Diet) ## [1] &quot;chow&quot; &quot;hf&quot; X &lt;- model.matrix(~ Diet, data=dat) head(X) ## (Intercept) Diethf ## 1 1 0 ## 2 1 0 ## 3 1 0 ## 4 1 0 ## 5 1 0 ## 6 1 0 Y &lt;- dat$Bodyweight X &lt;- model.matrix(~ Diet, data=dat) solve(t(X) %*% X) %*% t(X) %*% Y ## [,1] ## (Intercept) 23.813333 ## Diethf 3.020833 These coefficients are the average of the control group and the difference of the averages: s &lt;- split(dat$Bodyweight, dat$Diet) mean(s[[&quot;chow&quot;]]) ## [1] 23.81333 mean(s[[&quot;hf&quot;]]) - mean(s[[&quot;chow&quot;]]) ## [1] 3.020833 Finally, we use lm to run the linear model: fit &lt;- lm(Bodyweight ~ Diet, data=dat) summary(fit) ## ## Call: ## lm(formula = Bodyweight ~ Diet, data = dat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -6.1042 -2.4358 -0.4138 2.8335 7.1858 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 23.813 1.039 22.912 &lt;2e-16 *** ## Diethf 3.021 1.470 2.055 0.0519 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.6 on 22 degrees of freedom ## Multiple R-squared: 0.1611, Adjusted R-squared: 0.1229 ## F-statistic: 4.224 on 1 and 22 DF, p-value: 0.05192 (coefs &lt;- coef(fit)) ## (Intercept) Diethf ## 23.813333 3.020833 The following plot provides a visualization of the meaning of the coefficients with colored arrows. stripchart(dat$Bodyweight ~ dat$Diet, vertical=TRUE, method=&quot;jitter&quot;, main=&quot;Bodyweight over Diet&quot;, ylim=c(0,40), xlim=c(0,3)) a &lt;- -0.25 lgth &lt;- .1 library(RColorBrewer) cols &lt;- brewer.pal(3,&quot;Dark2&quot;) abline(h=0) arrows(1+a,0,1+a,coefs[1],lwd=3,col=cols[1],length=lgth) abline(h=coefs[1],col=cols[1]) arrows(2+a,coefs[1],2+a,coefs[1]+coefs[2],lwd=3,col=cols[2],length=lgth) abline(h=coefs[1]+coefs[2],col=cols[2]) legend(&quot;right&quot;,names(coefs),fill=cols,cex=.75,bg=&quot;white&quot;) Data source: Some of this data and code were obtained from https://github.com/genomicsclass/labs/blob/master/matrixalg/matrix_algebra_examples.Rmd Excersises Fit linear models for example 2 and example 3 using lm function. Solutions to exercises can be found in appendix B "],
["logistic-regression.html", "4 Supervised learning 4.1 Regression 4.2 Classification 4.3 Resources", " 4 Supervised learning Supervised learning refers to the general task of identifying functions from a labelled set of data, and using those functions for prediction. The labelled data typically consists of a matched pair of observations \\(\\{\\mathbf{X},\\mathbf{y}\\}\\), where \\(\\mathbf{X}\\) (the input variables) usually denotes a matrix of (real-valued) explanatory variables, with \\(\\mathbf{X}_i\\) denoting the \\(i\\)th column which contains observations for the \\(i\\)th variable, and \\(\\mathbf{y} = (y_1,\\ldots,y_n)^\\top\\) (the output variable) denotes a vector of observations for a variable of interest[^The input variables need not be real values vectors, and could instead represent any measurement including graphs, text etc.]. Depending on the nature of the output variable, supervised learning is split into regression and classification tasks. Within a regression setting, we aim to identify how the input variables map to the (continuous-valued) output variable(s). A simple example would involve measuring the population size of a bacterial culture, \\(\\mathbf{y} = (N_1,\\ldots,N_n)^\\top\\), at a set of time points, \\(\\mathbf{X} = (t_1,\\ldots,t_n)^\\top\\), and learning the function that maps from \\(\\mathbf{X}\\) to \\(\\mathbf{y}\\). Doing so should reveal something about the physical nature of the system, such as identifying the existence of distinct phases of growth. Correctly identifying these functions would also allow us to predict the output variable, \\(\\mathbf{y}^* = (N_i^*,\\ldots,N_k^*)^\\top\\), at a new set of times, \\(\\mathbf{X}^* = (t_i,\\ldots,t_k)^\\top\\). Classification algorithms, on the other hand, deal with discrete-valued outputs. Here each observation in \\(\\mathbf{y} = (y_1,\\ldots,y_n)\\) can take on only a finite number of values. For example, we may have a measurment that indicates “infected” versus “uninfected”, which can be represented in binary, \\(y_i \\in [0,1]\\). More generally we have data that falls into \\(K\\) classes e.g., “group 1” through to “group K”. As with regression, the aim is to identify how the (potentially continuous-valued) input variables map to the discrete set of class labels, and ultimately, assign labels to a new set of observations. Notable examples would be to identify how the expression levels of particular set of marker genes are predictive of a discrete phenotype. In section 4.1 we briefly recap linear regression, and introduce nonlinear approaches to regression based on Gaussian processes. We demonstrate the use of regression to predict gene expression values as a function of time, and how this can be used to inform us about the nature of the data. In section 4.2 we introduce a variety of classification algorithms, starting with logistic regression (section 4), and demonstrate how such approaches can be used to predict pathogen infection status in Arabidopsis thaliana. By doing so we identify key marker genes indicative of pathogen growth. Finally, we note the limitations of linear classification algorithms, and introduce nonlinear approaches based on Gaussian processes (section 4.2.2). 4.1 Regression In this section, we will make use of an existing dataset which captures the gene expression levels in the model plant Arabidopsis thaliana following innoculation with Botrytis cinerea (Windram et al. 2012), a necrotrophic pathogen considered to be one of the most important fungal plant pathogens due to its ability to cause disease in a range of plants. The dataset is a time series measuring the gene expression in Arabidopsis leaves following inoculation with Botrytis cinerea over a \\(48\\) hour time window at \\(2\\) hourly intervals. The dataset is available from GEO (GSE39597) but a pre-processed version has been deposited in the {data} folder. The pre-processed data contains the expression levels of a set of \\(163\\) marker genes in tab delimited format. The fist row contains gene IDs for the marker genes. Column \\(2\\) contains the time points of observations, with column \\(3\\) containing a binary indication of infection status, evalutated according to the prescence of Botrytis cinerea Tubulin protein. All subsequent columns indicate (\\(\\log_2\\)) normalised Arabidopsis gene expression values from microarrays (V4 TAIR V9 spotted cDNA array). The expression dataset itself contains two time series: the first \\(24\\) observations represent measurements of Arabidopsis gene expression in a control time series (uninfected), from \\(2h\\) through \\(48h\\) at \\(2\\)-hourly intervals, and therefore capture dynamic aspects natural plant processes, including circadian rhythms; the second set of \\(24\\) observations represents an infected dataset, again commencing \\(2h\\) after inoculation with Botyris cinerea through to \\(48h\\). Within this section our output variable will typically be the expression level of a particular gene of interest, denoted \\(\\mathbf{y} =(y_1,\\ldots,y_n)^\\top\\), with the explanatory variable being time, \\(\\mathbf{X} =(t_1,\\ldots,t_n)^\\top\\). We can read the dataset into {R} as follows: D &lt;- read.csv(file = &quot;data/Arabidopsis/Arabidopsis_Botrytis_transpose_3.csv&quot;, header = TRUE, sep = &quot;,&quot;, row.names=1) We can also extract out the names of the variables (gene names), and the unique vector of measurment times: genenames &lt;- colnames(D) Xs &lt;- D$Time[1:24] Exercise 3.1. Plot the gene expression profiles to familiarise yourself with the data. 4.1.1 Linear regression Recall that one of the simplest forms of regression, linear regression, assumes that the variable of interest, \\(y\\), depends on an explanatory variable, \\(x\\), via: \\(y = m x + c.\\) For a typical set of data, we have a vector of observations, \\(\\mathbf{y} = (y_1,y_2,\\ldots,y_n)\\) with a corresponding set of explanatory variables. For now we can assume that the explanatory variable is scalar, for example time (in hours), such that we have a set of observations, \\(\\mathbf{X} = (t_1,t_2,\\ldots,t_n)\\). Using linear regression we aim to infer the parameters \\(m\\) and \\(c\\), which will tell us something about the relationship between the two variables, and allow us to make predictions at a new set of locations, \\(\\mathbf{X}*\\). Within {R}, linear regression can be implemented via the {lm} function. In the example below, we perform linear regression for the gene expression of AT2G28890 as a function of time, using the infection time series only (hence we use only the first \\(24\\) datapoints): lm(AT2G28890~Time, data = D[25:nrow(D),]) ## ## Call: ## lm(formula = AT2G28890 ~ Time, data = D[25:nrow(D), ]) ## ## Coefficients: ## (Intercept) Time ## 10.14010 -0.04997 Linear regression is also implemented within the {caret} package, allowing us to make use of its various other utilities. In fact, within {caret}, linear regression is performed by calling the function {lm}. In the example, below, we perform linear regression for AT2G28890, and predict the expression pattern for that gene using the {predict} function: library(caret) ## Loading required package: lattice ## Loading required package: ggplot2 library(mlbench) set.seed(1) geneindex &lt;- which(genenames==&quot;AT2G28890&quot;) lrfit &lt;- train(y~., data=data.frame(x=Xs,y=D[25:nrow(D),geneindex]), method = &quot;lm&quot;) predictedValues&lt;-predict(lrfit) A summary of the model, including parameters, can be printed out to screen using the {summary} function: summary(lrfit) ## ## Call: ## lm(formula = .outcome ~ ., data = dat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.77349 -0.17045 -0.01839 0.15795 0.63098 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 10.14010 0.13975 72.56 &lt; 2e-16 *** ## x -0.04997 0.00489 -10.22 8.14e-10 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.3317 on 22 degrees of freedom ## Multiple R-squared: 0.826, Adjusted R-squared: 0.8181 ## F-statistic: 104.4 on 1 and 22 DF, p-value: 8.136e-10 Finally, we can fit a linear model to the control dataset, and plot the inferred results alongside the observation data: lrfit2 &lt;- train(y~., data=data.frame(x=Xs,y=D[1:24,geneindex]), method = &quot;lm&quot;) predictedValues2&lt;-predict(lrfit2) plot(Xs,D[25:nrow(D),geneindex],type=&quot;p&quot;,col=&quot;black&quot;,ylim=c(min(D[,geneindex])-0.2, max(D[,geneindex]+0.2)),main=genenames[geneindex]) points(Xs,D[1:24,geneindex],type=&quot;p&quot;,col=&quot;red&quot;) points(Xs,predictedValues,type=&quot;l&quot;,col=&quot;black&quot;) points(Xs,predictedValues2,type=&quot;l&quot;,col=&quot;red&quot;) Whilst the model appeared to do resonably well at capturing the general trends in the datset, if we look at the control data (in red), you may notice that, visually, there appears to be more structure to the data than indicated by the model fit. Indeed, if we look AT2G28890 up on CircadianNET, we will see it is likely circadian in nature (\\(p&lt;5\\times10^{-5}\\)). 4.1.2 Polynomial regression In general, linear models will not be appropriate for a large variety of datasets, particularly when the variables of interest are nonlinear. We can instead try to fit more complex models, such as a quadratic function, which has the following form: \\(y = m_1 x + m_2 x^2 + c,\\) where \\(m = [m_1,m_2,c]\\) represent the parameters we’re interested in inferring. An \\(n\\)th-order polynomial has the form: \\(y = \\sum_{i=1}^{n} m_i x^i + c.\\) where \\(m = [m_1,\\ldots,m_n,c]\\) are the free parameters. Within {R} we can infer more complex polynomials to the data using the {lm} package by calling the {poly} function when specififying the symbolic model. In the example below we fit a \\(3\\)rd order polynomial (the order of the polynomial is specified via the {degree} variable): lrfit3 &lt;- lm(y~poly(x,degree=3), data=data.frame(x=D[1:24,1],y=D[1:24,geneindex])) We can do this within {caret}: in the snippet, below, we fit \\(3\\)rd order polynomials to the control and infected datasets, and plot the fits alongside the data. lrfit3 &lt;- train(y~poly(x,degree=3), data=data.frame(x=D[1:24,1],y=D[1:24,geneindex]), method = &quot;lm&quot;) lrfit4 &lt;- train(y~poly(x,degree=3), data=data.frame(x=D[25:nrow(D),1],y=D[25:nrow(D),geneindex]), method = &quot;lm&quot;) plot(Xs,D[25:nrow(D),geneindex],type=&quot;p&quot;,col=&quot;black&quot;,ylim=c(min(D[,geneindex])-0.2, max(D[,geneindex]+0.2)),main=genenames[geneindex]) points(Xs,D[1:24,geneindex],type=&quot;p&quot;,col=&quot;red&quot;) lines(Xs,fitted(lrfit3),type=&quot;l&quot;,col=&quot;red&quot;) lines(Xs,fitted(lrfit4),type=&quot;l&quot;,col=&quot;black&quot;) Note that, by eye, the fit appears to be a little better than for the linear regression model. Well, maybe! We can quantify the accuracy of the models by looking at the root-mean-square error (RMSE) on hold-out data (cross validation), defined as: \\(\\mbox{RMSE} = \\sqrt{\\sum_{i=1}^n (\\hat{y_i}-y_i)^2/n}\\) where \\(\\hat{y_i}\\) is the predicted value and \\(y_i\\) the observed value of the \\(i\\)th (held out) datapoint. In previous sections we explicitly specified a set of training data and hold-out data (test data). If we do not specify this in {caret}, the data is split by default values. Exercise 3.4. What happens if we fit a much higher order polynomial? Try fitting a polynomial with degree = 20 and plotting the result. As we increase the model complexity the fit appears to match much more closely to the observed data. However, intuitively we feel this is wrong. Whilst it may be possible that the data was generated by such complex polynomials, it’s far more likely that we are overfitting the data. We can evaluate how good the model really is by holding some data back and looking at the RMSE from bootstrapped samples. Try splitting the data into training and test datasets, and fitting polynomials of increasing complexity. Plot the RMSE on the training and the test datasets as a function of degree. How does the RMSE compare? Which model seems to be best? 4.1.3 Distributions of fits In the previous section we explored fitting a polynomial function to the data. Recall that we can fit a \\(4\\)th order polynomial to the control datasets as follows: lrfit3 &lt;- lm(y~poly(x,degree=4), data=data.frame(x=D[1:24,1],y=D[1:24,geneindex])) plot(Xs,D[1:24,geneindex],type=&quot;p&quot;,col=&quot;black&quot;,ylim=c(min(D[,geneindex])-0.2, max(D[,geneindex]+0.2)),main=genenames[geneindex]) lines(Xs,fitted(lrfit3),type=&quot;l&quot;,col=&quot;red&quot;) It looks reasonable, but how does it compare to the following shown in blue? lrfit4 &lt;- lrfit3 lrfit4$coefficients &lt;- lrfit4$coefficients + 0.1*matrix(rnorm(length(lrfit4$coefficients)),length(lrfit4$coefficients)); pred1&lt;-predict(lrfit4, data=data.frame(x=D[1:24,1],y=D[1:24,geneindex])) plot(Xs,D[1:24,geneindex],type=&quot;p&quot;,col=&quot;black&quot;,ylim=c(min(D[,geneindex])-0.2, max(D[,geneindex]+0.2)),main=genenames[geneindex]) lines(Xs,fitted(lrfit3),type=&quot;l&quot;,col=&quot;red&quot;) lines(Xs,pred1,type=&quot;l&quot;,col=&quot;blue&quot;) Our new fit was generated by slightly perturbing the optimised parameters via the addition of a small amount of noise. We can see that the new fit is almost as good, and will have a very similar SSE[^This should give us some intuition on the notion of over-fitting. For example, if we make a small perturbation to the parameters of a simpler model, the function will not change all that much; if the simpler model is doing a resonable job of explaining the data, then there may be no necessity of fitting a more complex one. On the other hand, if we made a small perturbation to the parameters of a more complex polynomial, the function may look drastically different. To explain the data with the more complex model would therefore require very specific sets of parameters]. In general, inferring a single fit to a model is prone to overfitting. A much better approach is to instead fit a distribution over fits. We can generate samples from a linear model using the {coef} function. To do so we must use the {lm} function directly, and not via the {caret} package. library(&quot;arm&quot;) ## Loading required package: MASS ## Loading required package: Matrix ## Loading required package: lme4 ## ## arm (Version 1.10-1, built: 2018-4-12) ## Working directory is /home/participant/Matt/intro-machine-learning-2018-master lrfit4 &lt;- lm(y~poly(x,degree=4), data=data.frame(x=D[1:24,1],y=D[1:24,geneindex])) simulate &lt;- coef(sim(lrfit4)) paramsamp &lt;- head(simulate,10) This will sample model parameters that are likely to be explaining the dataset, in this case we have produced \\(10\\) different set of sample parameters. In the code, below, we plot these \\(10\\) sample polynomials: plot(Xs,D[1:24,geneindex],type=&quot;p&quot;,col=&quot;black&quot;,ylim=c(min(D[,geneindex])-0.2, max(D[,geneindex]+0.2)),main=genenames[geneindex]) for (i in c(1,2,3,4,5,6,7,8,9,10)){ lrfit4$coefficients &lt;- paramsamp[i,] pred1&lt;-predict(lrfit4, data=data.frame(x=D[1:24,1],y=D[1:24,geneindex])) lines(Xs,pred1,type=&quot;l&quot;,col=&quot;red&quot;) } Alternatively, we can visualise the confidence bounds directly: lrfit4 &lt;- lm(y~poly(x,degree=4), data=data.frame(x=D[1:24,1],y=D[1:24,geneindex])) pred1&lt;-predict(lrfit4, interval=&quot;predict&quot;) ## Warning in predict.lm(lrfit4, interval = &quot;predict&quot;): predictions on current data refer to _future_ responses plot(Xs,D[1:24,geneindex],type=&quot;p&quot;,col=&quot;black&quot;,ylim=c(min(D[,geneindex])-0.2, max(D[,geneindex]+0.2)),main=genenames[geneindex]) lines(Xs,pred1[,1],type=&quot;l&quot;,col=&quot;red&quot;) lines(Xs,pred1[,2],type=&quot;l&quot;,col=&quot;red&quot;) lines(Xs,pred1[,3],type=&quot;l&quot;,col=&quot;red&quot;) 4.1.4 Gaussian process regression In the previous section we briefly explored fitting multiple polynomials to our data. However, we still had to decide on the order of the polynomial beforehand. A far more powerful approach is Gaussian processes (GP) regression (Williams and Rasmussen 2006). Gaussian process regression represent a Bayesian nonparametric approach to regression capable of inferring nonlinear functions from a set of observations. Within a GP regression setting we assume the following model for the data: \\(y = f(\\mathbf{X})\\) where \\(f(\\cdot)\\) represents an unknown nonlinear function. Formally, Gaussian processes are defined as a collections of random variables, any finite subset of which are jointly Gaussian distributed (Williams and Rasmussen 2006). The significance of this might not be immediately clear, and another way to think of GPs is as an infinite dimensional extension to the standard multivariate normal distribution. In the same way a Gaussian distribution is defined by its mean, \\(\\mathbf{\\mu}\\), and covaraiance matrix, \\(\\mathbf{K}\\), a Gaussian processes is completely defined by its mean function, \\(m(X)\\), and covariance function, \\(k(X,X^\\prime)\\), and we use the notation \\(f(x) \\sim \\mathcal{GP}(m(x), k(x,x^\\prime))\\) to denote that \\(f(X)\\) is drawn from a Gaussian process prior. As it is an infinite dimensional object, dealing directly with the GP prior is not feasible. However, we can make good use of the properties of a Gaussian distributions to sidestep this. Notably, the integral of a Gaussian distribution is itself a Gaussian distribution, which means that if we had a two-dimensional Gaussian distribution (defined over an x-axis and y-axis), we could integrate out the effect of y-axis to give us a (Gaussian) distribution over the x-axis. Gaussian processes share this property, which means that if we are interested only in the distribution of the function at a set of locations, \\(\\mathbf{X}\\) and \\(\\mathbf{X}^*\\), we can specify the distribution of the function over the entirity of the input domain (all of x), and analytically integrate out the effect at all other locations. This induces a natural prior distribution over the output variable that is, itself, Gaussian: \\[ \\begin{eqnarray*} \\begin{pmatrix}\\mathbf{y}^\\top\\\\ \\mathbf{y^*}^\\top \\end{pmatrix} &amp; \\sim &amp; N\\left(\\left[\\begin{array}{c} \\mathbf{0}\\\\ \\mathbf{0}\\\\ \\end{array}\\right],\\left[\\begin{array}{ccc} K(\\mathbf{x},\\mathbf{x}) &amp; K(\\mathbf{x},\\mathbf{x}^*)\\\\ K(\\mathbf{x}^*,\\mathbf{x}) &amp; K(\\mathbf{x}^*,\\mathbf{x}^*) \\\\ \\end{array}\\right)\\right] \\end{eqnarray*} \\] Quite often we deal with noisy data where: \\(y = f(\\mathbf{x}) + \\varepsilon\\), and \\(\\varepsilon\\) represents independent Gaussian noise. In this setting we are interested in inferring the function \\(\\mathbf{f}^*\\) at \\(\\mathbf{X}*\\) i.e., using the noise corrupted data to infer the underlying function, \\(f(\\cdot)\\). To do so we note that a priori we have the following joint distribution: \\[ \\begin{eqnarray*} \\begin{pmatrix}\\mathbf{y}^\\top\\\\ \\mathbf{f^*}^\\top \\end{pmatrix} &amp; \\sim &amp; N\\left(\\left[\\begin{array}{c} \\mathbf{0}\\\\ \\mathbf{0}\\\\ \\end{array}\\right],\\left[\\begin{array}{ccc} K(\\mathbf{x},\\mathbf{x})+\\sigma_n^2 \\mathbb{I} &amp; K(\\mathbf{x},\\mathbf{x}^*)\\\\ K(\\mathbf{x}^*,\\mathbf{x}) &amp; K(\\mathbf{x}^*,\\mathbf{x}^*) \\\\ \\end{array}\\right)\\right] \\end{eqnarray*} \\] 4.1.4.1 Sampling from the prior In the examples below we start by sampling from a GP prior as a way of illustrating what it is that we’re actualy doing. We first require a number of packages: require(MASS) require(plyr) ## Loading required package: plyr require(reshape2) ## Loading required package: reshape2 require(ggplot2) Recall that the GP is completely defined by its mean function and covariance function. We can assume a zero-mean function without loss of generality. Until this point, we have not said much about what the covariance function is. In general, the covariance function encodes all information about the type of functions we’re interested in: is it smooth? Periodic? Does it have more complex structure? Does it branching? A good starting point, and the most commonly used covariance function, is the squared exponential covariance function: \\(k(X,X^\\prime) = \\sigma^2 \\exp\\biggl{(}\\frac{(X-X^\\prime)^2}{2l^2}\\biggr{)}\\). This encodes for smooth functions (functions that are infinitely differentiable), and has two hyperparameters: a length-scale hyperparameter \\(l\\), which defines how fast the functions change over the input space (in our example this would time), and a process variance hyperparameter, \\(\\sigma\\), which encodes the amplitude of the function (in our examples this represents roughly the amplitude of gene expression levels). In the snippet of code, below, we implement a squared exponential covariance function covSE &lt;- function(X1,X2,l=1,sig=1) { K &lt;- matrix(rep(0, length(X1)*length(X2)), nrow=length(X1)) for (i in 1:nrow(K)) { for (j in 1:ncol(K)) { K[i,j] &lt;- sig^2*exp(-0.5*(abs(X1[i]-X2[j]))^2 /l^2) } } return(K) } To get an idea of what this means, we can generate samples from the GP prior at a set of defined positions along \\(X\\). Recall that due to the nature of GPs this is Gaussian distributed: x.star &lt;- seq(-5,5,len=500) ####Define a set of points at which to evaluate the functions sigma &lt;- covSE(x.star,x.star) ###Evaluate the covariance function at those locations, to give the covariance matrix. y1 &lt;- mvrnorm(1, rep(0, length(x.star)), sigma) y2 &lt;- mvrnorm(1, rep(0, length(x.star)), sigma) y3 &lt;- mvrnorm(1, rep(0, length(x.star)), sigma) plot(y1,type = &#39;l&#39;,ylim=c(min(y1,y2,y3),max(y1,y2,y3))) lines(y2) lines(y3) When we specify a GP, we are essentially encoding a distribution over a whole range of functions. Exactly how those functions behave depends upon the choice of covariance function and the hyperparameters. To get a feel for this, try changing the hyperparameters in the above code. What do the functions look like? A variety of other covariance functions exist, and can be found, with examples in the Kernel Cookbook. Exercise 3.4 (optional): Try implementing another covariance function from the Kernel Cookbook and generating samples from the GP prior. Since we have already seen that some of our genes are circadian, a useuful covariance function to try would be the periodic covariance function. 4.1.4.2 Inference with GPs We can generate samples from the GP prior, but what about inference? In linear regression we aimed to infer the parameters, \\(m\\) and \\(a\\). What is the GP doing during inference? Essentially, it’s representing the (unknown) function in terms of the observed data and the hyperparameters. Another way to look at it is that we have specified a prior distribution (encoding for all functions of a particular kind) over the input space; during inference in the noise-free case, we then discard all functions that don’t pass through those observations. During inference for noisy data we assign greater weight to those functions that pass close to our observed datapoints. Essentially we’re using the data to pin down a subset of the prior functions that behave in the appropriate way. For the purpose of inference, we typically have a set of observations, \\(\\mathbf{X}\\), and outputs \\(\\mathbf{y}\\), and are interested in inferring the (unnoisy) values, \\(\\mathbf{f}^*\\), at new set of test locations, \\(\\mathbf{X}^*\\). We can infer a posterior distribution for \\(\\mathbf{f}^*\\) using Bayes’ rule: \\(p(\\mathbf{f}^* | \\mathbf{X}, \\mathbf{y}, \\mathbf{X}^*) = \\frac{p(\\mathbf{y}, \\mathbf{f}^* | \\mathbf{X}, \\mathbf{X}^*)}{p(\\mathbf{y}|\\mathbf{X})}.\\) A key advantage of GPs is that the preditive distribution is analytically tractible and has the following Gaussian form: \\(\\mathbf{f}^* | \\mathbf{X}, \\mathbf{y}, \\mathbf{X}* \\sim \\mathcal{N}(\\hat{f}^*,\\hat{K}^*)\\) where, \\(\\hat{f}^* = K(\\mathbf{X},\\mathbf{X}^*)^\\top(K(\\mathbf{X},\\mathbf{X})+\\sigma^2\\mathbb{I})^{-1} \\mathbf{y}\\), \\(\\hat{K}^* = K(\\mathbf{X}^*,\\mathbf{X}^*)^{-1} - K(\\mathbf{X},\\mathbf{X}^*)^\\top (K(\\mathbf{X},\\mathbf{X})+\\sigma^2\\mathbb{I})^{-1} K(\\mathbf{X},\\mathbf{X}^*)\\). To demonstrate this, let’s assume we have an unknown function we want to infer. In our example, for data generation, we will assume this to be \\(y = \\sin(X)\\) as an illustrative example of a nonlinear function (although we know this, the GP will only ever see samples from this function, never the function itself). We might have some observations from this function at a set of input positions \\(X\\) e.g., one observation at \\(x=-2\\): f &lt;- data.frame(x=c(-2), y=sin(c(-2))) We can infer a posterior GP (and plot this against the true underlying function in red): x &lt;- f$x k.xx &lt;- covSE(x,x) k.xxs &lt;- covSE(x,x.star) k.xsx &lt;- covSE(x.star,x) k.xsxs &lt;- covSE(x.star,x.star) f.star.bar &lt;- k.xsx%*%solve(k.xx)%*%f$y ###Mean cov.f.star &lt;- k.xsxs - k.xsx%*%solve(k.xx)%*%k.xxs ###Var plot(x.star,sin(x.star),type = &#39;l&#39;,col=&quot;red&quot;,ylim=c(-2.2, 2.2)) points(f,type=&#39;o&#39;) lines(x.star,f.star.bar,type = &#39;l&#39;) lines(x.star,f.star.bar+2*sqrt(diag(cov.f.star)),type = &#39;l&#39;,pch=22, lty=2, col=&quot;black&quot;) lines(x.star,f.star.bar-2*sqrt(diag(cov.f.star)),type = &#39;l&#39;,pch=22, lty=2, col=&quot;black&quot;) We can see that the GP has pinned down functions that pass close to the datapoint. Of course, at this stage, the fit is not particularly good, but that’s not surprising as we only had one observation. Crucially, we can see that the GP encodes the idea of uncertainty. Although the model fit is not particularly good, we can see exactly where it is no good. Exercise 3.5 (optional): Try plotting some sample function from the posterior GP. Hint: these will be Gaussian distributed with mean {f.star.bar} and covariance {cov.f.star}. Let’s start by adding more observations. Here’s what the posterior fit looks like if we include 4 observations (at \\(x \\in [-4,-2,0,1]\\)): f &lt;- data.frame(x=c(-4,-2,0,1), y=sin(c(-4,-2,0,1))) x &lt;- f$x k.xx &lt;- covSE(x,x) k.xxs &lt;- covSE(x,x.star) k.xsx &lt;- covSE(x.star,x) k.xsxs &lt;- covSE(x.star,x.star) f.star.bar &lt;- k.xsx%*%solve(k.xx)%*%f$y ###Mean cov.f.star &lt;- k.xsxs - k.xsx%*%solve(k.xx)%*%k.xxs ###Var plot(x.star,sin(x.star),type = &#39;l&#39;,col=&quot;red&quot;,ylim=c(-2.2, 2.2)) points(f,type=&#39;o&#39;) lines(x.star,f.star.bar,type = &#39;l&#39;) lines(x.star,f.star.bar+2*sqrt(diag(cov.f.star)),type = &#39;l&#39;,pch=22, lty=2, col=&quot;black&quot;) lines(x.star,f.star.bar-2*sqrt(diag(cov.f.star)),type = &#39;l&#39;,pch=22, lty=2, col=&quot;black&quot;) And with \\(7\\) observations: f &lt;- data.frame(x=c(-4,-3,-2,-1,0,1,2), y=sin(c(-4,-3,-2,-1,0,1,2))) x &lt;- f$x k.xx &lt;- covSE(x,x) k.xxs &lt;- covSE(x,x.star) k.xsx &lt;- covSE(x.star,x) k.xsxs &lt;- covSE(x.star,x.star) f.star.bar &lt;- k.xsx%*%solve(k.xx)%*%f$y ###Mean cov.f.star &lt;- k.xsxs - k.xsx%*%solve(k.xx)%*%k.xxs ###Var plot(x.star,sin(x.star),type = &#39;l&#39;,col=&quot;red&quot;,ylim=c(-2.2, 2.2)) points(f,type=&#39;o&#39;) lines(x.star,f.star.bar,type = &#39;l&#39;) lines(x.star,f.star.bar+2*sqrt(diag(cov.f.star)),type = &#39;l&#39;,pch=22, lty=2, col=&quot;black&quot;) lines(x.star,f.star.bar-2*sqrt(diag(cov.f.star)),type = &#39;l&#39;,pch=22, lty=2, col=&quot;black&quot;) We can see that with \\(7\\) observations the posterior GP has begun to resemble the true (nonlinear) function very well: the mean of the GP lies very close to the true function and, perhaps more importantly, we continue to have an treatment for the uncertainty. 4.1.4.3 Marginal Likelihood and Optimisation of Hyperparameters Another key aspect of GP regression is the ability to analytically evaluate the marginal likelihood, otherwise referred to as the “model evidence”. The marginal likelihood is the probability of generating the observed datasets under the specified prior. For a GP this would be the probability of seeing the observations \\(\\mathbf{X}\\) under a Gaussian distribtion, \\(\\mathcal{N}(\\mathbf{0},K(\\mathbf{X},\\mathbf{X}))\\). The log marginal likelihood for a noise-free model is: \\(\\ln p(\\mathbf{y}|\\mathbf{X}) = -\\frac{1}{2}\\mathbf{y}^\\top [K(\\mathbf{X},\\mathbf{X})+\\sigma_n^2\\mathbb{I}]^{-1} \\mathbf{y} -\\frac{1}{2} \\ln |K(\\mathbf{X},\\mathbf{X})+\\sigma_n^2\\mathbb{I}| - \\frac{n}{2}\\ln 2\\pi\\) We calculate this in the snippet of code, below, hard-coding a small amount of Gaussian noise: calcML &lt;- function(f,l=1,sig=1) { f2 &lt;- t(f) yt &lt;- f2[2,] y &lt;- f[,2] K &lt;- covSE(f[,1],f[,1],l,sig) ML &lt;- -0.5*yt%*%ginv(K+0.1^2*diag(length(y)))%*%y -0.5*log(det(K)) -(length(f[,1])/2)*log(2*pi); return(ML) } The ability to calculate the marginal likelihood gives us a way to automatically select the hyperparameters. We can increment hyperparameters over a range of values, and choose the values that yield the greatest marginal likelihood. In the example, below, we increment both the length-scale and process variance hyperparameter: library(plot3D) par &lt;- seq(.1,10,by=0.1) ML &lt;- matrix(rep(0, length(par)^2), nrow=length(par), ncol=length(par)) for(i in 1:length(par)) { for(j in 1:length(par)) { ML[i,j] &lt;- calcML(f,par[i],par[j]) } } persp3D(z = ML,theta = 120) ind&lt;-which(ML==max(ML), arr.ind=TRUE) print(c(&quot;length-scale&quot;, par[ind[1]])) ## [1] &quot;length-scale&quot; &quot;2.4&quot; print(c(&quot;process variance&quot;, par[ind[2]])) ## [1] &quot;process variance&quot; &quot;1.3&quot; Here we have performed a grid search to identify the optimal hyperparameters. In practice, the derivative of the marginal likelihood with respect to the hyperparameters is analytically tractable, allowing us to optimise using gradient search algorithms. Exercise 3.6: Try plotting the GP using the optimised hyperparameter values. Exercise 3.7: Now try fitting a Gaussian process to one of the gene expression profiles in the Botrytis dataset. Hint: You may need to normalise the time axis. Since this data also contains a high level of noise you will also need to use a covariance function/ML calculation that incorporates noise. The snippet of code, below, does this, with the noise now representing a \\(3\\)rd hyperparameter. covSEn &lt;- function(X1,X2,l=1,sig=1,sigman=0.1) { K &lt;- matrix(rep(0, length(X1)*length(X2)), nrow=length(X1)) for (i in 1:nrow(K)) { for (j in 1:ncol(K)) { K[i,j] &lt;- sig^2*exp(-0.5*(abs(X1[i]-X2[j]))^2 /l^2) if (i==j){ K[i,j] &lt;- K[i,j] + sigman^2 } } } return(K) } calcMLn &lt;- function(f,l=1,sig=1,sigman=0.1) { f2 &lt;- t(f) yt &lt;- f2[2,] y &lt;- f[,2] K &lt;- covSE(f[,1],f[,1],l,sig) ML &lt;- -0.5*yt%*%ginv(K+diag(length(y))*sigman^2)%*%y -0.5*log(det(K+diag(length(y))*sigman^2)) -(length(f[,1])/2)*log(2*pi); return(ML) } 4.1.4.4 Model Selection As well as being a useful criterion for selecting hyperparameters, the marginal likelihood can be used as a basis for selecting models. For example, we might be interested in comparing how well we fit the data using two different covariance functions: a squared exponential covariance function (model 1, \\(M_1\\)) versus a periodic covariance function (model 2, \\(M_2\\)). By taking the ratio of the marginal likelihoods we can calculate the Bayes’ Factor (BF) which allows us to determine which model is the best: \\(\\mbox{BF} = \\frac{ML(M_1)}{ML(M_2)}\\). High values for the BF indicate strong evidence for \\(M_1\\) over \\(M_2\\), whilst low values would indicate the contrary. Excercise: Using our previous example, \\(y = sin(x)\\) try fitting a periodic covariance function. How well does it generalise e.g., how well does it fit \\(f(\\cdot)\\) far from the observation data? How does this compare to a squared-exponential? 4.1.4.5 Advanced application 1: differential expression of time series Differential expression analysis is concerned with identifying if two sets of data are significantly different from one another. For example, if we measured the expression level of a gene in two different conditions (control versus treatment), you could use an appropriate statistical test to determine whether the expression of that gene had been affected by the treatment. Most statistical tests used for this are not appropriate when dealing with time series data (illustrated in Figure 4.1). Figure 4.1: Differential expression analysis for time series. Here we have two time series with very different behaviour (right). However, as a whole the mean and variance of the time series is identical (left) and the datasets are not differentially expressed using a t-test (p&lt;0.9901) Gaussian processes regression represents a useful way of modelling time series, and can therefore be used as a basis for detecting differential expression in time series. To do so we write down two competing modes: (i) the two time series are differentially expressed, and are therefore best described by two independent GPs; (ii) the two time series are noisy observations from an identical underlying process, and are therefore best described by a single joint GP applied to the union of the data. Exercise 3.8 (optional): Write a function for determining differential expression for two genes. Hint: you will need to fit \\(3\\) GPs: one to the mock/control, one to the infected dataset, and one to the union of mock/control and infected. You can use the Bayes’ Factor to determine if the gene is differentially expressed. 4.1.4.6 Advanced Application 2: Timing of differential expression Nonlinear Bayesian regression represent a powerful tool for modelling time series. In the previous section we have shown how GPs can be used to model if two time series are differentially expressed. More advanced models using GPs aim to identify when two (or more) time series diverge (Stegle et al. 2010,J. Yang et al. (2016),C. A. Penfold et al. (2017)). The DEtime package (J. Yang et al. 2016) is one way to do so. ###install.packages(&quot;devtools&quot;) library(devtools) ###install_github(&quot;ManchesterBioinference/DEtime&quot;) ###import(DEtime) library(DEtime) Within {DEtime}, we can call the function {DEtime_rank} to calculate marginal likelihood ratios for two time series, similar to our application in the previous section. Note that here, the hyperparameters are optimised by gradient search rather than grid searches. res_rank &lt;- DEtime_rank(ControlTimes = Xs, ControlData = D[1:24,3], PerturbedTimes = Xs, PerturbedData = D[25:48,3], savefile=TRUE) ## [1] &quot;gene IDs are not provided. Numbers are used instead&quot; ## rank list saved in DEtime_rank.txt For genes that are DE, we identify the timing of divergence between two time series using the function {DEtime_infer} and visualise the plot using the {plot_DEtime} function. res &lt;- DEtime_infer(ControlTimes = Xs, ControlData = D[1:24,3], PerturbedTimes = Xs, PerturbedData = D[25:48,3]) ## gene IDs are not provided. Numbers are used instead. ## Testing perturbation time points are not provided. Default one is used. ## gene 1 is done ## DEtime inference is done. ## Please use print_DEtime or plot_DEtime to view the results. print_DEtime ## function (DEtimeOutput) ## { ## cat(&quot;Perturbation point inference results from DEtime package: \\n&quot;) ## cat(&quot;==========================================================\\n&quot;) ## print(DEtimeOutput$result, sep = &quot;\\t&quot;, zero.print = &quot;.&quot;) ## cat(&quot;==========================================================\\n&quot;) ## } ## &lt;bytecode: 0xd461690&gt; ## &lt;environment: namespace:DEtime&gt; plot_DEtime(res) ## All genes will be plotted ## 1 is plotted We can do it for all genes using the example below: res &lt;- DEtime_infer(ControlTimes = Xs, ControlData = t(D[1:24,]), PerturbedTimes = Xs, PerturbedData = t(D[25:48,])) ## ControlData is accepted ## PerturbedData is accepted ## gene IDs are not provided. Numbers are used instead. ## Testing perturbation time points are not provided. Default one is used. ## gene 1 is done ## gene 2 is done ## gene 3 is done ## gene 4 is done ## gene 5 is done ## gene 6 is done ## gene 7 is done ## gene 8 is done ## gene 9 is done ## gene 10 is done ## gene 11 is done ## gene 12 is done ## gene 13 is done ## gene 14 is done ## gene 15 is done ## gene 16 is done ## gene 17 is done ## gene 18 is done ## gene 19 is done ## gene 20 is done ## gene 21 is done ## gene 22 is done ## gene 23 is done ## gene 24 is done ## gene 25 is done ## gene 26 is done ## gene 27 is done ## gene 28 is done ## gene 29 is done ## gene 30 is done ## gene 31 is done ## gene 32 is done ## gene 33 is done ## gene 34 is done ## gene 35 is done ## gene 36 is done ## gene 37 is done ## gene 38 is done ## gene 39 is done ## gene 40 is done ## gene 41 is done ## gene 42 is done ## gene 43 is done ## gene 44 is done ## gene 45 is done ## gene 46 is done ## gene 47 is done ## gene 48 is done ## gene 49 is done ## gene 50 is done ## gene 51 is done ## gene 52 is done ## gene 53 is done ## gene 54 is done ## gene 55 is done ## gene 56 is done ## gene 57 is done ## gene 58 is done ## gene 59 is done ## gene 60 is done ## gene 61 is done ## gene 62 is done ## gene 63 is done ## gene 64 is done ## gene 65 is done ## gene 66 is done ## gene 67 is done ## gene 68 is done ## gene 69 is done ## gene 70 is done ## gene 71 is done ## gene 72 is done ## gene 73 is done ## gene 74 is done ## gene 75 is done ## gene 76 is done ## gene 77 is done ## gene 78 is done ## gene 79 is done ## gene 80 is done ## gene 81 is done ## gene 82 is done ## gene 83 is done ## gene 84 is done ## gene 85 is done ## gene 86 is done ## gene 87 is done ## gene 88 is done ## gene 89 is done ## gene 90 is done ## gene 91 is done ## gene 92 is done ## gene 93 is done ## gene 94 is done ## gene 95 is done ## gene 96 is done ## gene 97 is done ## gene 98 is done ## gene 99 is done ## gene 100 is done ## gene 101 is done ## gene 102 is done ## gene 103 is done ## gene 104 is done ## gene 105 is done ## gene 106 is done ## gene 107 is done ## gene 108 is done ## gene 109 is done ## gene 110 is done ## gene 111 is done ## gene 112 is done ## gene 113 is done ## gene 114 is done ## gene 115 is done ## gene 116 is done ## gene 117 is done ## gene 118 is done ## gene 119 is done ## gene 120 is done ## gene 121 is done ## gene 122 is done ## gene 123 is done ## gene 124 is done ## gene 125 is done ## gene 126 is done ## gene 127 is done ## gene 128 is done ## gene 129 is done ## gene 130 is done ## gene 131 is done ## gene 132 is done ## gene 133 is done ## gene 134 is done ## gene 135 is done ## gene 136 is done ## gene 137 is done ## gene 138 is done ## gene 139 is done ## gene 140 is done ## gene 141 is done ## gene 142 is done ## gene 143 is done ## gene 144 is done ## gene 145 is done ## gene 146 is done ## gene 147 is done ## gene 148 is done ## gene 149 is done ## gene 150 is done ## gene 151 is done ## gene 152 is done ## gene 153 is done ## gene 154 is done ## gene 155 is done ## gene 156 is done ## gene 157 is done ## gene 158 is done ## gene 159 is done ## gene 160 is done ## gene 161 is done ## gene 162 is done ## gene 163 is done ## gene 164 is done ## DEtime inference is done. ## Please use print_DEtime or plot_DEtime to view the results. By systematically evaluating the time of divergence for all genes (and visualising the results as a histogram), we can we can begin to shed light on the temporal progression of the infection process. hist(as.numeric(res$result[,2]),breaks=20) 4.1.4.7 Scalability Whilst GPs represent a powerful approach to nonlinear regression, they do have some limitations. GPs do not scale well with the number of observations, and standard GP approaches are not suitable when we have a very large datasets (thousands of observations). To overcome these limitations, approximate approaches to inference with GPs have been developed. 4.2 Classification Classification algorithms are a supervised learning techniques that assign data to categorical outputs. For example we may have a continuous input variable, \\(X\\), and want to learn how that variable maps to a discrete valued output, \\(y\\in [0,1]\\), which might represent two distinct phenotypes “infected” versus “uninfected”. This section is split as follows: in section 4 we introduce logistic regression, a simple classification algorithm based on linear models; and in section @ref(#gp-classification) we demonstrate the use of nonlinear classifiers based on Gaussian process, highlighting when GP classifiers are more appropriate. 4.2.1 Logistic regression The type of linear regression models we’ve been using up to this point deal with real-valued observation data, \\(\\mathbf{y}\\), and are therefore not appropriate for classification. To deal with cases where \\(\\mathbf{y}\\) is a binary outcome, we instead fit a linear model to the logit (natural log) of the log-odds ratio: \\(\\ln \\biggl{(}\\frac{p(x)}{1-p(x)}\\biggr{)} = c + m_1 x_1.\\) Although this model is not immediately intuitive, if we solve for \\(p(x)\\) we get: \\(p(x) = \\frac{1}{1+\\exp(-c - m_1 x_1)}\\). We have thus specified a function that indicates the probability of success for a given value of \\(x\\) e.g., \\(P(y=1|x)\\). Note that in our observation data \\(\\mathbf{y}\\) itself can only take on one of two values. We can think of our data as a being a sample from a Bernoulli trial, and we can therefore write down the likelihood for a set of observations \\({\\mathbf{X},\\mathbf{y}}\\): \\(\\mathcal{L}(c,m_1) = \\prod_{i=1}^n p(x_i)^{y_i} (1-p(x_i)^{1-y_i})\\). In general, these models do not admit a closed form solution, but can be solved iteratively via maximum likelihood, that is by finding the values \\((c,m_1)\\) that return the greatest value of \\(\\mathcal{L}(c,m_1)\\). Within {caret}, logistic regression can applied using the {glm} function. To illustate this we will again make use of our plant dataset. Recall that the second column represents a binary variable indicative of infection status e.g., population growth of the Botrytis cinerea pathogen indicated by statistical enrichment of the Botrytis Tubulin versus the earliest time point. In the excercises, below, we will aim to learn a set of markers capable of predicting infection status using logistic regression. To begin with, let’s see if time is informative of infection status: library(pROC) ## Type &#39;citation(&quot;pROC&quot;)&#39; for a citation. ## ## Attaching package: &#39;pROC&#39; ## The following objects are masked from &#39;package:stats&#39;: ## ## cov, smooth, var library(ROCR) ## Loading required package: gplots ## ## Attaching package: &#39;gplots&#39; ## The following object is masked from &#39;package:stats&#39;: ## ## lowess options(warn=-1) mod_fit &lt;- train(y ~ ., data=data.frame(x = D$Time, y = as.factor(D$Class)), method=&quot;glm&quot;, family=&quot;binomial&quot;) To evaluate the model, we will load in a second (related) dataset, containnig a new set of observations not seen by the model, and predict infection status. Dpred &lt;- read.csv(file = &quot;data/Arabidopsis/Arabidopsis_Botrytis_pred_transpose_3.csv&quot;, header = TRUE, sep = &quot;,&quot;, row.names=1) prob &lt;- predict(mod_fit, newdata=data.frame(x = Dpred$Time, y = as.factor(Dpred$Class)), type=&quot;prob&quot;) pred &lt;- prediction(prob$`1`, as.factor(Dpred$Class)) To evaluate how well the algorithm has done, we can calculate a variety of summary statistics. For example the number of true positives, true negatives, false positive and false negatives. A useful summary is to plot the ROC curve (false positive rate versus true positive rate) and calculate the area under the curve. For a perfect algorithm, the area under this curve (AUC) will be equal to \\(1\\), whereas random assignment would give an area of \\(0.5\\). In the example below, we will calculate the AUC for a logistic regression model: perf &lt;- performance(pred, measure = &quot;tpr&quot;, x.measure = &quot;fpr&quot;) plot(perf) auc &lt;- performance(pred, measure = &quot;auc&quot;) auc &lt;- auc@y.values[[1]] auc ## [1] 0.6111111 Okay, so a score of \\(0.61\\) is certainly better than random, but not particularly good. This is perhaps not surprising, as half the time series (the control) is uninfected over the entirety of the time series, whilst in the second times series Botrytis is able to infect from around time point 8 onwards. The slighty better than random performence therefore arises due the slight bias in the number of instances of each class. In the example, below, we instead try to regress infection status against individual gene expression levels. The idea is to identify genes that have expression values indicative of Botrytis infection: marker genes. aucscore &lt;- matrix(rep(0, 164), 1, 164) for (i in seq(3,164)){ mod_fit &lt;- train(y ~ ., data=data.frame(x = D[,i], y = as.factor(D$Class)), method=&quot;glm&quot;, family=&quot;binomial&quot;) prob &lt;- predict(mod_fit, newdata=data.frame(x = Dpred[,i], y = as.factor(Dpred$Class)), type=&quot;prob&quot;) pred &lt;- prediction(prob$`1`, as.factor(Dpred$Class)) perf &lt;- performance(pred, measure = &quot;tpr&quot;, x.measure = &quot;fpr&quot;) auc &lt;- performance(pred, measure = &quot;auc&quot;) aucscore[i] &lt;- auc@y.values[[1]] } plot(aucscore[1,3:ncol(aucscore)],ylab=&quot;AUC&quot;,xlab=&quot;gene index&quot;) We note that, several genes in the list apear to have AUC scores much greater than \\(0.6\\). We can take a look at some of the genes with high predictive power: genenames[which(aucscore&gt;0.8)] ## [1] &quot;AT1G29990&quot; &quot;AT1G67170&quot; &quot;AT2G21380&quot; &quot;AT2G28890&quot; &quot;AT2G35500&quot; ## [6] &quot;AT2G45660&quot; &quot;AT3G09980&quot; &quot;AT3G11590&quot; &quot;AT3G13720&quot; &quot;AT3G25710&quot; ## [11] &quot;AT3G44720&quot; &quot;AT3G48150&quot; &quot;AT4G00710&quot; &quot;AT4G02150&quot; &quot;AT4G16380&quot; ## [16] &quot;AT4G19700&quot; &quot;AT4G26450&quot; &quot;AT4G28640&quot; &quot;AT4G34710&quot; &quot;AT4G36970&quot; ## [21] &quot;AT4G39050&quot; &quot;AT5G11980&quot; &quot;AT5G22630&quot; &quot;AT5G24660&quot; &quot;AT5G43700&quot; ## [26] &quot;AT5G50010&quot; &quot;AT5G56250&quot; Unsurprisingly, amongst these genes we see a variety of genes whose proteins are known to be targeted by various pathogen effectors, and are therefore directly implicated in the immune response (Table 3.1). Gene Effector AT3G25710 ATR1_ASWA1 AT4G19700 ATR13_NOKS1 AT4G34710 ATR13_NOKS1 AT4G39050 ATR13_NOKS1 AT5G24660 ATR13_NOKS1 AT4G00710 AvrRpt2_Pto JL1065_CatalyticDead AT4G16380 HARXL44 AT2G45660 HARXL45 AT5G11980 HARXL73 AT2G35500 HARXLL445 AT1G67170 HARXLL470_WACO9 AT4G36970 HARXLL470_WACO9 AT5G56250 HARXLL470_WACO9 AT3G09980 HARXLL516_WACO9 AT5G50010 HARXLL60 AT3G44720 HARXLL73_2_WACO9 AT5G22630 HARXLL73_2_WACO9 AT5G43700 HopH1_Psy B728A Table 3.1: Genes predictive of infection status of Botrytis cinerea whose proteins are targeted by effectors of a variety of pathogens Let’s take a look at what the data looks like. In this case we plot the training data labels and the fit from the logistic regression i.e., \\(p(\\mathbf{y}=1|\\mathbf{x})\\): bestpredictor &lt;- which(aucscore==max(aucscore)) best_mod_fit &lt;- train(y ~., data=data.frame(x = D[,bestpredictor], y = as.factor(D$Class)), family=&quot;binomial&quot;, method=&quot;glm&quot;) plot(D[,bestpredictor],D$Class,xlab=genenames[bestpredictor],ylab=&quot;Class&quot;) lines(seq(min(D[,bestpredictor]),max(D[,bestpredictor]),length=200),predict(best_mod_fit,newdata=data.frame(x = seq(min(D[,bestpredictor]),max(D[,bestpredictor]),length=200)),type=&quot;prob&quot;)[,2]) 4.2.2 GP classification Classification approaches using Gaussian processes are also possible. Unlike Gaussian process regression, Gaussian process classification is not analytically tractable, and we must instead use approximations. A GP classifier has been implemented in {caret} using a polynomial kernel, and can be called using the following code: mod_fit2 &lt;- train(y~., data=data.frame(x = D$Time, y = as.factor(D$Class)), method=&quot;gaussprPoly&quot;) Again, we can systematically evaluate how well different genes predict the observed phenotype. library(kernlab) ## ## Attaching package: &#39;kernlab&#39; ## The following object is masked from &#39;package:ggplot2&#39;: ## ## alpha aucscore2 &lt;- matrix(rep(0, 164), 1, 164) for (i in seq(3,164)){ mod_fit2 &lt;- gausspr(D[,i], as.factor(D$Class), scaled = TRUE, type= NULL, kernel=&quot;rbfdot&quot;, kpar=&quot;automatic&quot;, variance.model = FALSE, tol=0.0005, cross=0, fit=TRUE) prob&lt;-predict(mod_fit2, Dpred[,i], type=&quot;probabilities&quot;) pred &lt;- prediction(prob[,2], as.factor(Dpred$Class)) perf &lt;- performance(pred, measure = &quot;tpr&quot;, x.measure = &quot;fpr&quot;) auc &lt;- performance(pred, measure = &quot;auc&quot;) aucscore2[i] &lt;- auc@y.values[[1]] } ## Using automatic sigma estimation (sigest) for RBF or laplace kernel ## Using automatic sigma estimation (sigest) for RBF or laplace kernel ## Using automatic sigma estimation (sigest) for RBF or laplace kernel ## Using automatic sigma estimation (sigest) for RBF or laplace kernel ## Using automatic sigma estimation (sigest) for RBF or laplace kernel ## Using automatic sigma estimation (sigest) for RBF or laplace kernel ## Using automatic sigma estimation (sigest) for RBF or laplace kernel ## Using automatic sigma estimation (sigest) for RBF or laplace kernel ## Using automatic sigma estimation (sigest) for RBF or laplace kernel ## Using automatic sigma estimation (sigest) for RBF or laplace kernel ## Using automatic sigma estimation (sigest) for RBF or laplace kernel ## Using automatic sigma estimation (sigest) for RBF or laplace kernel ## Using automatic sigma estimation (sigest) for RBF or laplace kernel ## Using automatic sigma estimation (sigest) for RBF or laplace kernel ## Using automatic sigma estimation (sigest) for RBF or laplace kernel ## Using automatic sigma estimation (sigest) for RBF or laplace kernel ## Using automatic sigma estimation (sigest) for RBF or laplace kernel ## Using automatic sigma estimation (sigest) for RBF or laplace kernel ## Using automatic sigma estimation (sigest) for RBF or laplace kernel ## Using automatic sigma estimation (sigest) for RBF or laplace kernel ## Using automatic sigma estimation (sigest) for RBF or laplace kernel ## Using automatic sigma estimation (sigest) for RBF or laplace kernel ## Using automatic sigma estimation (sigest) for RBF or laplace kernel ## Using automatic sigma estimation (sigest) for RBF or laplace kernel ## Using automatic sigma estimation (sigest) for RBF or laplace kernel ## Using automatic sigma estimation (sigest) for RBF or laplace kernel ## Using automatic sigma estimation (sigest) for RBF or laplace kernel ## Using automatic sigma estimation (sigest) for RBF or laplace kernel ## Using automatic sigma estimation (sigest) for RBF or laplace kernel ## Using automatic sigma estimation (sigest) for RBF or laplace kernel ## Using automatic sigma estimation (sigest) for RBF or laplace kernel ## Using automatic sigma estimation (sigest) for RBF or laplace kernel ## Using automatic sigma estimation (sigest) for RBF or laplace kernel ## Using automatic sigma estimation (sigest) for RBF or laplace kernel ## Using automatic sigma estimation (sigest) for RBF or laplace kernel ## Using automatic sigma estimation (sigest) for RBF or laplace kernel ## Using automatic sigma estimation (sigest) for RBF or laplace kernel ## Using automatic sigma estimation (sigest) for RBF or laplace kernel ## Using automatic sigma estimation (sigest) for RBF or laplace kernel ## Using automatic sigma estimation (sigest) for RBF or laplace kernel ## Using automatic sigma estimation (sigest) for RBF or laplace kernel ## Using automatic sigma estimation (sigest) for RBF or laplace kernel ## Using automatic sigma estimation (sigest) for RBF or laplace kernel ## Using automatic sigma estimation (sigest) for RBF or laplace kernel ## Using automatic sigma estimation (sigest) for RBF or laplace kernel ## Using automatic sigma estimation (sigest) for RBF or laplace kernel ## Using automatic sigma estimation (sigest) for RBF or laplace kernel ## Using automatic sigma estimation (sigest) for RBF or laplace kernel ## Using automatic sigma estimation (sigest) for RBF or laplace kernel ## Using automatic sigma estimation (sigest) for RBF or laplace kernel ## Using automatic sigma estimation (sigest) for RBF or laplace kernel ## Using automatic sigma estimation (sigest) for RBF or laplace kernel ## Using automatic sigma estimation (sigest) for RBF or laplace kernel ## Using automatic sigma estimation (sigest) for RBF or laplace kernel ## Using automatic sigma estimation (sigest) for RBF or laplace kernel ## Using automatic sigma estimation (sigest) for RBF or laplace kernel ## Using automatic sigma estimation (sigest) for RBF or laplace kernel ## Using automatic sigma estimation (sigest) for RBF or laplace kernel ## Using automatic sigma estimation (sigest) for RBF or laplace kernel ## Using automatic sigma estimation (sigest) for RBF or laplace kernel ## Using automatic sigma estimation (sigest) for RBF or laplace kernel ## Using automatic sigma estimation (sigest) for RBF or laplace kernel ## Using automatic sigma estimation (sigest) for RBF or laplace kernel ## Using automatic sigma estimation (sigest) for RBF or laplace kernel ## Using automatic sigma estimation (sigest) for RBF or laplace kernel ## Using automatic sigma estimation (sigest) for RBF or laplace kernel ## Using automatic sigma estimation (sigest) for RBF or laplace kernel ## Using automatic sigma estimation (sigest) for RBF or laplace kernel ## Using automatic sigma estimation (sigest) for RBF or laplace kernel ## Using automatic sigma estimation (sigest) for RBF or laplace kernel ## Using automatic sigma estimation (sigest) for RBF or laplace kernel ## Using automatic sigma estimation (sigest) for RBF or laplace kernel ## Using automatic sigma estimation (sigest) for RBF or laplace kernel ## Using automatic sigma estimation (sigest) for RBF or laplace kernel ## Using automatic sigma estimation (sigest) for RBF or laplace kernel ## Using automatic sigma estimation (sigest) for RBF or laplace kernel ## Using automatic sigma estimation (sigest) for RBF or laplace kernel ## Using automatic sigma estimation (sigest) for RBF or laplace kernel ## Using automatic sigma estimation (sigest) for RBF or laplace kernel ## Using automatic sigma estimation (sigest) for RBF or laplace kernel ## Using automatic sigma estimation (sigest) for RBF or laplace kernel ## Using automatic sigma estimation (sigest) for RBF or laplace kernel ## Using automatic sigma estimation (sigest) for RBF or laplace kernel ## Using automatic sigma estimation (sigest) for RBF or laplace kernel ## Using automatic sigma estimation (sigest) for RBF or laplace kernel ## Using automatic sigma estimation (sigest) for RBF or laplace kernel ## Using automatic sigma estimation (sigest) for RBF or laplace kernel ## Using automatic sigma estimation (sigest) for RBF or laplace kernel ## Using automatic sigma estimation (sigest) for RBF or laplace kernel ## Using automatic sigma estimation (sigest) for RBF or laplace kernel ## Using automatic sigma estimation (sigest) for RBF or laplace kernel ## Using automatic sigma estimation (sigest) for RBF or laplace kernel ## Using automatic sigma estimation (sigest) for RBF or laplace kernel ## Using automatic sigma estimation (sigest) for RBF or laplace kernel ## Using automatic sigma estimation (sigest) for RBF or laplace kernel ## Using automatic sigma estimation (sigest) for RBF or laplace kernel ## Using automatic sigma estimation (sigest) for RBF or laplace kernel ## Using automatic sigma estimation (sigest) for RBF or laplace kernel ## Using automatic sigma estimation (sigest) for RBF or laplace kernel ## Using automatic sigma estimation (sigest) for RBF or laplace kernel ## Using automatic sigma estimation (sigest) for RBF or laplace kernel ## Using automatic sigma estimation (sigest) for RBF or laplace kernel ## Using automatic sigma estimation (sigest) for RBF or laplace kernel ## Using automatic sigma estimation (sigest) for RBF or laplace kernel ## Using automatic sigma estimation (sigest) for RBF or laplace kernel ## Using automatic sigma estimation (sigest) for RBF or laplace kernel ## Using automatic sigma estimation (sigest) for RBF or laplace kernel ## Using automatic sigma estimation (sigest) for RBF or laplace kernel ## Using automatic sigma estimation (sigest) for RBF or laplace kernel ## Using automatic sigma estimation (sigest) for RBF or laplace kernel ## Using automatic sigma estimation (sigest) for RBF or laplace kernel ## Using automatic sigma estimation (sigest) for RBF or laplace kernel ## Using automatic sigma estimation (sigest) for RBF or laplace kernel ## Using automatic sigma estimation (sigest) for RBF or laplace kernel ## Using automatic sigma estimation (sigest) for RBF or laplace kernel ## Using automatic sigma estimation (sigest) for RBF or laplace kernel ## Using automatic sigma estimation (sigest) for RBF or laplace kernel ## Using automatic sigma estimation (sigest) for RBF or laplace kernel ## Using automatic sigma estimation (sigest) for RBF or laplace kernel ## Using automatic sigma estimation (sigest) for RBF or laplace kernel ## Using automatic sigma estimation (sigest) for RBF or laplace kernel ## Using automatic sigma estimation (sigest) for RBF or laplace kernel ## Using automatic sigma estimation (sigest) for RBF or laplace kernel ## Using automatic sigma estimation (sigest) for RBF or laplace kernel ## Using automatic sigma estimation (sigest) for RBF or laplace kernel ## Using automatic sigma estimation (sigest) for RBF or laplace kernel ## Using automatic sigma estimation (sigest) for RBF or laplace kernel ## Using automatic sigma estimation (sigest) for RBF or laplace kernel ## Using automatic sigma estimation (sigest) for RBF or laplace kernel ## Using automatic sigma estimation (sigest) for RBF or laplace kernel ## Using automatic sigma estimation (sigest) for RBF or laplace kernel ## Using automatic sigma estimation (sigest) for RBF or laplace kernel ## Using automatic sigma estimation (sigest) for RBF or laplace kernel ## Using automatic sigma estimation (sigest) for RBF or laplace kernel ## Using automatic sigma estimation (sigest) for RBF or laplace kernel ## Using automatic sigma estimation (sigest) for RBF or laplace kernel ## Using automatic sigma estimation (sigest) for RBF or laplace kernel ## Using automatic sigma estimation (sigest) for RBF or laplace kernel ## Using automatic sigma estimation (sigest) for RBF or laplace kernel ## Using automatic sigma estimation (sigest) for RBF or laplace kernel ## Using automatic sigma estimation (sigest) for RBF or laplace kernel ## Using automatic sigma estimation (sigest) for RBF or laplace kernel ## Using automatic sigma estimation (sigest) for RBF or laplace kernel ## Using automatic sigma estimation (sigest) for RBF or laplace kernel ## Using automatic sigma estimation (sigest) for RBF or laplace kernel ## Using automatic sigma estimation (sigest) for RBF or laplace kernel ## Using automatic sigma estimation (sigest) for RBF or laplace kernel ## Using automatic sigma estimation (sigest) for RBF or laplace kernel ## Using automatic sigma estimation (sigest) for RBF or laplace kernel ## Using automatic sigma estimation (sigest) for RBF or laplace kernel ## Using automatic sigma estimation (sigest) for RBF or laplace kernel ## Using automatic sigma estimation (sigest) for RBF or laplace kernel ## Using automatic sigma estimation (sigest) for RBF or laplace kernel ## Using automatic sigma estimation (sigest) for RBF or laplace kernel ## Using automatic sigma estimation (sigest) for RBF or laplace kernel ## Using automatic sigma estimation (sigest) for RBF or laplace kernel ## Using automatic sigma estimation (sigest) for RBF or laplace kernel ## Using automatic sigma estimation (sigest) for RBF or laplace kernel ## Using automatic sigma estimation (sigest) for RBF or laplace kernel ## Using automatic sigma estimation (sigest) for RBF or laplace kernel ## Using automatic sigma estimation (sigest) for RBF or laplace kernel ## Using automatic sigma estimation (sigest) for RBF or laplace kernel We can compare the performence against the logistic regression: plot(t(aucscore),t(aucscore2),xlab=&quot;Logistic&quot;,ylab=&quot;GP&quot;) Note that the results are similar. Dissapointingly similar. We have gone to the effort to utilise a far more powerful method yet the empircal results are no better. The power of GPs and other nonlinear approaches will not be clear unless the data is nonlinear. To illustrate this we will construct an artifical dataset in which low expression levels of a gene indicates no infection, with moderate levels indicating infection; very high levels of the gene, however, do not indicate infected status, but only arise artifically, due to e.g., inducible overexpression. For this dataset very high levels are thus labelled as uninfected. Below we construct this in silico dataset based loosley on the expression levels of AT3G44720. xtrain = D[,bestpredictor] ytrain = as.numeric(D$Class) ytrain[which(xtrain&gt;12.5)]=0 ytrain[which(xtrain&lt;10)]=0 ytrain = as.factor(ytrain) xpred = Dpred[,bestpredictor] ypred = as.numeric(Dpred$Class) ypred[which(xpred&gt;12.5)]=0 ypred[which(xpred&lt;10)]=0 ypred = as.factor(ypred) Let’s first fit a logistic model and visualise the result: mod_fit3 &lt;- train(y ~., data=data.frame(x = xtrain, y= as.factor(ytrain)), family=&quot;binomial&quot;, method = &quot;glm&quot;) plot(xtrain,as.numeric(ytrain)-1,xlab=&quot;Marker gene&quot;,ylab=&quot;Class&quot;) lines(seq(min(xtrain),max(xtrain),length=200),predict(mod_fit3,newdata=data.frame(x = seq(min(xtrain),max(xtrain),length=200),y= matrix(200,1,1)),type=&quot;prob&quot;)[,2]) mod_fit3$results$Accuracy ## [1] 0.8660466 We can see from the plot that the model fit is very poor. However, if we look at the accuracy (printed at the bottom) the result appears to be good. This is due to the skewed number of samples from each class: there are far more non infected samples than there are infected, which means that if the model predicts uninfected for every instance, it will be correct more than it’s incorrect. We can similary check the result on our test dataset: prob&lt;-predict(mod_fit3, newdata=data.frame(x =xpred, y = as.factor(ypred)), type=&quot;prob&quot;) pred &lt;- prediction(prob[,2], ypred) perf &lt;- performance(pred, measure = &quot;tpr&quot;, x.measure = &quot;fpr&quot;) auc &lt;- performance(pred, measure = &quot;auc&quot;) auc &lt;- auc@y.values[[1]] auc ## [1] 0.7104377 We can now instead attempt to take advantage the extra flexibility of GPs. mod_fit2 &lt;- gausspr(y~x,x=xtrain, y=ytrain, scaled = TRUE, type= NULL, kernel=&quot;rbfdot&quot;, kpar=&quot;automatic&quot;, variance.model = FALSE, tol=0.0005, cross=0, fit=TRUE) ## Using automatic sigma estimation (sigest) for RBF or laplace kernel plot(xtrain,as.numeric(ytrain)-1,xlab=&quot;Marker gene&quot;,ylab=&quot;Class&quot;) lines(seq(min(xtrain),max(xtrain),length=200),predict(mod_fit3,newdata=data.frame(x = seq(min(xtrain),max(xtrain),length=200),y= matrix(200,1,1)),type=&quot;prob&quot;)[,2]) lines(seq(min(xtrain),max(xtrain),length=200),predict(mod_fit2,seq(min(xtrain),max(xtrain),length=200), type=&quot;prob&quot;)[,2],col=&quot;red&quot;) prob2&lt;-kernlab::predict(mod_fit2, newdata=data.frame(x =xpred), type=&quot;probabilities&quot;) prob&lt;-predict(mod_fit2, xpred, type=&quot;probabilities&quot;) pred &lt;- prediction(prob2[,2], ypred) perf &lt;- performance(pred, measure = &quot;tpr&quot;, x.measure = &quot;fpr&quot;) auc &lt;- performance(pred, measure = &quot;auc&quot;) auc &lt;- auc@y.values[[1]] auc ## [1] 0.8193042 Boom! We can see that this is a much better model, both in terms of the model fit, which has found a nonlinear classifier, and in terms of the AUC score. 4.2.3 Other classification approaches. Quite often in ML we are interested in predicting class labels with maximum accuracy. That is, we are less concerned about intepreting what our function says about the system, and are only interested in our ability to predict \\(\\mathbf{y}*\\) at the new locations \\(\\mathbf{X}*\\). A variety of classifiers are availble in {caret}, including those based on random forests and support vector machines, as well as those based on neural networks. In the examples, below, we use some of these approaches to predict infection status from expression data. library(&quot;stepPlr&quot;) library(&quot;party&quot;) ## Loading required package: grid ## Loading required package: mvtnorm ## Loading required package: modeltools ## Loading required package: stats4 ## ## Attaching package: &#39;modeltools&#39; ## The following object is masked from &#39;package:kernlab&#39;: ## ## prior ## The following object is masked from &#39;package:plyr&#39;: ## ## empty ## The following object is masked from &#39;package:lme4&#39;: ## ## refit ## Loading required package: strucchange ## Loading required package: zoo ## ## Attaching package: &#39;zoo&#39; ## The following objects are masked from &#39;package:base&#39;: ## ## as.Date, as.Date.numeric ## Loading required package: sandwich library(&quot;kernlab&quot;) mod_fit1 &lt;- train(y ~ x, data=data.frame(x = D$AT3G44720, y = as.factor(D$Class)), method=&quot;plr&quot;) ## ## Convergence warning in plr: 6 #mod_fit2 &lt;- train(y ~ x, data=data.frame(x = D$AT3G44720, y = as.factor(D$Class)), method=&quot;cforest&quot;) mod_fit3 &lt;- train(y ~ x, data=data.frame(x = D$AT3G44720, y = as.factor(D$Class)), method=&quot;svmRadialWeights&quot;) Although we can run a variety of algorithms and check the accuracy of their predictions seperately, another approach would be to combine predicitons to achieve increased accuracy. More information on ensemble learning is available from CRAN. 4.3 Resources A variety of examples using {caret} to perform regression and classification have been implemented here. More comprehensive Gaussian process packages are available including the Matlab package GPML, and Python package GPy. GP code based on Tensorflow is also available in the form of GPflow and GPflowopt. Coming soon GPflow for R ======= ## Exercises Solutions to exercises can be found in appendix C. References "],
["dimensionality-reduction.html", "5 Dimensionality reduction 5.1 Linear Dimensionality Reduction 5.2 Nonlinear Dimensionality Reduction 5.3 Other dimensionality reduction techniques", " 5 Dimensionality reduction In machine learning, dimensionality reduction broadly refers to any modelling approach that reduces the number of variables in a dataset to a few highly informative or representative ones (see Figure 5.1). This is necessitated by the fact that large datasets with many variables are inherently difficult for humans to develop a clear intuition for. Dimensionality reduction is therefore an integral step in the analysis of large, complex biological datasets, allowing exploratory analyses and more intuitive visualisation that may aid interpretability. Figure 5.1: Example of a dimensionality reduction. Here we have a two-dimensional dataset embeded in a three-dimensional space (swiss roll dataset). In biological applications, systems-level measurements are typically used to decipher complex mechanisms. These include measurements of gene expression from collections of microarrays (Breeze et al. 2011,Windram et al. (2012),Lewis et al. (2015),Bechtold et al. (2016)) or RNA-sequencing experiments (Irie et al. 2015,Tang et al. (2015)) that provide quantitative measurments for tens-of-thousands of genes. Studies like these, based on bulk measurements (that is pooled material), provide observations for many variables (in this case many genes) but with relatively few samples e.g., few time points or conditions. The imbalance between the number of variables and the number of observations is referred to as large p, small n, and makes statistical analysis difficult. Dimensionality reduction techniques therefore prove to be a useful first step in any analysis, identifying potential structure that exists in the dataset or highlighting which (combinations of) variables are the most informative. The increasing prevalence of single cell RNA-sequencing (scRNA-seq) means the scale of datasets has shifted away from large p, small n, towards providing measurements of many variables but with a corresponding large number of observations (large n) albeit from potentially heterogeneous populations. scRNA-sequencing was largely driven by the need to investigate the transcrptomes of cells that were limited in quantity, such as embryonic cells, with early applications in mouse blastomeres (F. Tang et al. 2009). As of 2017, scRNA-seq experiments routinely generate datasets with tens to hundreds-of-thousands of cells (see e.g., (Svensson, Vento-Tormo, and Teichmann 2017)). Indeed, in 2016, the 10x Genomics million cell experiment provided sequencing for over 1.3 million cells taken from the cortex, hippocampus and ventricular zone of embryonic mice, and large international consortiums, such as the Human Cell Atlas aim to create a comprehensive maps of all cell types in the human body. A key goal when dealing with datasets of this magnitude is the identification of subpopulations of cells that may have gone undetected in bulk experiments; another, perhaps more ambitious task, aims to take advantage of any heterogeneity within the population in order to identify a temporal or mechanistic progression of developmental processes or disease. Of course, whilst dimensionality reduction allows humans to inspect the dataset manually, particularly when the data can be represented in two or three dimensions, we should keep in mind that humans are exceptionally good at identifying patterns in two or three dimensional data, even when no real structure exists (Figure 5.2. It is therefore useful to employ other statistical approaches to search for patterns in the reduced dimensional space. In this sense, dimensionality reduction forms an integral component in the analysis of complex datasets that will typically be combined a variety of machine learning techniques, such as classification, regression, and clustering. Figure 5.2: Humans are exceptionally good at identifying patterns in two and three-dimensional spaces - sometimes too good. To illustrate this, note the Great Britain shapped cloud in the image (presumably drifting away from an EU shaped cloud, not shown). More whimsical shaped clouds can also be seen if you have a spare afternoon. Golcar Matt/Weatherwatchers BBC News In this chapter we will explore two forms of dimensionality reduction, principle component analysis (PCA) and t-distributed stochastic neighbour embedding (tSNE), highlighting the advantages and potential pitfalls of each method. As an illustrative example, we will use these approaches to analyse single cell RNA-sequencing data of early human development. 5.1 Linear Dimensionality Reduction The most widely used form of dimensionality reduction is principle component analysis (PCA), which was introduced by Pearson in the early 1900’s (Pearson 1901), and independently rediscovered by Hotelling (Hotelling 1933). PCA has a long history of use in biological and ecological applications, with early use in population studies (Sforza and Edwards 1964), and later for the analysis of gene expression data (Vohradsky, Li, and Thompson 1997,Craig et al. (1997),Hilsenbeck et al. (1999)). PCA is not a dimensionality reduction technique per se, but an alternative way of representing the data that more naturally captures the variance in the system. Specifically, it finds a new co-ordinate system, so that the new “x-axis” (which is called the first principle component; PC1) is aligned along the direction of greatest variance, with an orthogonal “y-axis” aligned along the direction with second greatest variance (the second principle component; PC2), and so forth. At this stage there has been no inherent reduction in the dimensionality of the system, we have simply rotated the data around. To illustrate PCA we can repeat the analysis of (Ringnér 2008) using the dataset of (Saal et al. 2007) (GEO GSE5325). This dataset contains gene expression profiles for \\(105\\) breast tumour samples measured using Swegene Human 27K RAP UniGene188 arrays. Within the population of cells, (Ringnér 2008) focused on the expression of GATA3 and XBP1, whose expression was known to correlate with estrogen receptor status [^](Breast cancer cells may be estrogen receptor positive, ER\\(^+\\), or negative, ER\\(^-\\), indicating capacity to respond to estrogen signalling, which has impliations for treatment), representing a two dimensional system. A pre-processed dataset containing the expression levels for GATA3 and XBP1, and ER status, can be loaded into R using the code, below: D &lt;- read.csv(file = &quot;data/GSE5325/GSE5325_markers.csv&quot;, header = TRUE, sep = &quot;,&quot;, row.names=1) We can now plot the expression levels of GATA3 and XBP1 (rows 1 and 2) against one another to visualise the data in the two-dimensional space: plot(t(D[1,which(D[3,]==0)]),t(D[2,which(D[3,]==0)]),&#39;p&#39;,col=&#39;red&#39;, ylab=&quot;XBP1&quot;, xlab=&quot;GATA3&quot;,xlim=c(min(D[2,],na.rm = TRUE), max(D[2,],na.rm = TRUE)),ylim=c(min(D[1,],na.rm = TRUE), max(D[1,],na.rm = TRUE))) points(t(D[1,which(D[3,]==1)]),t(D[2,which(D[3,]==1)]),&#39;p&#39;,col=&#39;blue&#39;) We can perform PCA in R using the function. To do so, we must first filter out datapoints that have missing observations, as PCA does not, inherently, deal with missing observations: Dommitsamps &lt;- t(na.omit(t(D[,]))); #Get the subset of samples pca1 &lt;- prcomp(t(Dommitsamps[1:2,]), center = TRUE, scale=FALSE) ERexp &lt;- Dommitsamps[3,]; ER_neg &lt;- pca1$x[which(ERexp==0),] ER_pos &lt;- pca1$x[which(ERexp==1),] plot(ER_neg[,1],ER_neg[,2],&#39;p&#39;,col=&#39;red&#39;, xlab=&quot;PC1&quot;, ylab=&quot;PC2&quot;,xlim=c(-4.5, 4.2),ylim=c(-3, 2.5)) points(ER_pos[,1],ER_pos[,2],&#39;p&#39;,col=&#39;blue&#39;) Note that the has the option to centre and scale the data. That is, to normalise each variable to have a zero-mean and unit variance. This is particularly important when dealing with variables that may exist over very different scales. For example, for ecological datasets we may have variables that were measured in seconds with others measured in hours. Without normalisation there would appear to be much greater variance in the variable measured in seconds, potentially skewing the results. In general, when dealing with variables that are measured on similar scales (for example gene expression) it is not desirable to normalise the data. We can better visualise what the PCA has done by plotting the original data side-by-side with the transformed data (note that here we have plotted the negative of PC1). par(mfrow=c(1,2)) plot(t(D[1,which(D[3,]==0)]),t(D[2,which(D[3,]==0)]),&#39;p&#39;,col=&#39;red&#39;, ylab=&quot;XBP1&quot;, xlab=&quot;GATA3&quot;,xlim=c(min(D[2,],na.rm = TRUE), max(D[2,],na.rm = TRUE)),ylim=c(min(D[1,],na.rm = TRUE), max(D[1,],na.rm = TRUE))) points(t(D[1,which(D[3,]==1)]),t(D[2,which(D[3,]==1)]),&#39;p&#39;,col=&#39;blue&#39;) plot(-ER_neg[,1],ER_neg[,2],&#39;p&#39;,col=&#39;red&#39;, xlab=&quot;-PC1&quot;, ylab=&quot;PC2&quot;,xlim=c(-4.5, 4.2),ylim=c(-3, 2.5)) points(-ER_pos[,1],ER_pos[,2],&#39;p&#39;,col=&#39;blue&#39;) We can seen that we have simply rotated the original data, so that the greatest variance aligns along the x-axis and so forth. We can find out how much of the variance each of the principle components explains by looking at variable: par(mfrow=c(1,1)) barplot(((pca1$sdev)^2 / sum(pca1$sdev^2))*100, names.arg=c(&quot;PC1&quot;,&quot;PC2&quot;), ylab=&quot;% variance&quot;) Here we can see that PC1 explains the vast majority of the variance in the observations (for this example we should be able to see this by eye). The dimensionality reduction step of PCA occurs when we choose to discard the later PCs. Of course, by doing so we loose information about the system, but this may be an acceptable loss compared to the increased interpretability achieved by visualising the system in lower dimensions. In the example below, we follow from (Ringnér 2008), and visualise the data using only PC1. par(mfrow=c(1,1)) plot(-ER_neg[,1],matrix(-1, 1, length(ER_neg[,1])),&#39;p&#39;,col=&#39;red&#39;, xlab=&quot;PC1&quot;,xlim=c(-4, 3),ylim=c(-1.5,1.5),yaxt=&quot;n&quot;, ylab=&quot;&quot;) points(-ER_pos[,1],matrix(-1, 1, length(ER_pos[,1])),&#39;p&#39;,col=&#39;blue&#39;) points(-ER_neg[,1],matrix(1, 1, length(ER_neg[,1])),&#39;p&#39;,col=&#39;red&#39;, xlab=&quot;PC1&quot;,xlim=c(-4, 3)) points(-ER_pos[,1],matrix(0, 1, length(ER_pos[,1])),&#39;p&#39;,col=&#39;blue&#39;) axis(side = 2, at = seq(-1, 1, by = 1), labels = c(&quot;All&quot;,&quot;ER-&quot;,&quot;ER+&quot;)) So reducing the system down to one dimension appears to have done a good job at separating out the ER\\(^+\\) cells from the ER\\(^-\\) cells, suggesting that it may be of biological use. Precisely how many PCs to retain remains subjective. For visualisation purposed, it is typical to look at the first two or three only. However, when using PCA as an intermediate step within more complex workflows, more PCs are often retained e.g., by thresholding to a suitable level of explanatory variance. 5.1.1 Interpreting the Principle Component Axes In the original data, the individual axes had very obvious interpretations: the x-axis represented expression levels of GATA3 and the y-axis represented the expression level of XBP1. Other than indicating maximum variance, what does PC1 mean? The individual axes represent linear combinations of the expression of various genes. This may not be immediately intuitive, but we can get a feel by projecting the original axes (gene expression) onto the (reduced dimensional) co-ordinate system. genenames &lt;- c(&quot;GATA3&quot;,&quot;XBP1&quot;) plot(-pca1$rotation[,1],pca1$rotation[,2], type=&quot;n&quot;, xlim=c(-2, 2), ylim=c(-2, 2), xlab=&quot;PC1&quot;, ylab=&quot;PC2&quot;) text(-pca1$rotation[,1], pca1$rotation[,2], genenames, cex = .4) arrows(0, 0, x1 = -pca1$rotation[,1], y1 = -pca1$rotation[,2],length=0.1) In this particular case, we can see that both genes appear to be reasonably strongly associated with PC1. When dealing with much larger systems e.g., with more genes, we can, of course, project the original axes into the reduced dimensional space. In general this is particularly useful for identifying genes associated with particular PCs, and ultimately assigning a biological interpretation to the PCs. 5.1.2 Horseshoe effect Principle component analysis is a linear dimensionality reduction technique, and is not always appropriate for complex datasets, particularly when dealing with nonlinearities. To illustrate this, let’s consider an simulated expression set containing \\(8\\) genes, with \\(10\\) timepoints/conditions. We can represent this dataset in terms of a matrix: X &lt;- matrix( c(2,4,2,0,0,0,0,0,0,0, 0,2,4,2,0,0,0,0,0,0, 0,0,2,4,2,0,0,0,0,0, 0,0,0,2,4,2,0,0,0,0, 0,0,0,0,2,4,2,0,0,0, 0,0,0,0,0,2,4,2,0,0, 0,0,0,0,0,0,2,4,2,0, 0,0,0,0,0,0,0,2,4,2), nrow=8, ncol=10, byrow = TRUE) Or we can visualise by plotting a few of the genes: plot(1:10,X[1,],type=&quot;l&quot;,col=&quot;red&quot;,xlim=c(0, 14),xlab=&quot;Time&quot;,ylab=&quot;Expression&quot;) points(1:10,X[2,],type=&quot;l&quot;,col=&quot;blue&quot;) points(1:10,X[5,],type=&quot;l&quot;,col=&quot;black&quot;) legend(8, 4, legend=c(&quot;gene 1&quot;, &quot;gene 2&quot;, &quot;gene 5&quot;), col=c(&quot;red&quot;, &quot;blue&quot;, &quot;black&quot;),lty=1, cex=0.8) By eye, we see that the data can be separated out by a single direction: that is, we can order the data from time/condition 1 through to time/condition 10. Intuitively, then, the data can be represented by a single dimension. Let’s run PCA as we would normally, and visualise the result, plotting the first two PCs: pca2 &lt;- prcomp(t(X),center = TRUE,scale=FALSE) condnames = c(&#39;TP1&#39;,&#39;TP2&#39;,&#39;TP3&#39;,&#39;TP4&#39;,&#39;TP5&#39;,&#39;TP6&#39;,&#39;TP7&#39;,&#39;TP8&#39;,&#39;TP9&#39;,&#39;TP10&#39;) plot(pca2$x[,1:2],type=&quot;p&quot;,col=&quot;red&quot;,xlim=c(-5, 5),ylim=c(-5, 5)) text(pca2$x[,1:2]+0.5, condnames, cex = 0.7) We see that the PCA plot has placed the datapoints in a horseshoe shape, with condition/time point 1 very close to condition/time point 10. From the earlier plots of gene expression profiles we can see that the relationships between the various genes are not entirely straightforward. For example, gene 1 is initially correlated with gene 2, then negatively correlated, and finally uncorrelated, whilst no correlation exists between gene 1 and genes 5 - 8. These nonlinearities make it difficult for PCA which, in general, attempts to preserve large pairwise distances, leading to the well known horseshoe effect (Novembre and Stephens 2008,Reich, Price, and Patterson (2008)). These types of artefacts may be problematic when trying to interpret data, and due care must be given when these type of effects are seen. 5.1.3 PCA analysis of mammalian development Now that we have a feel for PCA and understand some of the basic commands we can apply it in a real setting. Here we will make use of preprocessed data taken from (L. Yan et al. 2013) (GEO GSE36552) and (F. Guo et al. 2015) (GEO GSE63818). The data from (L. Yan et al. 2013) represents single cell RNA-seq measurements from human embryos from the zygote stage (a single cell produced following fertilisation of an egg) through to the blastocyst stage (an embryo consisting of around 64 cells), as well as human embryonic stem cells (hESC; cells extracted from an early blsatocyst stage embryo and maintained in vitro). The dataset of (F. Guo et al. 2015) contains scRNA-seq data from human primordial germ cells (hPGCs), precursors of sperm or eggs that are specified early in the developing human embryo soon after implantation (around week 2-3 in humans), and somatic cells. Together, these datasets provide useful insights into early human development, and possible mechanisms for the specification of early cell types, such as PGCs. Preprocessed data contains \\(\\log_2\\) normalised counts for around \\(400\\) cells using \\(2957\\) marker genes can be found in the file . Note that the first line of data in the file is an indicator denoting cell type (-1 = ESC, 0 = pre-implantation, 1 = PGC, and 2 = somatic cell). The second row indicates the sex of the cell (0 = unknown/unlabelled, 1 = XX, 2 = XY), with the third row indicating capture time (-1 = ESC, 0 - 7 denotes various developmental stages from zygote to blastocyst, 8 - 13 indicates increasing times of embryo development from week 4 through to week 19). Exercise 5.1. First load in the expression data into R and plot some example expression patterns. Exercise 5.2. Use to perform PCA on the data. Exercise 5.3. Try plotting visualising the original axis. Can we identify any genes of interest that may be particularly important for PGCs? Exercise 5.4. Does the data separate well? Perform k-means cluster analysis on the data to see if we can identify distinct clusters. Exercise 5.5. Perform a differential expression analysis between blastocyst cells and the PGCs. 5.2 Nonlinear Dimensionality Reduction Whilst PCA is extremely useful for exploratory analysis, it is not always appropriate, particularly for datasets with nonlinearities. A large number of nonlinear dimensionality reduction techniques have therefore been developed. Perhaps the most commonly applied technique is t-distributed stochastic neighbour embedding (tSNE) (L. van der Maaten and Hinton 2008,L. van der Maaten (2009),Van der Maaten and Hinton (2012),Van Der Maaten (2014)). In general, tSNE attempts to take points in a high-dimensional space and find a faithful representation of those points in a lower-dimensional space. The SNE algorithm initially converts the high-dimensional Euclidean distances between datapoints into conditional probabilities. Here \\(p_{j|i}\\), indicates the probability that datapoint \\(x_i\\) would pick \\(x_j\\) as its neighbour if neighbours were picked in proportion to their probability density under a Gaussian centred at \\(x_i\\): \\(p_{j|i} = \\frac{\\exp(-|\\mathbf{x}_i - \\mathbf{x}_j|^2/2\\sigma_i^2)}{\\sum_{k\\neq l}\\exp(-|\\mathbf{x}_k - \\mathbf{x}_l|^2/2\\sigma_i^2)}\\) We can define a similar conditional probability for the datapoints in the reduced dimensional space, \\(y_j\\) and \\(y_j\\) as: \\(q_{j|i} = \\frac{\\exp(-|\\mathbf{y}_i - \\mathbf{y}_j|^2)}{\\sum_{k\\neq l}\\exp(-|\\mathbf{y}_k - \\mathbf{y}_l|^2)}\\). Natural extensions to this would instead use a Student-t distribution for the lower dimensional space. \\(q_{j|i} = \\frac{(1+|\\mathbf{y}_i - \\mathbf{y}_j|^2)^{-1}}{\\sum_{k\\neq l}(1+|\\mathbf{y}_i - \\mathbf{y}_j|^2)^{-1}}\\). If SNE has mapped points \\(\\mathbf{y}_i\\) and \\(\\mathbf{y}_j\\) faithfully, we have \\(p_{j|i} = q_{j|i}\\). We can define a similarity measure over these distribution based on the Kullback-Leibler-divergence: \\(C = \\sum KL(P_i||Q_i)= \\sum_i \\sum_j p_{i|j} \\log \\biggl{(} \\frac{p_{i|j}}{q_{i|j}} \\biggr{)}\\) If \\(p_{j|i} = q_{j|i}\\), that is, if our reduced dimensionality representation faithfully captures the higher dimensional data, this value will be equal to zero, otherwise it will be a positive number. We can attempt to minimise this value using gradient descent. Note that in many cases this lower dimensionality space can be initialised using PCA or other dimensionality reduction technique. The tSNE algorithm is implemented in R via the package. library(Rtsne) library(scatterplot3d) set.seed(12345) To get a feel for tSNE we will first generate some artificial data. In this case we generate two different groups that exist in a 3-dimensional space. We choose these groups to be Gaussian distributed, with different means and variances: D1 &lt;- matrix( rnorm(5*3,mean=0,sd=1), 100, 3) D2 &lt;- matrix( rnorm(5*3,mean=5,sd=3), 100, 3) G1 &lt;- matrix( 1, 100, 1) G2 &lt;- matrix( 2, 100, 1) D3 &lt;- rbind(D1,D2) G3 &lt;- rbind(G1,G2) colors &lt;- c(&quot;red&quot;, &quot;blue&quot;) colors &lt;- colors[G3] scatterplot3d(D3,color=colors, main=&quot;3D Scatterplot&quot;,xlab=&quot;x&quot;,ylab=&quot;y&quot;,zlab=&quot;z&quot;) We can run tSNE on this dataset and try to condense the data down from a three-dimensional to a two-dimensional representation. Unlike PCA, which has no real free parameters, tSNE has a variety of parameters that need to be set. First, we have the perplexity parameter which, in essence, balances local and global aspects of the data. For low values of perplexity, the algorithm will tend to entirely focus on keeping datapoints locally together. tsne_model_1 &lt;- Rtsne(as.matrix(D3), check_duplicates=FALSE, pca=TRUE, perplexity=10, theta=0.5, dims=2) y1 &lt;- tsne_model_1$Y[which(D[1,]==-1),1:2] tsne_model_1 &lt;- Rtsne(as.matrix(D3), check_duplicates=FALSE, pca=TRUE, perplexity=10, theta=0.5, dims=2) plot(tsne_model_1$Y[1:100,1:2],type=&quot;p&quot;,col=&quot;red&quot;,xlim=c(-45, 45),ylim=c(-45, 45),xlab=&quot;tSNE1&quot;,ylab=&quot;tSNE1&quot;) points(tsne_model_1$Y[101:200,1:2],type=&quot;p&quot;,col=&quot;blue&quot;) Note that here we have set the perplexity parameter reasonably low, and tSNE appears to have identified a lot of local structure that (we know) doesn’t exist. Let’s try again using a larger value for the perplexity parameter. y1 &lt;- tsne_model_1$Y[which(D[1,]==-1),1:2] tsne_model_1 &lt;- Rtsne(as.matrix(D3), check_duplicates=FALSE, pca=TRUE, perplexity=50, theta=0.5, dims=2) plot(tsne_model_1$Y[1:100,1:2],type=&quot;p&quot;,col=&quot;red&quot;,xlim=c(-45, 45),ylim=c(-45, 45),xlab=&quot;tSNE1&quot;,ylab=&quot;tSNE2&quot;) points(tsne_model_1$Y[101:200,1:2],type=&quot;p&quot;,col=&quot;blue&quot;) This appears to have done a better job of representing the data in a two-dimensional space. 5.2.1 Nonlinear warping In our previous example we showed that if the perplexity parameter was correctly set, tSNE seperated out the two populations very well. If we plot the original data next to the tSNE reduced dimensionality represention, however, we will notice something interesting: par(mfrow=c(1,2)) scatterplot3d(D3,color=colors, main=&quot;3D Scatterplot&quot;,xlab=&quot;x&quot;,ylab=&quot;y&quot;,zlab=&quot;z&quot;) plot(tsne_model_1$Y[1:100,1:2],type=&quot;p&quot;,col=&quot;red&quot;,xlim=c(-45, 45),ylim=c(-45, 45),xlab=&quot;tSNE1&quot;, ylab=&quot;tSNE2&quot;) points(tsne_model_1$Y[101:200,1:2],type=&quot;p&quot;,col=&quot;blue&quot;) Whilst in the origianl data the two groups had very different variances, in the reduced dimensionality representation they appeared to show a similar spread. This is down to tSNEs ability to represent nonlinearities, and the algorithm performs different transformations on different regions. This is, of course, important to keep in mind: the spread in a tSNE output are not always indicative of the level of heterogeneity in the data. 5.2.2 Stochasticity A final important point to note is that tSNE is stochastic in nature. Unlike PCA which, for the same dataset, will always yield the same result, if you run tSNE twice you will likely find different results. We can illustrate this below, by running tSNE again for perplexity \\(30\\), and plotting the results alongside the previous ones. set.seed(123456) tsne_model_1 &lt;- Rtsne(as.matrix(D3), check_duplicates=FALSE, pca=TRUE, perplexity=30, theta=0.5, dims=2) tsne_model_2 &lt;- Rtsne(as.matrix(D3), check_duplicates=FALSE, pca=TRUE, perplexity=30, theta=0.5, dims=2) par(mfrow=c(1,2)) plot(tsne_model_1$Y[1:100,1:2],type=&quot;p&quot;,col=&quot;red&quot;,xlim=c(-45, 45),ylim=c(-45, 45),xlab=&quot;tSNE1&quot;,ylab=&quot;tSNE2&quot;) points(tsne_model_1$Y[101:200,1:2],type=&quot;p&quot;,col=&quot;blue&quot;) plot(tsne_model_2$Y[1:100,1:2],type=&quot;p&quot;,col=&quot;red&quot;,xlim=c(-45, 45),ylim=c(-45, 45),xlab=&quot;tSNE1&quot;,ylab=&quot;tSNE2&quot;) points(tsne_model_2$Y[101:200,1:2],type=&quot;p&quot;,col=&quot;blue&quot;) Note that this stochasticity, itself, may be a useful property, allowing us to gauge robustness of our biological interpretations. A comprehensive blog discussing the various pitfalls of tSNE is available here. 5.2.3 Analysis of mammalian development In earlier sections we used PCA to analyse scRNA-seq datasets of early human embryo development. In general PCA seemed adept at picking out different cell types and idetifying putative regulators associated with those cell types. We will now use tSNE to analyse the same data. Excercise 8.6. Load in the single cell dataset and run tSNE. Excercise 8.7.Note that cells labelled as pre-implantation actually consists of a variety of cells, from oocytes through to blastocyst stage. Take a look at the pre-implantation cells only using tSNE. Hint: a more refined categorisation of the developmental stage of pre-implantation cells can be found by looking at the developmental time variable (0=oocyte, 1=zygote, 2=2C, 3=4C, 4=8C, 5=Morula, 6=blastocyst). Try plotting the data from tSNE colouring the data according to developmental stage. Excercise 8.8. How do pre-implantation cells cluster in tSNE? 5.3 Other dimensionality reduction techniques A large number of alternative dimensionality reduction techniques exist with corresponding implementation in R. These include probabilistic extensions to PCA pcaMethods, as well as other nonlinear dimensionality reduction techniques Isomap, as well as those based on Gaussian Processes (GPLVM; Lawrence 2004). Other packages such as kernlab provide a general suite of tools for dimensionality reduction. Solutions to exercises can be found in appendix D. References "],
["clustering.html", "6 Clustering 6.1 Introduction 6.2 Distance metrics 6.3 Hierarchic agglomerative 6.4 K-means 6.5 DBSCAN 6.6 Evaluating cluster quality 6.7 Exercises", " 6 Clustering 6.1 Introduction Clustering attempts to find groups (clusters) of similar objects. The members of a cluster should be more similar to each other, than to objects in other clusters. Clustering algorithms aim to minimize intra-cluster variation and maximize inter-cluster variation. Methods of clustering can be broadly divided into two types: Hierarchic techniques produce dendrograms (trees) through a process of division or agglomeration. Partitioning algorithms divide objects into non-overlapping subsets (examples include k-means and DBSCAN) Figure 6.1: Example clusters. A, blobs; B, aggregation (Gionis, Mannila, and Tsaparas 2007); C, noisy moons; D, different density; E, anisotropic distributions; F, no structure. 6.2 Distance metrics Various distance metrics can be used with clustering algorithms. We will use Euclidean distance in the examples and exercises in this chapter. \\[\\begin{equation} distance\\left(p,q\\right)=\\sqrt{\\sum_{i=1}^{n} (p_i-q_i)^2} \\tag{6.1} \\end{equation}\\] Figure 6.2: Euclidean distance. Figure 6.3: Euclidean distance. 6.3 Hierarchic agglomerative Figure 6.4: Building a dendrogram using hierarchic agglomerative clustering. 6.3.1 Linkage algorithms Table 6.1: Example distance matrix A B C D B 2 C 6 5 D 10 10 5 E 9 8 3 4 Single linkage - nearest neighbours linkage Complete linkage - furthest neighbours linkage Average linkage - UPGMA (Unweighted Pair Group Method with Arithmetic Mean) Table 6.2: Merge distances for objects in the example distance matrix using three different linkage methods. Groups Single Complete Average A,B,C,D,E 0 0 0.0 (A,B),C,D,E 2 2 2.0 (A,B),(C,E),D 3 3 3.0 (A,B)(C,D,E) 4 5 4.5 (A,B,C,D,E) 5 10 8.0 Figure 6.5: Dendrograms for the example distance matrix using three different linkage methods. 6.3.2 Example: clustering synthetic data sets 6.3.2.1 Step-by-step instructions Load required packages. library(RColorBrewer) library(dendextend) ## ## --------------------- ## Welcome to dendextend version 1.8.0 ## Type citation(&#39;dendextend&#39;) for how to cite the package. ## ## Type browseVignettes(package = &#39;dendextend&#39;) for the package vignette. ## The github page is: https://github.com/talgalili/dendextend/ ## ## Suggestions and bug-reports can be submitted at: https://github.com/talgalili/dendextend/issues ## Or contact: &lt;tal.galili@gmail.com&gt; ## ## To suppress this message use: suppressPackageStartupMessages(library(dendextend)) ## --------------------- ## ## Attaching package: &#39;dendextend&#39; ## The following object is masked from &#39;package:ggdendro&#39;: ## ## theme_dendro ## The following object is masked from &#39;package:stats&#39;: ## ## cutree library(ggplot2) library(GGally) Retrieve a palette of eight colours. cluster_colours &lt;- brewer.pal(8,&quot;Dark2&quot;) Read in data for blobs example. blobs &lt;- read.csv(&quot;data/example_clusters/blobs.csv&quot;, header=F) Create distance matrix using Euclidean distance metric. d &lt;- dist(blobs[,1:2]) Perform hierarchical clustering using the average agglomeration method and convert the result to an object of class dendrogram. A dendrogram object can be edited using the advanced features of the dendextend package. dend &lt;- as.dendrogram(hclust(d, method=&quot;average&quot;)) Cut the tree into three clusters clusters &lt;- cutree(dend,3,order_clusters_as_data=F) The vector clusters contains the cluster membership (in this case 1, 2 or 3) of each observation (data point) in the order they appear on the dendrogram. We can use this vector to colour the branches of the dendrogram by cluster. dend &lt;- color_branches(dend, clusters=clusters, col=cluster_colours[1:3]) We can use the labels function to annotate the leaves of the dendrogram. However, it is not possible to create legible labels for the 1,500 leaves in our example dendrogram, so we will set the label for each leaf to an empty string. labels(dend) &lt;- rep(&quot;&quot;, length(blobs[,1])) If we want to plot the dendrogram using ggplot, we must convert it to an object of class ggdend. ggd &lt;- as.ggdend(dend) The nodes attribute of ggd is a data.frame of parameters related to the plotting of dendogram nodes. The nodes data.frame contains some NAs which will generate warning messages when ggd is processed by ggplot. Since we are not interested in annotating dendrogram nodes, the easiest option here is to delete all of the rows of nodes. ggd$nodes &lt;- ggd$nodes[!(1:length(ggd$nodes[,1])),] We can use the cluster membership of each observation contained in the vector clusters to assign colours to the data points of a scatterplot. However, first we need to reorder the vector so that the cluster memberships are in the same order that the observations appear in the data.frame of observations. Fortunately the names of the elements of the vector are the indices of the observations in the data.frame and so reordering can be accomplished in one line. clusters &lt;- clusters[order(as.numeric(names(clusters)))] We are now ready to plot a dendrogram and scatterplot. We will use the ggmatrix function from the GGally package to place the plots side-by-side. plotList &lt;- list(ggplot(ggd), ggplot(blobs, aes(V1,V2)) + geom_point(col=cluster_colours[clusters], size=0.2) ) pm &lt;- ggmatrix( plotList, nrow=1, ncol=2, showXAxisPlotLabels = F, showYAxisPlotLabels = F, xAxisLabels=c(&quot;dendrogram&quot;, &quot;scatter plot&quot;) ) + theme_bw() pm Figure 6.6: Hierarchical clustering of the blobs data set. 6.3.2.2 Clustering of other synthetic data sets aggregation &lt;- read.table(&quot;data/example_clusters/aggregation.txt&quot;) noisy_moons &lt;- read.csv(&quot;data/example_clusters/noisy_moons.csv&quot;, header=F) diff_density &lt;- read.csv(&quot;data/example_clusters/different_density.csv&quot;, header=F) aniso &lt;- read.csv(&quot;data/example_clusters/aniso.csv&quot;, header=F) no_structure &lt;- read.csv(&quot;data/example_clusters/no_structure.csv&quot;, header=F) hclust_plots &lt;- function(data_set, n){ d &lt;- dist(data_set[,1:2]) dend &lt;- as.dendrogram(hclust(d, method=&quot;average&quot;)) clusters &lt;- cutree(dend,n,order_clusters_as_data=F) dend &lt;- color_branches(dend, clusters=clusters, col=cluster_colours[1:n]) clusters &lt;- clusters[order(as.numeric(names(clusters)))] labels(dend) &lt;- rep(&quot;&quot;, length(data_set[,1])) ggd &lt;- as.ggdend(dend) ggd$nodes &lt;- ggd$nodes[!(1:length(ggd$nodes[,1])),] plotPair &lt;- list(ggplot(ggd), ggplot(data_set, aes(V1,V2)) + geom_point(col=cluster_colours[clusters], size=0.2)) return(plotPair) } plotList &lt;- c( hclust_plots(aggregation, 7), hclust_plots(noisy_moons, 2), hclust_plots(diff_density, 2), hclust_plots(aniso, 3), hclust_plots(no_structure, 3) ) pm &lt;- ggmatrix( plotList, nrow=5, ncol=2, showXAxisPlotLabels = F, showYAxisPlotLabels = F, xAxisLabels=c(&quot;dendrogram&quot;, &quot;scatter plot&quot;), yAxisLabels=c(&quot;aggregation&quot;, &quot;noisy moons&quot;, &quot;different density&quot;, &quot;anisotropic&quot;, &quot;no structure&quot;) ) + theme_bw() pm Figure 6.7: Hierarchical clustering of synthetic data-sets. 6.3.3 Example: gene expression profiling of human tissues 6.3.3.1 Basics Load required libraries library(RColorBrewer) library(dendextend) Load data load(&quot;data/tissues_gene_expression/tissuesGeneExpression.rda&quot;) Inspect data table(tissue) ## tissue ## cerebellum colon endometrium hippocampus kidney liver ## 38 34 15 31 39 26 ## placenta ## 6 dim(e) ## [1] 22215 189 Compute distance between each sample d &lt;- dist(t(e)) perform hierarchical clustering hc &lt;- hclust(d, method=&quot;average&quot;) plot(hc, labels=tissue, cex=0.5, hang=-1, xlab=&quot;&quot;, sub=&quot;&quot;) Figure 6.8: Clustering of tissue samples based on gene expression profiles. 6.3.3.2 Colour labels The dendextend library can be used to plot dendrogram with colour labels tissue_type &lt;- unique(tissue) dend &lt;- as.dendrogram(hc) dend_colours &lt;- brewer.pal(length(unique(tissue)),&quot;Dark2&quot;) names(dend_colours) &lt;- tissue_type labels(dend) &lt;- tissue[order.dendrogram(dend)] labels_colors(dend) &lt;- dend_colours[tissue][order.dendrogram(dend)] labels_cex(dend) = 0.5 plot(dend, horiz=T) Figure 6.9: Clustering of tissue samples based on gene expression profiles with labels coloured by tissue type. 6.3.3.3 Defining clusters by cutting tree Define clusters by cutting tree at a specific height plot(dend, horiz=T) abline(v=125, lwd=2, lty=2, col=&quot;blue&quot;) Figure 6.10: Clusters found by cutting tree at a height of 125 hclusters &lt;- cutree(dend, h=125) table(tissue, cluster=hclusters) ## cluster ## tissue 1 2 3 4 5 6 ## cerebellum 0 36 0 0 2 0 ## colon 0 0 34 0 0 0 ## endometrium 15 0 0 0 0 0 ## hippocampus 0 31 0 0 0 0 ## kidney 37 0 0 0 2 0 ## liver 0 0 0 24 2 0 ## placenta 0 0 0 0 0 6 Select a specific number of clusters. plot(dend, horiz=T) abline(v = heights_per_k.dendrogram(dend)[&quot;8&quot;], lwd = 2, lty = 2, col = &quot;blue&quot;) Figure 6.11: Selection of eight clusters from the dendogram hclusters &lt;- cutree(dend, k=8) table(tissue, cluster=hclusters) ## cluster ## tissue 1 2 3 4 5 6 7 8 ## cerebellum 0 31 0 0 2 0 5 0 ## colon 0 0 34 0 0 0 0 0 ## endometrium 0 0 0 0 0 15 0 0 ## hippocampus 0 31 0 0 0 0 0 0 ## kidney 37 0 0 0 2 0 0 0 ## liver 0 0 0 24 2 0 0 0 ## placenta 0 0 0 0 0 0 0 6 6.3.3.4 Heatmap Base R provides a heatmap function, but we will use the more advanced heatmap.2 from the gplots package. library(gplots) ## ## Attaching package: &#39;gplots&#39; ## The following object is masked from &#39;package:stats&#39;: ## ## lowess Define a colour palette (also known as a lookup table). heatmap_colours &lt;- colorRampPalette(brewer.pal(9, &quot;PuBuGn&quot;))(100) Calculate the variance of each gene. geneVariance &lt;- apply(e,1,var) Find the row numbers of the 40 genes with the highest variance. idxTop40 &lt;- order(-geneVariance)[1:40] Define colours for tissues. tissueColours &lt;- palette(brewer.pal(8, &quot;Dark2&quot;))[as.numeric(as.factor(tissue))] Plot heatmap. heatmap.2(e[idxTop40,], labCol=tissue, trace=&quot;none&quot;, ColSideColors=tissueColours, col=heatmap_colours) Figure 6.12: Heatmap of the expression of the 40 genes with the highest variance. 6.4 K-means 6.4.1 Algorithm Pseudocode for the K-means algorithm randomly choose k objects as initial centroids while true: 1. create k clusters by assigning each object to closest centroid 2. compute k new centroids by averaging the objects in each cluster 3. if none of the centroids differ from the previous iteration: return the current set of clusters Figure 6.13: Iterations of the k-means algorithm The default setting of the kmeans function is to perform a maximum of 10 iterations and if the algorithm fails to converge a warning is issued. The maximum number of iterations is set with the argument iter.max. 6.4.2 Choosing initial cluster centres library(RColorBrewer) point_shapes &lt;- c(15,17,19) point_colours &lt;- brewer.pal(3,&quot;Dark2&quot;) point_size = 1.5 center_point_size = 8 blobs &lt;- as.data.frame(read.csv(&quot;data/example_clusters/blobs.csv&quot;, header=F)) good_centres &lt;- as.data.frame(matrix(c(2,8,7,3,12,7), ncol=2, byrow=T)) bad_centres &lt;- as.data.frame(matrix(c(13,13,8,12,2,2), ncol=2, byrow=T)) good_result &lt;- kmeans(blobs[,1:2], centers=good_centres) bad_result &lt;- kmeans(blobs[,1:2], centers=bad_centres) plotList &lt;- list( ggplot(blobs, aes(V1,V2)) + geom_point(col=point_colours[good_result$cluster], shape=point_shapes[good_result$cluster], size=point_size) + geom_point(data=good_centres, aes(V1,V2), shape=3, col=&quot;black&quot;, size=center_point_size) + theme_bw(), ggplot(blobs, aes(V1,V2)) + geom_point(col=point_colours[bad_result$cluster], shape=point_shapes[bad_result$cluster], size=point_size) + geom_point(data=bad_centres, aes(V1,V2), shape=3, col=&quot;black&quot;, size=center_point_size) + theme_bw() ) pm &lt;- ggmatrix( plotList, nrow=1, ncol=2, showXAxisPlotLabels = T, showYAxisPlotLabels = T, xAxisLabels=c(&quot;A&quot;, &quot;B&quot;) ) + theme_bw() pm Figure 6.14: Initial centres determine clusters. The starting centres are shown as crosses. A, real clusters found; B, convergence to a local minimum. Convergence to a local minimum can be avoided by starting the algorithm multiple times, with different random centres. The nstart argument to the k-means function can be used to specify the number of random sets and optimal solution will be selected automatically. 6.4.3 Choosing k point_colours &lt;- brewer.pal(9,&quot;Set1&quot;) k &lt;- 1:9 res &lt;- lapply(k, function(i){kmeans(blobs[,1:2], i, nstart=50)}) plotList &lt;- lapply(k, function(i){ ggplot(blobs, aes(V1, V2)) + geom_point(col=point_colours[res[[i]]$cluster], size=1) + geom_point(data=as.data.frame(res[[i]]$centers), aes(V1,V2), shape=3, col=&quot;black&quot;, size=5) + annotate(&quot;text&quot;, x=2, y=13, label=paste(&quot;k=&quot;, i, sep=&quot;&quot;), size=8, col=&quot;black&quot;) + theme_bw() } ) pm &lt;- ggmatrix( plotList, nrow=3, ncol=3, showXAxisPlotLabels = T, showYAxisPlotLabels = T ) + theme_bw() pm Figure 6.15: K-means clustering of the blobs data set using a range of values of k from 1-9. Cluster centres indicated with a cross. tot_withinss &lt;- sapply(k, function(i){res[[i]]$tot.withinss}) qplot(k, tot_withinss, geom=c(&quot;point&quot;, &quot;line&quot;), ylab=&quot;Total within-cluster sum of squares&quot;) + theme_bw() Figure 6.16: Variance within the clusters. Total within-cluster sum of squares plotted against k. N.B. we have set nstart=50 to run the algorithm 50 times, starting from different, random sets of centroids. 6.4.4 Example: clustering synthetic data sets Let’s see how k-means performs on the other toy data sets. First we will define some variables and functions we will use in the analysis of all data sets. k=1:9 point_shapes &lt;- c(15,17,19,5,6,0,1) point_colours &lt;- brewer.pal(7,&quot;Dark2&quot;) point_size = 1.5 center_point_size = 8 plot_tot_withinss &lt;- function(kmeans_output){ tot_withinss &lt;- sapply(k, function(i){kmeans_output[[i]]$tot.withinss}) qplot(k, tot_withinss, geom=c(&quot;point&quot;, &quot;line&quot;), ylab=&quot;Total within-cluster sum of squares&quot;) + theme_bw() } plot_clusters &lt;- function(data_set, kmeans_output, num_clusters){ ggplot(data_set, aes(V1,V2)) + geom_point(col=point_colours[kmeans_output[[num_clusters]]$cluster], shape=point_shapes[kmeans_output[[num_clusters]]$cluster], size=point_size) + geom_point(data=as.data.frame(kmeans_output[[num_clusters]]$centers), aes(V1,V2), shape=3,col=&quot;black&quot;,size=center_point_size) + theme_bw() } 6.4.4.1 Aggregation aggregation &lt;- as.data.frame(read.table(&quot;data/example_clusters/aggregation.txt&quot;)) res &lt;- lapply(k, function(i){kmeans(aggregation[,1:2], i, nstart=50)}) plot_tot_withinss(res) Figure 6.17: K-means clustering of the aggregation data set: variance within clusters. plotList &lt;- list( plot_clusters(aggregation, res, 3), plot_clusters(aggregation, res, 7) ) pm &lt;- ggmatrix( plotList, nrow=1, ncol=2, showXAxisPlotLabels = T, showYAxisPlotLabels = T, xAxisLabels=c(&quot;k=3&quot;, &quot;k=7&quot;) ) + theme_bw() pm Figure 6.18: K-means clustering of the aggregation data set: scatterplots of clusters for k=3 and k=7. Cluster centres indicated with a cross. 6.4.4.2 Noisy moons noisy_moons &lt;- read.csv(&quot;data/example_clusters/noisy_moons.csv&quot;, header=F) res &lt;- lapply(k, function(i){kmeans(noisy_moons[,1:2], i, nstart=50)}) plot_tot_withinss(res) Figure 6.19: K-means clustering of the noisy moons data set: variance within clusters. plot_clusters(noisy_moons, res, 2) Figure 6.20: K-means clustering of the noisy moons data set: scatterplot of clusters for k=2. Cluster centres indicated with a cross. 6.4.4.3 Different density diff_density &lt;- as.data.frame(read.csv(&quot;data/example_clusters/different_density.csv&quot;, header=F)) res &lt;- lapply(k, function(i){kmeans(diff_density[,1:2], i, nstart=50)}) ## Warning: did not converge in 10 iterations ## Warning: did not converge in 10 iterations Failure to converge, so increase number of iterations. res &lt;- lapply(k, function(i){kmeans(diff_density[,1:2], i, iter.max=20, nstart=50)}) plot_tot_withinss(res) Figure 6.21: K-means clustering of the different density distributions data set: variance within clusters. plot_clusters(diff_density, res, 2) Figure 6.22: K-means clustering of the different density distributions data set: scatterplots of clusters for k=2 and k=3. Cluster centres indicated with a cross. 6.4.4.4 Anisotropic distributions aniso &lt;- as.data.frame(read.csv(&quot;data/example_clusters/aniso.csv&quot;, header=F)) res &lt;- lapply(k, function(i){kmeans(aniso[,1:2], i, nstart=50)}) plot_tot_withinss(res) Figure 6.23: K-means clustering of the anisotropic distributions data set: variance within clusters. plotList &lt;- list( plot_clusters(aniso, res, 2), plot_clusters(aniso, res, 3) ) pm &lt;- ggmatrix( plotList, nrow=1, ncol=2, showXAxisPlotLabels = T, showYAxisPlotLabels = T, xAxisLabels=c(&quot;k=2&quot;, &quot;k=3&quot;) ) + theme_bw() pm Figure 6.24: K-means clustering of the anisotropic distributions data set: scatterplots of clusters for k=2 and k=3. Cluster centres indicated with a cross. 6.4.4.5 No structure no_structure &lt;- as.data.frame(read.csv(&quot;data/example_clusters/no_structure.csv&quot;, header=F)) res &lt;- lapply(k, function(i){kmeans(no_structure[,1:2], i, nstart=50)}) plot_tot_withinss(res) Figure 6.25: K-means clustering of the data set with no structure: variance within clusters. plot_clusters(no_structure, res, 4) Figure 6.26: K-means clustering of the data set with no structure: scatterplot of clusters for k=4. Cluster centres indicated with a cross. 6.4.5 Example: gene expression profiling of human tissues Let’s return to the data on gene expression of human tissues. Load data load(&quot;data/tissues_gene_expression/tissuesGeneExpression.rda&quot;) As we saw earlier, the data set contains expression levels for over 22,000 transcripts in seven tissues. table(tissue) ## tissue ## cerebellum colon endometrium hippocampus kidney liver ## 38 34 15 31 39 26 ## placenta ## 6 dim(e) ## [1] 22215 189 First we will examine the total intra-cluster variance with different values of k. Our data-set is fairly large, so clustering it for several values or k and with multiple random starting centres is computationally quite intensive. Fortunately the task readily lends itself to parallelization; we can assign the analysis of each ‘k’ to a different processing core. As we have seen in the previous chapters on supervised learning, caret has parallel processing built in and we simply have to load a package for multicore processing, such as doMC, and then register the number of cores we would like to use. Running kmeans in parallel is slightly more involved, but still very easy. We will start by loading doMC and registering all available cores: library(doMC) ## Loading required package: foreach ## Loading required package: iterators ## Loading required package: parallel registerDoMC(detectCores()) To find out how many cores we have registered we can use: getDoParWorkers() ## [1] 8 Instead of using the lapply function to vectorize our code, we will instead use the parallel equivalent, foreach. Like lapply, foreach returns a list by default. For this example we have set a seed, rather than generate a random number, for the sake of reproducibility. Ordinarily we would omit set.seed(42) and .options.multicore=list(set.seed=FALSE). k&lt;-1:15 set.seed(42) res_k_15 &lt;- foreach( i=k, .options.multicore=list(set.seed=FALSE)) %dopar% kmeans(t(e), i, nstart=10) plot_tot_withinss(res_k_15) Figure 6.27: K-means clustering of human tissue gene expression: variance within clusters. There is no obvious elbow, but since we know that there are seven tissues in the data set we will try k=7. table(tissue, res_k_15[[7]]$cluster) ## ## tissue 1 2 3 4 5 6 7 ## cerebellum 0 0 0 33 0 0 5 ## colon 0 0 0 0 0 34 0 ## endometrium 0 0 0 0 15 0 0 ## hippocampus 0 0 0 0 0 0 31 ## kidney 0 0 39 0 0 0 0 ## liver 26 0 0 0 0 0 0 ## placenta 0 6 0 0 0 0 0 The analysis has found a distinct cluster for each tissue and therefore performed slightly better than the earlier hierarchical clustering analysis, which placed endometrium and kidney observations in the same cluster. To visualize the result in a 2D scatter plot we first need to apply dimensionality reduction. We will use principal component analysis (PCA), which was described in chapter 5. pca &lt;- prcomp(t(e)) ggplot(data=as.data.frame(pca$x), aes(PC1,PC2)) + geom_point(col=brewer.pal(7,&quot;Dark2&quot;)[res_k_15[[7]]$cluster], shape=c(49:55)[res_k_15[[7]]$cluster], size=5) + theme_bw() Figure 6.28: K-means clustering of human gene expression (k=7): scatterplot of first two principal components. 6.5 DBSCAN Density-based spatial clustering of applications with noise 6.5.1 Algorithm Abstract DBSCAN algorithm in pseudocode (Schubert et al. 2017) 1 Compute neighbours of each point and identify core points // Identify core points 2 Join neighbouring core points into clusters // Assign core points 3 foreach non-core point do Add to a neighbouring core point if possible // Assign border points Otherwise, add to noise // Assign noise points Figure 6.29: Illustration of the DBSCAN algorithm. By Chire (Own work) [CC BY-SA 3.0 (http://creativecommons.org/licenses/by-sa/3.0)], via Wikimedia Commons. The method requires two parameters; MinPts that is the minimum number of samples in any cluster; Eps that is the maximum distance of the sample to at least one other sample within the same cluster. This algorithm works on a parametric approach. The two parameters involved in this algorithm are: e (eps) is the radius of our neighborhoods around a data point p. minPts is the minimum number of data points we want in a neighborhood to define a cluster. 6.5.2 Implementation in R DBSCAN is implemented in two R packages: dbscan and fpc. We will use the package dbscan, because it is significantly faster and can handle larger data sets than fpc. The function has the same name in both packages and so if for any reason both packages have been loaded into our current workspace, there is a danger of calling the wrong implementation. To avoid this we can specify the package name when calling the function, e.g.: dbscan::dbscan We load the dbscan package in the usual way: library(dbscan) 6.5.3 Choosing parameters The algorithm only needs parameteres eps and minPts. Read in data and use kNNdist function from dbscan package to plot the distances of the 10-nearest neighbours for each observation (figure 6.30). blobs &lt;- read.csv(&quot;data/example_clusters/blobs.csv&quot;, header=F) kNNdistplot(blobs[,1:2], k=10) abline(h=0.6) Figure 6.30: 10-nearest neighbour distances for the blobs data set res &lt;- dbscan::dbscan(blobs[,1:2], eps=0.6, minPts = 10) table(res$cluster) ## ## 0 1 2 3 ## 43 484 486 487 ggplot(blobs, aes(V1,V2)) + geom_point(col=brewer.pal(8,&quot;Dark2&quot;)[c(8,1:7)][res$cluster+1], shape=c(4,15,17,19)[res$cluster+1], size=1.5) + theme_bw() Figure 6.31: DBSCAN clustering (eps=0.6, minPts=10) of the blobs data set. Outlier observations are shown as grey crosses. 6.5.4 Example: clustering synthetic data sets point_shapes &lt;- c(4,15,17,19,5,6,0,1) point_colours &lt;- brewer.pal(8,&quot;Dark2&quot;)[c(8,1:7)] point_size = 1.5 center_point_size = 8 plot_dbscan_clusters &lt;- function(data_set, dbscan_output){ ggplot(data_set, aes(V1,V2)) + geom_point(col=point_colours[dbscan_output$cluster+1], shape=point_shapes[dbscan_output$cluster+1], size=point_size) + theme_bw() } 6.5.4.1 Aggregation aggregation &lt;- read.table(&quot;data/example_clusters/aggregation.txt&quot;) kNNdistplot(aggregation[,1:2], k=10) abline(h=1.8) Figure 6.32: 10-nearest neighbour distances for the aggregation data set res &lt;- dbscan::dbscan(aggregation[,1:2], eps=1.8, minPts = 10) table(res$cluster) ## ## 0 1 2 3 4 5 6 ## 2 168 307 105 127 45 34 plot_dbscan_clusters(aggregation, res) Figure 6.33: DBSCAN clustering (eps=1.8, minPts=10) of the aggregation data set. Outlier observations are shown as grey crosses. 6.5.4.2 Noisy moons noisy_moons &lt;- read.csv(&quot;data/example_clusters/noisy_moons.csv&quot;, header=F) kNNdistplot(noisy_moons[,1:2], k=10) abline(h=0.075) Figure 6.34: 10-nearest neighbour distances for the noisy moons data set res &lt;- dbscan::dbscan(noisy_moons[,1:2], eps=0.075, minPts = 10) table(res$cluster) ## ## 0 1 2 ## 8 748 744 plot_dbscan_clusters(noisy_moons, res) Figure 6.35: DBSCAN clustering (eps=0.075, minPts=10) of the noisy moons data set. Outlier observations are shown as grey crosses. 6.5.4.3 Different density diff_density &lt;- read.csv(&quot;data/example_clusters/different_density.csv&quot;, header=F) kNNdistplot(diff_density[,1:2], k=10) abline(h=0.9) abline(h=0.6, lty=2) Figure 6.36: 10-nearest neighbour distances for the different density distributions data set res &lt;- dbscan::dbscan(diff_density[,1:2], eps=0.9, minPts = 10) table(res$cluster) ## ## 0 1 ## 40 1460 plot_dbscan_clusters(diff_density, res) Figure 6.37: DBSCAN clustering of the different density distribution data set with eps=0.9 and minPts=10. Outlier observations are shown as grey crosses. res &lt;- dbscan::dbscan(diff_density[,1:2], eps=0.6, minPts = 10) table(res$cluster) ## ## 0 1 2 ## 109 399 992 plot_dbscan_clusters(diff_density, res) Figure 6.38: DBSCAN clustering of the different density distribution data set with eps=0.6 and minPts=10. Outlier observations are shown as grey crosses. 6.5.4.4 Anisotropic distributions aniso &lt;- read.csv(&quot;data/example_clusters/aniso.csv&quot;, header=F) kNNdistplot(aniso[,1:2], k=10) abline(h=0.35) Figure 6.39: 10-nearest neighbour distances for the anisotropic distributions data set res &lt;- dbscan::dbscan(aniso[,1:2], eps=0.35, minPts = 10) table(res$cluster) ## ## 0 1 2 3 ## 29 489 488 494 plot_dbscan_clusters(aniso, res) Figure 6.40: DBSCAN clustering (eps=0.3, minPts=10) of the anisotropic distributions data set. Outlier observations are shown as grey crosses. 6.5.4.5 No structure no_structure &lt;- read.csv(&quot;data/example_clusters/no_structure.csv&quot;, header=F) kNNdistplot(no_structure[,1:2], k=10) abline(h=0.057) Figure 6.41: 10-nearest neighbour distances for the data set with no structure. res &lt;- dbscan::dbscan(no_structure[,1:2], eps=0.57, minPts = 10) table(res$cluster) ## ## 1 ## 1500 6.5.5 Example: gene expression profiling of human tissues Returning again to the data on gene expression of human tissues. load(&quot;data/tissues_gene_expression/tissuesGeneExpression.rda&quot;) table(tissue) ## tissue ## cerebellum colon endometrium hippocampus kidney liver ## 38 34 15 31 39 26 ## placenta ## 6 We’ll try k=5 (default for dbscan), because there are only six observations for placenta. kNNdistplot(t(e), k=5) abline(h=85) Figure 6.42: Five-nearest neighbour distances for the gene expression profiling of human tissues data set. set.seed(42) res &lt;- dbscan::dbscan(t(e), eps=85, minPts=5) table(res$cluster) ## ## 0 1 2 3 4 5 6 ## 12 37 62 34 24 15 5 table(tissue, res$cluster) ## ## tissue 0 1 2 3 4 5 6 ## cerebellum 2 0 31 0 0 0 5 ## colon 0 0 0 34 0 0 0 ## endometrium 0 0 0 0 0 15 0 ## hippocampus 0 0 31 0 0 0 0 ## kidney 2 37 0 0 0 0 0 ## liver 2 0 0 0 24 0 0 ## placenta 6 0 0 0 0 0 0 pca &lt;- prcomp(t(e)) ggplot(data=as.data.frame(pca$x), aes(PC1,PC2)) + geom_point(col=brewer.pal(8,&quot;Dark2&quot;)[c(8,1:7)][res$cluster+1], shape=c(48:55)[res$cluster+1], size=5) + theme_bw() Figure 6.43: Clustering of human tissue gene expression: scatterplot of first two principal components. 6.6 Evaluating cluster quality 6.6.1 Silhouette method Silhouette \\[\\begin{equation} s(i) = \\frac{b(i) - a(i)}{max\\left(a(i),b(i)\\right)} \\tag{6.2} \\end{equation}\\] Where a(i) - average dissimmilarity of i with all other data within the cluster. a(i) can be interpreted as how well i is assigned to its cluster (the smaller the value, the better the assignment). b(i) - the lowest average dissimilarity of i to any other cluster, of which i is not a member. Observations with a large s(i) (close to 1) are very well clustered. Observations lying between clusters will have a small s(i) (close to 0). If an observation has a negative s(i), it has probably been placed in the wrong cluster. 6.6.2 Example - k-means clustering of blobs data set Load library required for calculating silhouette coefficients and plotting silhouettes. library(cluster) We are going to take another look at k-means clustering of the blobs data-set (figure 6.15). Specifically we are going to see if silhouette analysis supports our original choice of k=3 as the optimum number of clusters (figure 6.16). Silhouette analysis requires a minimum of two clusters, so we’ll try values of k from 2 to 9. k &lt;- 2:9 Create a palette of colours for plotting. kColours &lt;- brewer.pal(9,&quot;Set1&quot;) Perform k-means clustering for each value of k from 2 to 9. res &lt;- lapply(k, function(i){kmeans(blobs[,1:2], i, nstart=50)}) Calculate the Euclidean distance matrix d &lt;- dist(blobs[,1:2]) Silhouette plot for k=2 s2 &lt;- silhouette(res[[2-1]]$cluster, d) plot(s2, border=NA, col=kColours[sort(res[[2-1]]$cluster)], main=&quot;&quot;) Figure 6.44: Silhouette plot for k-means clustering of the blobs data set with k=2. Silhouette plot for k=9 s9 &lt;- silhouette(res[[9-1]]$cluster, d) plot(s9, border=NA, col=kColours[sort(res[[9-1]]$cluster)], main=&quot;&quot;) Figure 6.45: Silhouette plot for k-means clustering of the blobs data set with k=9. Let’s take a look at the silhouette plot for k=3. s3 &lt;- silhouette(res[[3-1]]$cluster, d) plot(s3, border=NA, col=kColours[sort(res[[3-1]]$cluster)], main=&quot;&quot;) Figure 6.46: Silhouette plot for k-means clustering of the blobs data set with k=3. So far the silhouette plots have shown that k=3 appears to be the optimum number of clusters, but we should investigate the silhouette coefficients at other values of k. Rather than produce a silhouette plot for each value of k, we can get a useful summary by making a barplot of average silhouette coefficients. First we will calculate the silhouette coefficient for every observation (we need to index our list of kmeans outputs by i-1, because we are counting from k=2 ). s &lt;- lapply(k, function(i){silhouette(res[[i-1]]$cluster, d)}) We can then calculate the mean silhouette coefficient for each value of k from 2 to 9. avgS &lt;- sapply(s, function(x){mean(x[,3])}) Now we have the data we need to produce a barplot. dat &lt;- data.frame(k, avgS) ggplot(data=dat, aes(x=k, y=avgS)) + geom_bar(stat=&quot;identity&quot;, fill=&quot;steelblue&quot;) + geom_text(aes(label=round(avgS,2)), vjust=1.6, color=&quot;white&quot;, size=3.5)+ labs(y=&quot;Average silhouette coefficient&quot;) + scale_x_continuous(breaks=2:9) + theme_bw() Figure 6.47: Barplot of the average silhouette coefficients resulting from k-means clustering of the blobs data-set using values of k from 1-9. The bar plot (figure 6.47) confirms that the optimum number of clusters is three. 6.6.3 Example - DBSCAN clustering of noisy moons The clusters that DBSCAN found in the noisy moons data set are shown in figure 6.35. Let’s repeat clustering, because the original result is no longer in memory. res &lt;- dbscan::dbscan(noisy_moons[,1:2], eps=0.075, minPts = 10) Identify noise points as we do not want to include these in the silhouette analysis # identify and remove noise points noise &lt;- res$cluster==0 Remove noise points from cluster results clusters &lt;- res$cluster[!noise] Generate distance matrix from noisy_moons data.frame, exluding noise points. d &lt;- dist(noisy_moons[!noise,1:2]) Silhouette analysis clusterColours &lt;- brewer.pal(9,&quot;Set1&quot;) sil &lt;- silhouette(clusters, d) plot(sil, border=NA, col=clusterColours[sort(clusters)], main=&quot;&quot;) Figure 6.48: Silhouette plot for DBSCAN clustering of the noisy moons data set. The silhouette analysis suggests that DBSCAN has found clusters of poor quality in the noisy moons data set. However, we saw by eye that it it did a good job of deliminiting the two clusters. The result demonstrates that the silhouette method is less useful when dealing with clusters that are defined by density, rather than inertia. 6.7 Exercises 6.7.1 Exercise 1 Image segmentation is used to partition digital images into distinct regions containing pixels with similar attributes. Applications include identifying objects or structures in biomedical images. The aim of this exercise is to use k-means clustering to segment the image of a histological section of lung tissue (figure 6.49) into distinct biological structures, based on pixel colour. Figure 6.49: Image of haematoxylin and eosin (H&amp;E) stained section of lung tissue from a patient with end-stage emphysema. CC BY 2.0, https://commons.wikimedia.org/w/index.php?curid=437645. The haematoxylin and eosin (H &amp; E) staining reveals four types of biological objects, identified by the following colours: blue-purple: cell nuclei red: red blood cells pink: other cell bodies and extracellular material white: air spaces Consider the following questions: Can k-means clustering find the four biological objects in the image based on pixel colour? Earlier we saw that if we plot the total within-cluster sum of squares against k, the position of the “elbow” is a useful guide to choosing the appropriate value of k (see section 6.4.3. According to the “elbow” method, how many distinct clusters (colours) of pixels are present in the image? Hints: If you haven’t worked with images in R before, you may find the following information helpful. The package EBImage provides a suite of tools for working with images. We will use it to read the file containing the image of the lung section. library(EBImage) ## ## Attaching package: &#39;EBImage&#39; ## The following object is masked from &#39;package:dendextend&#39;: ## ## rotate library(methods) img &lt;- readImage(&quot;data/histology/Emphysema_H_and_E.jpg&quot;) img is an object of the EBImage class Image; it is essentially a multidimensional array containing the pixel intensities. To see the dimensions of the array, run: dim(img) ## [1] 528 393 3 In the case of this colour image, the array is 3-dimensional with 528 x 393 x 3 elements. These dimensions correspond to the image width (in pixels), image height and number of colour channels, respectively. The colour channels are red, green and blue (RGB). Before we can cluster the pixels on colour, we need to convert the 3D array into a 2D data.frame (or matrix). Specifically, we require a data.frame (or matrix) where rows represent pixels and there is a column for the intensity of each of the three colour channels. We also need columns for the x and y coordinates of each pixel. imgDim &lt;- dim(img) imgDF &lt;- data.frame( x = rep(1:imgDim[1], imgDim[2]), y = rep(imgDim[2]:1, each=imgDim[1]), r = as.vector(img[,,1]), g = as.vector(img[,,2]), b = as.vector(img[,,3]) ) If the data in imgDF are correct, we should be able to display the image using ggplot: ggplot(data = imgDF, aes(x = x, y = y)) + geom_point(colour = rgb(imgDF[c(&quot;r&quot;, &quot;g&quot;, &quot;b&quot;)])) + xlab(&quot;x&quot;) + ylab(&quot;y&quot;) + theme_minimal() Figure 6.50: Image of lung tissue recreated from reshaped data. This should be all the information you need to perform this exercise. Solutions to exercises can be found in appendix E. References "],
["nearest-neighbours.html", "7 Nearest neighbours 7.1 Introduction 7.2 Classification: simulated data 7.3 Classification: cell segmentation 7.4 Regression 7.5 Exercises", " 7 Nearest neighbours 7.1 Introduction k-NN is by far the simplest method of supervised learning we will cover in this course. It is a non-parametric method that can be used for both classification (predicting class membership) and regression (estimating continuous variables). k-NN is categorized as instance based (memory based) learning, because all computation is deferred until classification. The most computationally demanding aspects of k-NN are finding neighbours and storing the entire learning set. A simple k-NN classification rule (figure 7.1) would proceed as follows: when presented with a new observation, find the k closest samples in the learning set predict the class by majority vote Figure 7.1: Illustration of k-nn classification. In this example we have two classes: blue squares and red triangles. The green circle represents a test object. If k=3 (solid line circle) the test object is assigned to the red triangle class. If k=5 the test object is assigned to the blue square class. By Antti Ajanki AnAj - Own work, CC BY-SA 3.0, https://commons.wikimedia.org/w/index.php?curid=2170282 A basic implementation of k-NN regression would calculate the average of the numerical outcome of the k nearest neighbours. The number of neighbours k can have a considerable impact on the predictive performance of k-NN in both classification and regression. The optimal value of k should be chosen using cross-validation. Euclidean distance is the most widely used distance metric in k-nn, and will be used in the examples and exercises in this chapter. However, other distance metrics can be used. Euclidean distance: \\[\\begin{equation} distance\\left(p,q\\right)=\\sqrt{\\sum_{i=1}^{n} (p_i-q_i)^2} \\tag{6.1} \\end{equation}\\] Figure 6.2: Euclidean distance. 7.2 Classification: simulated data A simulated data set will be used to demonstrate: bias-variance trade-off the knn function in R plotting decision boundaries choosing the optimum value of k The dataset has been partitioned into training and test sets. Load data load(&quot;data/example_binary_classification/bin_class_example.rda&quot;) str(xtrain) ## &#39;data.frame&#39;: 400 obs. of 2 variables: ## $ V1: num -0.223 0.944 2.36 1.846 1.732 ... ## $ V2: num -1.153 -0.827 -0.128 2.014 -0.574 ... str(xtest) ## &#39;data.frame&#39;: 400 obs. of 2 variables: ## $ V1: num 2.09 2.3 2.07 1.65 1.18 ... ## $ V2: num -1.009 1.0947 0.1644 0.3243 -0.0277 ... summary(as.factor(ytrain)) ## 0 1 ## 200 200 summary(as.factor(ytest)) ## 0 1 ## 200 200 library(ggplot2) library(GGally) library(RColorBrewer) point_shapes &lt;- c(15,17) point_colours &lt;- brewer.pal(3,&quot;Dark2&quot;) point_size = 2 ggplot(xtrain, aes(V1,V2)) + geom_point(col=point_colours[ytrain+1], shape=point_shapes[ytrain+1], size=point_size) + ggtitle(&quot;train&quot;) + theme_bw() + theme(plot.title = element_text(size=25, face=&quot;bold&quot;), axis.text=element_text(size=15), axis.title=element_text(size=20,face=&quot;bold&quot;)) ggplot(xtest, aes(V1,V2)) + geom_point(col=point_colours[ytest+1], shape=point_shapes[ytest+1], size=point_size) + ggtitle(&quot;test&quot;) + theme_bw() + theme(plot.title = element_text(size=25, face=&quot;bold&quot;), axis.text=element_text(size=15), axis.title=element_text(size=20,face=&quot;bold&quot;)) Figure 7.2: Scatterplots of the simulated training and test data sets that will be used in the demonstration of binary classification using k-nn 7.2.1 knn function For k-nn classification and regression we will use the knn function in the package class. library(class) Arguments to knn train : matrix or data frame of training set cases. test : matrix or data frame of test set cases. A vector will be interpreted as a row vector for a single case. cl : factor of true classifications of training set k : number of neighbours considered. l : minimum vote for definite decision, otherwise doubt. (More precisely, less than k-l dissenting votes are allowed, even if k is increased by ties.) prob : If this is true, the proportion of the votes for the winning class are returned as attribute prob. use.all : controls handling of ties. If true, all distances equal to the kth largest are included. If false, a random selection of distances equal to the kth is chosen to use exactly k neighbours. Let us perform k-nn on the training set with k=1. We will use the confusionMatrix function from the caret package to summarize performance of the classifier. library(caret) ## Loading required package: lattice knn1train &lt;- class::knn(train=xtrain, test=xtrain, cl=ytrain, k=1) confusionMatrix(knn1train, as.factor(ytrain)) ## Confusion Matrix and Statistics ## ## Reference ## Prediction 0 1 ## 0 200 0 ## 1 0 200 ## ## Accuracy : 1 ## 95% CI : (0.9908, 1) ## No Information Rate : 0.5 ## P-Value [Acc &gt; NIR] : &lt; 2.2e-16 ## ## Kappa : 1 ## Mcnemar&#39;s Test P-Value : NA ## ## Sensitivity : 1.0 ## Specificity : 1.0 ## Pos Pred Value : 1.0 ## Neg Pred Value : 1.0 ## Prevalence : 0.5 ## Detection Rate : 0.5 ## Detection Prevalence : 0.5 ## Balanced Accuracy : 1.0 ## ## &#39;Positive&#39; Class : 0 ## The classifier performs perfectly on the training set, because with k=1, each observation is being predicted by itself! Now let use the training set to predict on the test set. knn1test &lt;- class::knn(train=xtrain, test=xtest, cl=ytrain, k=1) confusionMatrix(knn1test, as.factor(ytest)) ## Confusion Matrix and Statistics ## ## Reference ## Prediction 0 1 ## 0 131 81 ## 1 69 119 ## ## Accuracy : 0.625 ## 95% CI : (0.5755, 0.6726) ## No Information Rate : 0.5 ## P-Value [Acc &gt; NIR] : 3.266e-07 ## ## Kappa : 0.25 ## Mcnemar&#39;s Test P-Value : 0.3691 ## ## Sensitivity : 0.6550 ## Specificity : 0.5950 ## Pos Pred Value : 0.6179 ## Neg Pred Value : 0.6330 ## Prevalence : 0.5000 ## Detection Rate : 0.3275 ## Detection Prevalence : 0.5300 ## Balanced Accuracy : 0.6250 ## ## &#39;Positive&#39; Class : 0 ## Performance on the test set is not so good. This is an example of a classifier being over-fitted to the training set. 7.2.2 Plotting decision boundaries Since we have just two dimensions we can visualize the decision boundary generated by the k-nn classifier in a 2D scatterplot. Situations where your original data set contains only two variables will be rare, but it is not unusual to reduce a high-dimensional data set to just two dimensions using the methods that will be discussed in chapter 5. Therefore, knowing how to plot decision boundaries will potentially be helpful for many different datasets and classifiers. Create a grid so we can predict across the full range of our variables V1 and V2. gridSize &lt;- 150 v1limits &lt;- c(min(c(xtrain[,1],xtest[,1])),max(c(xtrain[,1],xtest[,1]))) tmpV1 &lt;- seq(v1limits[1],v1limits[2],len=gridSize) v2limits &lt;- c(min(c(xtrain[,2],xtest[,2])),max(c(xtrain[,2],xtest[,2]))) tmpV2 &lt;- seq(v2limits[1],v2limits[2],len=gridSize) xgrid &lt;- expand.grid(tmpV1,tmpV2) names(xgrid) &lt;- names(xtrain) Predict values of all elements of grid. knn1grid &lt;- class::knn(train=xtrain, test=xgrid, cl=ytrain, k=1) V3 &lt;- as.numeric(as.vector(knn1grid)) xgrid &lt;- cbind(xgrid, V3) Plot point_shapes &lt;- c(15,17) point_colours &lt;- brewer.pal(3,&quot;Dark2&quot;) point_size = 2 ggplot(xgrid, aes(V1,V2)) + geom_point(col=point_colours[knn1grid], shape=16, size=0.3) + geom_point(data=xtrain, aes(V1,V2), col=point_colours[ytrain+1], shape=point_shapes[ytrain+1], size=point_size) + geom_contour(data=xgrid, aes(x=V1, y=V2, z=V3), breaks=0.5, col=&quot;grey30&quot;) + ggtitle(&quot;train&quot;) + theme_bw() + theme(plot.title = element_text(size=25, face=&quot;bold&quot;), axis.text=element_text(size=15), axis.title=element_text(size=20,face=&quot;bold&quot;)) ggplot(xgrid, aes(V1,V2)) + geom_point(col=point_colours[knn1grid], shape=16, size=0.3) + geom_point(data=xtest, aes(V1,V2), col=point_colours[ytest+1], shape=point_shapes[ytrain+1], size=point_size) + geom_contour(data=xgrid, aes(x=V1, y=V2, z=V3), breaks=0.5, col=&quot;grey30&quot;) + ggtitle(&quot;test&quot;) + theme_bw() + theme(plot.title = element_text(size=25, face=&quot;bold&quot;), axis.text=element_text(size=15), axis.title=element_text(size=20,face=&quot;bold&quot;)) Figure 7.3: Binary classification of the simulated training and test sets with k=1. 7.2.3 Bias-variance tradeoff The bias–variance tradeoff is the problem of simultaneously minimizing two sources of error that prevent supervised learning algorithms from generalizing beyond their training set: The bias is error from erroneous assumptions in the learning algorithm. High bias can cause an algorithm to miss the relevant relations between features and target outputs (underfitting). The variance is error from sensitivity to small fluctuations in the training set. High variance can cause an algorithm to model the random noise in the training data, rather than the intended outputs (overfitting). To demonstrate this phenomenon, let us look at the performance of the k-nn classifier over a range of values of k. First we will define a function to create a sequence of log spaced values. This is the lseq function from the emdbook package: lseq &lt;- function(from, to, length.out) { exp(seq(log(from), log(to), length.out = length.out)) } Get log spaced sequence of length 20, round and then remove any duplicates resulting from rounding. s &lt;- unique(round(lseq(1,400,20))) length(s) ## [1] 19 train_error &lt;- sapply(s, function(i){ yhat &lt;- knn(xtrain, xtrain, ytrain, i) return(1-mean(as.numeric(as.vector(yhat))==ytrain)) }) test_error &lt;- sapply(s, function(i){ yhat &lt;- knn(xtrain, xtest, ytrain, i) return(1-mean(as.numeric(as.vector(yhat))==ytest)) }) k &lt;- rep(s, 2) set &lt;- c(rep(&quot;train&quot;, length(s)), rep(&quot;test&quot;, length(s))) error &lt;- c(train_error, test_error) misclass_errors &lt;- data.frame(k, set, error) ggplot(misclass_errors, aes(x=k, y=error, group=set)) + geom_line(aes(colour=set, linetype=set), size=1.5) + scale_x_log10() + ylab(&quot;Misclassification Errors&quot;) + theme_bw() + theme(legend.position = c(0.5, 0.25), legend.title=element_blank(), legend.text=element_text(size=12), axis.title.x=element_text(face=&quot;italic&quot;, size=12)) Figure 7.4: Misclassification errors as a function of neighbourhood size. We see excessive variance (overfitting) at low values of k, and bias (underfitting) at high values of k. 7.2.4 Choosing k We will use the caret library. Caret provides a unified interface to a huge range of supervised learning packages in R. The design of its tools encourages best practice, especially in relation to cross-validation and testing. Additionally, it has automatic parallel processing built in, which is a significant advantage when dealing with large data sets. library(caret) To take advantage of Caret’s parallel processing functionality, we simply need to load the doMC package and register workers: library(doMC) ## Loading required package: foreach ## Loading required package: iterators ## Loading required package: parallel registerDoMC(detectCores()) To find out how many cores we have registered we can use: getDoParWorkers() ## [1] 8 The caret function train is used to fit predictive models over different values of k. The function trainControl is used to specify a list of computational and resampling options, which will be passed to train. We will start by configuring our cross-validation procedure using trainControl. We would like to make this demonstration reproducible and because we will be running the models in parallel, using the set.seed function alone is not sufficient. In addition to using set.seed we have to make use of the optional seeds argument to trainControl. We need to supply seeds with a list of integers that will be used to set the seed at each sampling iteration. The list is required to have a length of B+1, where B is the number of resamples. We will be repeating 10-fold cross-validation a total of ten times and so our list must have a length of 101. The first B elements of the list are required to be vectors of integers of length M, where M is the number of models being evaluated (in this case 19). The last element of the list only needs to be a single integer, which will be used for the final model. First we generate our list of seeds. set.seed(42) seeds &lt;- vector(mode = &quot;list&quot;, length = 101) for(i in 1:100) seeds[[i]] &lt;- sample.int(1000, 19) seeds[[101]] &lt;- sample.int(1000,1) We can now use trainControl to create a list of computational options for resampling. tc &lt;- trainControl(method=&quot;repeatedcv&quot;, number = 10, repeats = 10, seeds = seeds) There are two options for choosing the values of k to be evaluated by the train function: Pass a data.frame of values of k to the tuneGrid argument of train. Specify the number of different levels of k using the tuneLength function and allow train to pick the actual values. We will use the first option, so that we can try the values of k we examined earlier. The vector of values of k we created earlier should be converted into a data.frame. s &lt;- data.frame(s) names(s) &lt;- &quot;k&quot; We are now ready to run the cross-validation. knnFit &lt;- train(xtrain, as.factor(ytrain), method=&quot;knn&quot;, tuneGrid=s, trControl=tc) knnFit ## k-Nearest Neighbors ## ## 400 samples ## 2 predictors ## 2 classes: &#39;0&#39;, &#39;1&#39; ## ## No pre-processing ## Resampling: Cross-Validated (10 fold, repeated 10 times) ## Summary of sample sizes: 360, 360, 360, 360, 360, 360, ... ## Resampling results across tuning parameters: ## ## k Accuracy Kappa ## 1 0.63300 0.2660 ## 2 0.63875 0.2775 ## 3 0.67375 0.3475 ## 4 0.67900 0.3580 ## 5 0.69575 0.3915 ## 7 0.71100 0.4220 ## 9 0.71775 0.4355 ## 12 0.71500 0.4300 ## 17 0.72675 0.4535 ## 23 0.73800 0.4760 ## 32 0.73725 0.4745 ## 44 0.73875 0.4775 ## 60 0.74850 0.4970 ## 83 0.75500 0.5100 ## 113 0.73500 0.4700 ## 155 0.72575 0.4515 ## 213 0.70750 0.4150 ## 292 0.68825 0.3765 ## 400 0.51300 0.0260 ## ## Accuracy was used to select the optimal model using the largest value. ## The final value used for the model was k = 83. Cohen’s Kappa: \\[\\begin{equation} Kappa = \\frac{O-E}{1-E} \\tag{7.1} \\end{equation}\\] where O is the observed accuracy and E is the expected accuracy based on the marginal totals of the confusion matrix. Cohen’s Kappa takes values between -1 and 1; a value of zero indicates no agreement between the observed and predicted classes, while a value of one shows perfect concordance of the model prediction and the observed classes. If the prediction is in the opposite direction of the truth, a negative value will be obtained, but large negative values are rare in practice (Kuhn and Johnson 2013). We can plot accuracy (determined from repeated cross-validation) as a function of neighbourhood size. plot(knnFit) Figure 7.5: Accuracy (repeated cross-validation) as a function of neighbourhood size. We can also plot other performance metrics, such as Cohen’s Kappa, using the metric argument. plot(knnFit, metric=&quot;Kappa&quot;) Figure 7.6: Cohen’s Kappa (repeated cross-validation) as a function of neighbourhood size. Let us now evaluate how our classifier performs on the test set. test_pred &lt;- predict(knnFit, xtest) confusionMatrix(test_pred, as.factor(ytest)) ## Confusion Matrix and Statistics ## ## Reference ## Prediction 0 1 ## 0 154 68 ## 1 46 132 ## ## Accuracy : 0.715 ## 95% CI : (0.668, 0.7588) ## No Information Rate : 0.5 ## P-Value [Acc &gt; NIR] : &lt;2e-16 ## ## Kappa : 0.43 ## Mcnemar&#39;s Test P-Value : 0.0492 ## ## Sensitivity : 0.7700 ## Specificity : 0.6600 ## Pos Pred Value : 0.6937 ## Neg Pred Value : 0.7416 ## Prevalence : 0.5000 ## Detection Rate : 0.3850 ## Detection Prevalence : 0.5550 ## Balanced Accuracy : 0.7150 ## ## &#39;Positive&#39; Class : 0 ## Scatterplots with decision boundaries can be plotted using the methods described earlier. First create a grid so we can predict across the full range of our variables V1 and V2: gridSize &lt;- 150 v1limits &lt;- c(min(c(xtrain[,1],xtest[,1])),max(c(xtrain[,1],xtest[,1]))) tmpV1 &lt;- seq(v1limits[1],v1limits[2],len=gridSize) v2limits &lt;- c(min(c(xtrain[,2],xtest[,2])),max(c(xtrain[,2],xtest[,2]))) tmpV2 &lt;- seq(v2limits[1],v2limits[2],len=gridSize) xgrid &lt;- expand.grid(tmpV1,tmpV2) names(xgrid) &lt;- names(xtrain) Predict values of all elements of grid. knn1grid &lt;- predict(knnFit, xgrid) V3 &lt;- as.numeric(as.vector(knn1grid)) xgrid &lt;- cbind(xgrid, V3) Plot point_shapes &lt;- c(15,17) point_colours &lt;- brewer.pal(3,&quot;Dark2&quot;) point_size = 2 ggplot(xgrid, aes(V1,V2)) + geom_point(col=point_colours[knn1grid], shape=16, size=0.3) + geom_point(data=xtrain, aes(V1,V2), col=point_colours[ytrain+1], shape=point_shapes[ytrain+1], size=point_size) + geom_contour(data=xgrid, aes(x=V1, y=V2, z=V3), breaks=0.5, col=&quot;grey30&quot;) + ggtitle(&quot;train&quot;) + theme_bw() + theme(plot.title = element_text(size=25, face=&quot;bold&quot;), axis.text=element_text(size=15), axis.title=element_text(size=20,face=&quot;bold&quot;)) ggplot(xgrid, aes(V1,V2)) + geom_point(col=point_colours[knn1grid], shape=16, size=0.3) + geom_point(data=xtest, aes(V1,V2), col=point_colours[ytest+1], shape=point_shapes[ytrain+1], size=point_size) + geom_contour(data=xgrid, aes(x=V1, y=V2, z=V3), breaks=0.5, col=&quot;grey30&quot;) + ggtitle(&quot;test&quot;) + theme_bw() + theme(plot.title = element_text(size=25, face=&quot;bold&quot;), axis.text=element_text(size=15), axis.title=element_text(size=20,face=&quot;bold&quot;)) Figure 7.7: Binary classification of the simulated training and test sets with k=83. 7.3 Classification: cell segmentation The simulated data in our previous example were randomly sampled from a normal (Gaussian) distribution and so did not require pre-processing. In practice, data collected in real studies often require transformation and/or filtering. Furthermore, the simulated data contained only two predictors; in practice, you are likely to have many variables. For example, in a gene expression study you might have thousands of variables. When using k-nn for classification or regression, removing variables that are not associated with the outcome of interest may improve the predictive power of the model. The process of choosing the best predictors from the available variables is known as feature selection. For honest estimates of model performance, pre-processing and feature selection should be performed within the loops of the cross validation process. 7.3.1 Cell segmentation data set Pre-processing and feature selection will be demonstrated using the cell segmentation data of (A. A. Hill et al. (2007)). High Content Screening (HCS) automates the collection and analysis of biological images of cultured cells. However, image segmentation algorithms are not perfect and sometimes do not reliably quantitate the morphology of cells. Hill et al. sought to differentiate between well- and poorly-segmented cells based on the morphometric data collected in HCS. If poorly-segmented cells can be automatically detected and eliminated, then the accuracy of studies using HCS will be improved. Hill et al. collected morphometric data on 2019 cells and asked human reviewers to classify the cells as well- or poorly-segmented. Figure 7.8: Image segmentation in high content screening. Images b and c are examples of well-segmented cells; d and e show poor-segmentation. Source: Hill(2007) https://doi.org/10.1186/1471-2105-8-340 This data set is one of several included in caret. data(segmentationData) str(segmentationData) ## &#39;data.frame&#39;: 2019 obs. of 61 variables: ## $ Cell : int 207827637 207932307 207932463 207932470 207932455 207827656 207827659 207827661 207932479 207932480 ... ## $ Case : Factor w/ 2 levels &quot;Test&quot;,&quot;Train&quot;: 1 2 2 2 1 1 1 1 1 1 ... ## $ Class : Factor w/ 2 levels &quot;PS&quot;,&quot;WS&quot;: 1 1 2 1 1 2 2 1 2 2 ... ## $ AngleCh1 : num 143.25 133.75 106.65 69.15 2.89 ... ## $ AreaCh1 : int 185 819 431 298 285 172 177 251 495 384 ... ## $ AvgIntenCh1 : num 15.7 31.9 28 19.5 24.3 ... ## $ AvgIntenCh2 : num 4.95 206.88 116.32 102.29 112.42 ... ## $ AvgIntenCh3 : num 9.55 69.92 63.94 28.22 20.47 ... ## $ AvgIntenCh4 : num 2.21 164.15 106.7 31.03 40.58 ... ## $ ConvexHullAreaRatioCh1 : num 1.12 1.26 1.05 1.2 1.11 ... ## $ ConvexHullPerimRatioCh1: num 0.92 0.797 0.935 0.866 0.957 ... ## $ DiffIntenDensityCh1 : num 29.5 31.9 32.5 26.7 31.6 ... ## $ DiffIntenDensityCh3 : num 13.8 43.1 36 22.9 21.7 ... ## $ DiffIntenDensityCh4 : num 6.83 79.31 51.36 26.39 25.03 ... ## $ EntropyIntenCh1 : num 4.97 6.09 5.88 5.42 5.66 ... ## $ EntropyIntenCh3 : num 4.37 6.64 6.68 5.44 5.29 ... ## $ EntropyIntenCh4 : num 2.72 7.88 7.14 5.78 5.24 ... ## $ EqCircDiamCh1 : num 15.4 32.3 23.4 19.5 19.1 ... ## $ EqEllipseLWRCh1 : num 3.06 1.56 1.38 3.39 2.74 ... ## $ EqEllipseOblateVolCh1 : num 337 2233 802 725 608 ... ## $ EqEllipseProlateVolCh1 : num 110 1433 583 214 222 ... ## $ EqSphereAreaCh1 : num 742 3279 1727 1195 1140 ... ## $ EqSphereVolCh1 : num 1901 17654 6751 3884 3621 ... ## $ FiberAlign2Ch3 : num 1 1.49 1.3 1.22 1.49 ... ## $ FiberAlign2Ch4 : num 1 1.35 1.52 1.73 1.38 ... ## $ FiberLengthCh1 : num 27 64.3 21.1 43.1 34.7 ... ## $ FiberWidthCh1 : num 7.41 13.17 21.14 7.4 8.48 ... ## $ IntenCoocASMCh3 : num 0.01118 0.02805 0.00686 0.03096 0.02277 ... ## $ IntenCoocASMCh4 : num 0.05045 0.01259 0.00614 0.01103 0.07969 ... ## $ IntenCoocContrastCh3 : num 40.75 8.23 14.45 7.3 15.85 ... ## $ IntenCoocContrastCh4 : num 13.9 6.98 16.7 13.39 3.54 ... ## $ IntenCoocEntropyCh3 : num 7.2 6.82 7.58 6.31 6.78 ... ## $ IntenCoocEntropyCh4 : num 5.25 7.1 7.67 7.2 5.5 ... ## $ IntenCoocMaxCh3 : num 0.0774 0.1532 0.0284 0.1628 0.1274 ... ## $ IntenCoocMaxCh4 : num 0.172 0.0739 0.0232 0.0775 0.2785 ... ## $ KurtIntenCh1 : num -0.6567 -0.2488 -0.2935 0.6259 0.0421 ... ## $ KurtIntenCh3 : num -0.608 -0.331 1.051 0.128 0.952 ... ## $ KurtIntenCh4 : num 0.726 -0.265 0.151 -0.347 -0.195 ... ## $ LengthCh1 : num 26.2 47.2 28.1 37.9 36 ... ## $ NeighborAvgDistCh1 : num 370 174 158 206 205 ... ## $ NeighborMinDistCh1 : num 99.1 30.1 34.9 33.1 27 ... ## $ NeighborVarDistCh1 : num 128 81.4 90.4 116.9 111 ... ## $ PerimCh1 : num 68.8 154.9 84.6 101.1 86.5 ... ## $ ShapeBFRCh1 : num 0.665 0.54 0.724 0.589 0.6 ... ## $ ShapeLWRCh1 : num 2.46 1.47 1.33 2.83 2.73 ... ## $ ShapeP2ACh1 : num 1.88 2.26 1.27 2.55 2.02 ... ## $ SkewIntenCh1 : num 0.455 0.399 0.472 0.882 0.517 ... ## $ SkewIntenCh3 : num 0.46 0.62 0.971 1 1.177 ... ## $ SkewIntenCh4 : num 1.233 0.527 0.325 0.604 0.926 ... ## $ SpotFiberCountCh3 : int 1 4 2 4 1 1 0 2 1 1 ... ## $ SpotFiberCountCh4 : num 5 12 7 8 8 5 5 8 12 8 ... ## $ TotalIntenCh1 : int 2781 24964 11552 5545 6603 53779 43950 4401 7593 6512 ... ## $ TotalIntenCh2 : num 701 160998 47511 28870 30306 ... ## $ TotalIntenCh3 : int 1690 54675 26344 8042 5569 21234 20929 4136 6488 7503 ... ## $ TotalIntenCh4 : int 392 128368 43959 8843 11037 57231 46187 373 24325 23162 ... ## $ VarIntenCh1 : num 12.5 18.8 17.3 13.8 15.4 ... ## $ VarIntenCh3 : num 7.61 56.72 37.67 30.01 20.5 ... ## $ VarIntenCh4 : num 2.71 118.39 49.47 24.75 45.45 ... ## $ WidthCh1 : num 10.6 32.2 21.2 13.4 13.2 ... ## $ XCentroid : int 42 215 371 487 283 191 180 373 236 303 ... ## $ YCentroid : int 14 347 252 295 159 127 138 181 467 468 ... The first column of segmentationData is a unique identifier for each cell and the second column is a factor indicating how the observations were characterized into training and test sets in the original study; these two variables are irrelevant for the purposes of this demonstration and so can be discarded. The third column Class contains the class labels: PS (poorly-segmented) and WS (well-segmented). The last two columns are cell centroids and can be ignored. Columns 4-59 are the 58 morphological measurements available to be used as predictors. Let’s put the class labels in a vector and the predictors in their own data.frame. segClass &lt;- segmentationData$Class segData &lt;- segmentationData[,4:59] 7.3.2 Data splitting Before starting analysis we must partition the data into training and test sets, using the createDataPartition function in caret. set.seed(42) trainIndex &lt;- createDataPartition(y=segClass, times=1, p=0.5, list=F) segDataTrain &lt;- segData[trainIndex,] segDataTest &lt;- segData[-trainIndex,] segClassTrain &lt;- segClass[trainIndex] segClassTest &lt;- segClass[-trainIndex] This results in balanced class distributions within the splits: summary(segClassTrain) ## PS WS ## 650 360 summary(segClassTest) ## PS WS ## 650 359 N.B. The test set is set aside for now. It will be used only ONCE, to test the final model. 7.3.3 Identification of data quality issues Let’s check our training data set for some undesirable characteristics which may impact model performance and should be addressed through pre-processing. 7.3.3.1 Zero and near zero-variance predictors The function nearZeroVar identifies predictors that have one unique value. It also diagnoses predictors having both of the following characteristics: very few unique values relative to the number of samples the ratio of the frequency of the most common value to the frequency of the 2nd most common value is large. Such zero and near zero-variance predictors have a deleterious impact on modelling and may lead to unstable fits. nzv &lt;- nearZeroVar(segDataTrain, saveMetrics=T) nzv ## freqRatio percentUnique zeroVar nzv ## AngleCh1 1.000000 100.000000 FALSE FALSE ## AreaCh1 1.083333 37.326733 FALSE FALSE ## AvgIntenCh1 1.000000 100.000000 FALSE FALSE ## AvgIntenCh2 3.000000 99.801980 FALSE FALSE ## AvgIntenCh3 1.000000 100.000000 FALSE FALSE ## AvgIntenCh4 2.000000 99.900990 FALSE FALSE ## ConvexHullAreaRatioCh1 1.000000 98.910891 FALSE FALSE ## ConvexHullPerimRatioCh1 1.000000 100.000000 FALSE FALSE ## DiffIntenDensityCh1 1.000000 100.000000 FALSE FALSE ## DiffIntenDensityCh3 1.000000 100.000000 FALSE FALSE ## DiffIntenDensityCh4 1.000000 100.000000 FALSE FALSE ## EntropyIntenCh1 1.000000 100.000000 FALSE FALSE ## EntropyIntenCh3 1.000000 100.000000 FALSE FALSE ## EntropyIntenCh4 1.000000 100.000000 FALSE FALSE ## EqCircDiamCh1 1.083333 37.326733 FALSE FALSE ## EqEllipseLWRCh1 1.000000 100.000000 FALSE FALSE ## EqEllipseOblateVolCh1 1.000000 100.000000 FALSE FALSE ## EqEllipseProlateVolCh1 1.000000 100.000000 FALSE FALSE ## EqSphereAreaCh1 1.083333 37.326733 FALSE FALSE ## EqSphereVolCh1 1.083333 37.326733 FALSE FALSE ## FiberAlign2Ch3 1.304348 94.950495 FALSE FALSE ## FiberAlign2Ch4 7.285714 94.455446 FALSE FALSE ## FiberLengthCh1 1.000000 95.841584 FALSE FALSE ## FiberWidthCh1 1.000000 95.841584 FALSE FALSE ## IntenCoocASMCh3 1.000000 100.000000 FALSE FALSE ## IntenCoocASMCh4 1.000000 100.000000 FALSE FALSE ## IntenCoocContrastCh3 1.000000 100.000000 FALSE FALSE ## IntenCoocContrastCh4 1.000000 100.000000 FALSE FALSE ## IntenCoocEntropyCh3 1.000000 100.000000 FALSE FALSE ## IntenCoocEntropyCh4 1.000000 100.000000 FALSE FALSE ## IntenCoocMaxCh3 1.250000 94.158416 FALSE FALSE ## IntenCoocMaxCh4 1.250000 94.356436 FALSE FALSE ## KurtIntenCh1 1.000000 100.000000 FALSE FALSE ## KurtIntenCh3 1.000000 100.000000 FALSE FALSE ## KurtIntenCh4 1.000000 100.000000 FALSE FALSE ## LengthCh1 1.000000 100.000000 FALSE FALSE ## NeighborAvgDistCh1 1.000000 100.000000 FALSE FALSE ## NeighborMinDistCh1 1.166667 41.089109 FALSE FALSE ## NeighborVarDistCh1 1.000000 100.000000 FALSE FALSE ## PerimCh1 1.000000 63.762376 FALSE FALSE ## ShapeBFRCh1 1.000000 100.000000 FALSE FALSE ## ShapeLWRCh1 1.000000 100.000000 FALSE FALSE ## ShapeP2ACh1 1.000000 99.801980 FALSE FALSE ## SkewIntenCh1 1.000000 100.000000 FALSE FALSE ## SkewIntenCh3 1.000000 100.000000 FALSE FALSE ## SkewIntenCh4 1.000000 100.000000 FALSE FALSE ## SpotFiberCountCh3 1.212000 1.287129 FALSE FALSE ## SpotFiberCountCh4 1.152778 3.267327 FALSE FALSE ## TotalIntenCh1 1.000000 98.712871 FALSE FALSE ## TotalIntenCh2 1.500000 99.009901 FALSE FALSE ## TotalIntenCh3 1.000000 99.108911 FALSE FALSE ## TotalIntenCh4 1.000000 99.603960 FALSE FALSE ## VarIntenCh1 1.000000 100.000000 FALSE FALSE ## VarIntenCh3 1.000000 100.000000 FALSE FALSE ## VarIntenCh4 1.000000 100.000000 FALSE FALSE ## WidthCh1 1.000000 100.000000 FALSE FALSE 7.3.3.2 Scaling The variables in this data set are on different scales, for example: summary(segDataTrain$IntenCoocASMCh4) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.004874 0.017253 0.049458 0.101586 0.121245 0.867845 summary(segDataTrain$TotalIntenCh2) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 1 15846 49648 53143 72304 362465 In this situation it is important to centre and scale each predictor. A predictor variable is centered by subtracting the mean of the predictor from each value. To scale a predictor variable, each value is divided by its standard deviation. After centring and scaling the predictor variable has a mean of 0 and a standard deviation of 1. 7.3.3.3 Skewness Many of the predictors in the segmentation data set exhibit skewness, i.e. the distribution of their values is asymmetric, for example: qplot(segDataTrain$IntenCoocASMCh3, binwidth=0.1) + xlab(&quot;IntenCoocASMCh3&quot;) + theme_bw() Figure 7.9: Example of a predictor from the segmentation data set showing skewness. caret provides various methods for transforming skewed variables to normality, including the Box-Cox (Box and Cox 1964) and Yeo-Johnson (Yeo and Johnson 2000) transformations. 7.3.3.4 Correlated predictors Many of the variables in the segmentation data set are highly correlated. A correlogram provides a helpful visualization of the patterns of pairwise correlation within the data set. library(corrplot) ## corrplot 0.84 loaded corMat &lt;- cor(segDataTrain) corrplot(corMat, order=&quot;hclust&quot;, tl.cex=0.4) Figure 7.10: Correlogram of the segmentation data set. The preProcess function in caret has an option, corr to remove highly correlated variables. It considers the absolute values of pair-wise correlations. If two variables are highly correlated, preProcess looks at the mean absolute correlation of each variable and removes the variable with the largest mean absolute correlation. In the case of data-sets comprised of many highly correlated variables, an alternative to removing correlated predictors is the transformation of the entire data set to a lower dimensional space, using a technique such as principal component analysis (PCA). Methods for dimensionality reduction will be explored in chapter 5. 7.3.4 Fit model Generate a list of seeds. set.seed(42) seeds &lt;- vector(mode = &quot;list&quot;, length = 26) for(i in 1:25) seeds[[i]] &lt;- sample.int(1000, 50) seeds[[26]] &lt;- sample.int(1000,1) Create a list of computational options for resampling. In the interest of speed for this demonstration, we will perform 5-fold cross-validation a total of 5 times. In practice we would use a larger number of folds and repetitions. train_ctrl &lt;- trainControl(method=&quot;repeatedcv&quot;, number = 5, repeats = 5, #preProcOptions=list(cutoff=0.75), seeds = seeds) Create a grid of values of k for evaluation. tuneParam &lt;- data.frame(k=seq(5,500,10)) To deal with the issues of scaling, skewness and highly correlated predictors identified earlier, we need to pre-process the data. We will use the Yeo-Johnson transformation to reduce skewness, because it can deal with the zero values present in some of the predictors. Ideally the pre-processing procedures would be performed within each cross-validation loop, using the following command: knnFit &lt;- train(segDataTrain, segClassTrain, method=&quot;knn&quot;, preProcess = c(&quot;YeoJohnson&quot;, &quot;center&quot;, &quot;scale&quot;, &quot;corr&quot;), tuneGrid=tuneParam, trControl=train_ctrl) However, this is time-consuming, so for the purposes of this demonstration we will pre-process the entire training data-set before proceeding with training and cross-validation. transformations &lt;- preProcess(segDataTrain, method=c(&quot;YeoJohnson&quot;, &quot;center&quot;, &quot;scale&quot;, &quot;corr&quot;), cutoff=0.75) segDataTrain &lt;- predict(transformations, segDataTrain) The cutoff refers to the correlation coefficient threshold. str(segDataTrain) ## &#39;data.frame&#39;: 1010 obs. of 25 variables: ## $ AngleCh1 : num 1.045 0.873 -0.376 -0.994 1.586 ... ## $ ConvexHullPerimRatioCh1: num 0.31 -1.221 -0.363 1.22 1.113 ... ## $ EntropyIntenCh1 : num -2.443 -0.671 -1.688 0.554 0.425 ... ## $ EqEllipseOblateVolCh1 : num -0.414 1.693 0.711 -1.817 -1.667 ... ## $ FiberAlign2Ch3 : num -1.8124 0.0933 -0.9679 -0.6188 -0.8721 ... ## $ FiberAlign2Ch4 : num -1.729 -0.331 1.255 -0.291 0.463 ... ## $ FiberWidthCh1 : num -0.776 0.878 -0.779 0.712 0.758 ... ## $ IntenCoocASMCh4 : num -0.368 -0.64 -0.652 -0.665 -0.671 ... ## $ IntenCoocContrastCh3 : num 2.4777 0.0604 -0.0816 0.0634 0.6386 ... ## $ IntenCoocContrastCh4 : num 1.101 0.127 1.046 0.602 1.445 ... ## $ IntenCoocMaxCh3 : num -0.815 -0.232 -0.168 -1.366 -1.37 ... ## $ KurtIntenCh1 : num -0.97 -0.26 0.562 -0.187 0.296 ... ## $ KurtIntenCh3 : num -1.506 -1.133 -0.672 -1.908 -1.491 ... ## $ KurtIntenCh4 : num 0.68398 -0.00329 -0.09737 -0.1679 -0.79044 ... ## $ NeighborAvgDistCh1 : num 2.5376 -1.4791 -0.5357 0.1062 0.0663 ... ## $ NeighborMinDistCh1 : num 3.286 0.289 0.557 -1.679 -1.679 ... ## $ ShapeBFRCh1 : num 0.6733 -0.5448 -0.0649 0.8849 1.4669 ... ## $ ShapeLWRCh1 : num 1.23 -0.351 1.525 -1.832 -1.717 ... ## $ SkewIntenCh1 : num -0.213 -0.297 0.384 -2.41 -2.678 ... ## $ SpotFiberCountCh3 : num -0.366 1.275 1.275 -0.366 -1.573 ... ## $ TotalIntenCh2 : num -1.701 1.682 -0.233 1.107 1.019 ... ## $ VarIntenCh1 : num -2.118 -1.346 -1.917 1.062 0.856 ... ## $ VarIntenCh3 : num -1.9155 -0.1836 -0.8001 0.0478 -0.3658 ... ## $ VarIntenCh4 : num -2.304 0.332 -1.092 0.843 0.387 ... ## $ WidthCh1 : num -1.626 1.845 -0.718 -0.188 -0.333 ... Perform cross validation to find best value of k. knnFit &lt;- train(segDataTrain, segClassTrain, method=&quot;knn&quot;, tuneGrid=tuneParam, trControl=train_ctrl) knnFit ## k-Nearest Neighbors ## ## 1010 samples ## 25 predictors ## 2 classes: &#39;PS&#39;, &#39;WS&#39; ## ## No pre-processing ## Resampling: Cross-Validated (5 fold, repeated 5 times) ## Summary of sample sizes: 808, 808, 808, 808, 808, 808, ... ## Resampling results across tuning parameters: ## ## k Accuracy Kappa ## 5 0.7938614 0.5540746 ## 15 0.8053465 0.5829961 ## 25 0.8047525 0.5814369 ## 35 0.8065347 0.5843286 ## 45 0.8037624 0.5786056 ## 55 0.8035644 0.5781528 ## 65 0.8043564 0.5787198 ## 75 0.8037624 0.5777703 ## 85 0.8037624 0.5768178 ## 95 0.8063366 0.5823443 ## 105 0.8057426 0.5797978 ## 115 0.8019802 0.5709231 ## 125 0.7990099 0.5635596 ## 135 0.8001980 0.5653667 ## 145 0.8000000 0.5643285 ## 155 0.8003960 0.5650775 ## 165 0.8007921 0.5653539 ## 175 0.8013861 0.5667665 ## 185 0.8000000 0.5633628 ## 195 0.7996040 0.5623200 ## 205 0.8005941 0.5636922 ## 215 0.8003960 0.5631617 ## 225 0.8023762 0.5676501 ## 235 0.8019802 0.5661531 ## 245 0.8017822 0.5651162 ## 255 0.8011881 0.5631157 ## 265 0.7990099 0.5570764 ## 275 0.7978218 0.5536355 ## 285 0.7992079 0.5554954 ## 295 0.7990099 0.5540500 ## 305 0.7964356 0.5471107 ## 315 0.7940594 0.5401644 ## 325 0.7926733 0.5358168 ## 335 0.7928713 0.5344022 ## 345 0.7900990 0.5260618 ## 355 0.7912871 0.5268895 ## 365 0.7897030 0.5217485 ## 375 0.7877228 0.5153088 ## 385 0.7849505 0.5062599 ## 395 0.7796040 0.4911638 ## 405 0.7716832 0.4675901 ## 415 0.7689109 0.4564594 ## 425 0.7594059 0.4280275 ## 435 0.7504950 0.3989447 ## 445 0.7398020 0.3652969 ## 455 0.7316832 0.3376567 ## 465 0.7247525 0.3112618 ## 475 0.7186139 0.2871843 ## 485 0.7144554 0.2662335 ## 495 0.7059406 0.2341570 ## ## Accuracy was used to select the optimal model using the largest value. ## The final value used for the model was k = 35. plot(knnFit) Figure 7.11: Accuracy (repeated cross-validation) as a function of neighbourhood size for the segmentation training data with highly correlated predictors removed. Let’s retrieve some information on the final model. To see the optimum value of k found during the grid search, run either of the following lines: knnFit$finalModel$k ## [1] 35 knnFit$finalModel$tuneValue ## k ## 4 35 To find out which variables have been used in the final model, run: knnFit$finalModel$xNames ## [1] &quot;AngleCh1&quot; &quot;ConvexHullPerimRatioCh1&quot; ## [3] &quot;EntropyIntenCh1&quot; &quot;EqEllipseOblateVolCh1&quot; ## [5] &quot;FiberAlign2Ch3&quot; &quot;FiberAlign2Ch4&quot; ## [7] &quot;FiberWidthCh1&quot; &quot;IntenCoocASMCh4&quot; ## [9] &quot;IntenCoocContrastCh3&quot; &quot;IntenCoocContrastCh4&quot; ## [11] &quot;IntenCoocMaxCh3&quot; &quot;KurtIntenCh1&quot; ## [13] &quot;KurtIntenCh3&quot; &quot;KurtIntenCh4&quot; ## [15] &quot;NeighborAvgDistCh1&quot; &quot;NeighborMinDistCh1&quot; ## [17] &quot;ShapeBFRCh1&quot; &quot;ShapeLWRCh1&quot; ## [19] &quot;SkewIntenCh1&quot; &quot;SpotFiberCountCh3&quot; ## [21] &quot;TotalIntenCh2&quot; &quot;VarIntenCh1&quot; ## [23] &quot;VarIntenCh3&quot; &quot;VarIntenCh4&quot; ## [25] &quot;WidthCh1&quot; Let’s predict our test set using our final model. segDataTest &lt;- predict(transformations, segDataTest) test_pred &lt;- predict(knnFit, segDataTest) confusionMatrix(test_pred, segClassTest) ## Confusion Matrix and Statistics ## ## Reference ## Prediction PS WS ## PS 540 100 ## WS 110 259 ## ## Accuracy : 0.7919 ## 95% CI : (0.7655, 0.8165) ## No Information Rate : 0.6442 ## P-Value [Acc &gt; NIR] : &lt;2e-16 ## ## Kappa : 0.5488 ## Mcnemar&#39;s Test P-Value : 0.5346 ## ## Sensitivity : 0.8308 ## Specificity : 0.7214 ## Pos Pred Value : 0.8438 ## Neg Pred Value : 0.7019 ## Prevalence : 0.6442 ## Detection Rate : 0.5352 ## Detection Prevalence : 0.6343 ## Balanced Accuracy : 0.7761 ## ## &#39;Positive&#39; Class : PS ## 7.4 Regression k-nn can also be applied to the problem of regression as we will see in the following example. The BloodBrain dataset in the caret package contains data on 208 chemical compounds, organized in two objects: logBBB - a vector of the log ratio of the concentration of a chemical compound in the brain and the concentration in the blood. bbbDescr - a data frame of 134 molecular descriptors of the compounds. We’ll start by loading the data. data(BloodBrain) str(bbbDescr) ## &#39;data.frame&#39;: 208 obs. of 134 variables: ## $ tpsa : num 12 49.3 50.5 37.4 37.4 ... ## $ nbasic : int 1 0 1 0 1 1 1 1 1 1 ... ## $ negative : int 0 0 0 0 0 0 0 0 0 0 ... ## $ vsa_hyd : num 167.1 92.6 295.2 319.1 299.7 ... ## $ a_aro : int 0 6 15 15 12 11 6 12 12 6 ... ## $ weight : num 156 151 366 383 326 ... ## $ peoe_vsa.0 : num 76.9 38.2 58.1 62.2 74.8 ... ## $ peoe_vsa.1 : num 43.4 25.5 124.7 124.7 118 ... ## $ peoe_vsa.2 : num 0 0 21.7 13.2 33 ... ## $ peoe_vsa.3 : num 0 8.62 8.62 21.79 0 ... ## $ peoe_vsa.4 : num 0 23.3 17.4 0 0 ... ## $ peoe_vsa.5 : num 0 0 0 0 0 0 0 0 0 0 ... ## $ peoe_vsa.6 : num 17.24 0 8.62 8.62 8.62 ... ## $ peoe_vsa.0.1 : num 18.7 49 83.8 83.8 83.8 ... ## $ peoe_vsa.1.1 : num 43.5 0 49 68.8 36.8 ... ## $ peoe_vsa.2.1 : num 0 0 0 0 0 ... ## $ peoe_vsa.3.1 : num 0 0 0 0 0 0 0 0 0 0 ... ## $ peoe_vsa.4.1 : num 0 0 5.68 5.68 5.68 ... ## $ peoe_vsa.5.1 : num 0 13.567 2.504 0 0.137 ... ## $ peoe_vsa.6.1 : num 0 7.9 2.64 2.64 2.5 ... ## $ a_acc : int 0 2 2 2 2 2 2 2 0 2 ... ## $ a_acid : int 0 0 0 0 0 0 0 0 0 0 ... ## $ a_base : int 1 0 1 1 1 1 1 1 1 1 ... ## $ vsa_acc : num 0 13.57 8.19 8.19 8.19 ... ## $ vsa_acid : num 0 0 0 0 0 0 0 0 0 0 ... ## $ vsa_base : num 5.68 0 0 0 0 ... ## $ vsa_don : num 5.68 5.68 5.68 5.68 5.68 ... ## $ vsa_other : num 0 28.1 43.6 28.3 19.6 ... ## $ vsa_pol : num 0 13.6 0 0 0 ... ## $ slogp_vsa0 : num 18 25.4 14.1 14.1 14.1 ... ## $ slogp_vsa1 : num 0 23.3 34.8 34.8 34.8 ... ## $ slogp_vsa2 : num 3.98 23.86 0 0 0 ... ## $ slogp_vsa3 : num 0 0 76.2 76.2 76.2 ... ## $ slogp_vsa4 : num 4.41 0 3.19 3.19 3.19 ... ## $ slogp_vsa5 : num 32.9 0 9.51 0 0 ... ## $ slogp_vsa6 : num 0 0 0 0 0 0 0 0 0 0 ... ## $ slogp_vsa7 : num 0 70.6 148.1 144 140.7 ... ## $ slogp_vsa8 : num 113.2 0 75.5 75.5 75.5 ... ## $ slogp_vsa9 : num 33.3 41.3 28.3 55.5 26 ... ## $ smr_vsa0 : num 0 23.86 12.63 3.12 3.12 ... ## $ smr_vsa1 : num 18 25.4 27.8 27.8 27.8 ... ## $ smr_vsa2 : num 4.41 0 0 0 0 ... ## $ smr_vsa3 : num 3.98 5.24 8.43 8.43 8.43 ... ## $ smr_vsa4 : num 0 20.8 29.6 21.4 20.3 ... ## $ smr_vsa5 : num 113.2 70.6 235.1 235.1 234.6 ... ## $ smr_vsa6 : num 0 5.26 76.25 76.25 76.25 ... ## $ smr_vsa7 : num 66.2 33.3 0 31.3 0 ... ## $ tpsa.1 : num 16.6 49.3 51.7 38.6 38.6 ... ## $ logp.o.w. : num 2.948 0.889 4.439 5.254 3.8 ... ## $ frac.anion7. : num 0 0.001 0 0 0 0 0.001 0 0 0 ... ## $ frac.cation7. : num 0.999 0 0.986 0.986 0.986 0.986 0.996 0.946 0.999 0.976 ... ## $ andrewbind : num 3.4 -3.3 12.8 12.8 10.3 10 10.4 15.9 12.9 9.5 ... ## $ rotatablebonds : int 3 2 8 8 8 8 8 7 4 5 ... ## $ mlogp : num 2.5 1.06 4.66 3.82 3.27 ... ## $ clogp : num 2.97 0.494 5.137 5.878 4.367 ... ## $ mw : num 155 151 365 382 325 ... ## $ nocount : int 1 3 5 4 4 4 4 3 2 4 ... ## $ hbdnr : int 1 2 1 1 1 1 2 1 1 0 ... ## $ rule.of.5violations : int 0 0 1 1 0 0 0 0 1 0 ... ## $ alert : int 0 0 0 0 0 0 0 0 0 0 ... ## $ prx : int 0 1 6 2 2 2 1 0 0 4 ... ## $ ub : num 0 3 5.3 5.3 4.2 3.6 3 4.7 4.2 3 ... ## $ pol : int 0 2 3 3 2 2 2 3 4 1 ... ## $ inthb : int 0 0 0 0 0 0 1 0 0 0 ... ## $ adistm : num 0 395 1365 703 746 ... ## $ adistd : num 0 10.9 25.7 10 10.6 ... ## $ polar_area : num 21.1 117.4 82.1 65.1 66.2 ... ## $ nonpolar_area : num 379 248 638 668 602 ... ## $ psa_npsa : num 0.0557 0.4743 0.1287 0.0974 0.11 ... ## $ tcsa : num 0.0097 0.0134 0.0111 0.0108 0.0118 0.0111 0.0123 0.0099 0.0106 0.0115 ... ## $ tcpa : num 0.1842 0.0417 0.0972 0.1218 0.1186 ... ## $ tcnp : num 0.0103 0.0198 0.0125 0.0119 0.013 0.0125 0.0162 0.011 0.0109 0.0122 ... ## $ ovality : num 1.1 1.12 1.3 1.3 1.27 ... ## $ surface_area : num 400 365 720 733 668 ... ## $ volume : num 656 555 1224 1257 1133 ... ## $ most_negative_charge: num -0.617 -0.84 -0.801 -0.761 -0.857 ... ## $ most_positive_charge: num 0.307 0.497 0.541 0.48 0.455 ... ## $ sum_absolute_charge : num 3.89 4.89 7.98 7.93 7.85 ... ## $ dipole_moment : num 1.19 4.21 3.52 3.15 3.27 ... ## $ homo : num -9.67 -8.96 -8.63 -8.56 -8.67 ... ## $ lumo : num 3.4038 0.1942 0.0589 -0.2651 0.3149 ... ## $ hardness : num 6.54 4.58 4.34 4.15 4.49 ... ## $ ppsa1 : num 349 223 518 508 509 ... ## $ ppsa2 : num 679 546 2066 2013 1999 ... ## $ ppsa3 : num 31 42.3 64 61.7 61.6 ... ## $ pnsa1 : num 51.1 141.8 202 225.4 158.8 ... ## $ pnsa2 : num -99.3 -346.9 -805.9 -894 -623.3 ... ## $ pnsa3 : num -10.5 -44 -43.8 -42 -39.8 ... ## $ fpsa1 : num 0.872 0.611 0.719 0.693 0.762 ... ## $ fpsa2 : num 1.7 1.5 2.87 2.75 2.99 ... ## $ fpsa3 : num 0.0774 0.1159 0.0888 0.0842 0.0922 ... ## $ fnsa1 : num 0.128 0.389 0.281 0.307 0.238 ... ## $ fnsa2 : num -0.248 -0.951 -1.12 -1.22 -0.933 ... ## $ fnsa3 : num -0.0262 -0.1207 -0.0608 -0.0573 -0.0596 ... ## $ wpsa1 : num 139.7 81.4 372.7 372.1 340.1 ... ## $ wpsa2 : num 272 199 1487 1476 1335 ... ## $ wpsa3 : num 12.4 15.4 46 45.2 41.1 ... ## $ wnsa1 : num 20.4 51.8 145.4 165.3 106 ... ## $ wnsa2 : num -39.8 -126.6 -580.1 -655.3 -416.3 ... ## [list output truncated] str(logBBB) ## num [1:208] 1.08 -0.4 0.22 0.14 0.69 0.44 -0.43 1.38 0.75 0.88 ... Evidently the variables are on different scales which is problematic for k-nn. 7.4.1 Partition data Before proceeding the data set must be partitioned into a training and a test set. set.seed(42) trainIndex &lt;- createDataPartition(y=logBBB, times=1, p=0.8, list=F) descrTrain &lt;- bbbDescr[trainIndex,] concRatioTrain &lt;- logBBB[trainIndex] descrTest &lt;- bbbDescr[-trainIndex,] concRatioTest &lt;- logBBB[-trainIndex] 7.4.2 Data pre-processing Are there any issues with the data that might affect model fitting? Let’s start by considering correlation. cm &lt;- cor(descrTrain) corrplot(cm, order=&quot;hclust&quot;, tl.pos=&quot;n&quot;) Figure 7.12: Correlogram of the chemical compound descriptors. The number of variables exhibiting a pair-wise correlation coefficient above 0.75 can be determined: highCorr &lt;- findCorrelation(cm, cutoff=0.75) length(highCorr) ## [1] 68 A check for the presence of missing values: anyNA(descrTrain) ## [1] FALSE Detection of near zero variance predictors: nearZeroVar(descrTrain) ## [1] 3 16 22 25 50 60 We know there are issues with scaling, and the presence of highly correlated predictors and near zero variance predictors. These problems are resolved by pre-processing. First we define the procesing steps. transformations &lt;- preProcess(descrTrain, method=c(&quot;center&quot;, &quot;scale&quot;, &quot;corr&quot;, &quot;nzv&quot;), cutoff=0.75) Then this transformation can be applied to the compound descriptor data set. descrTrain &lt;- predict(transformations, descrTrain) 7.4.3 Search for optimum k The optimum value of k can be found by cross-validation, following similar methodology to that used to find the best k for classification. We’ll start by generating seeds to make this example reproducible. set.seed(42) seeds &lt;- vector(mode = &quot;list&quot;, length = 26) for(i in 1:25) seeds[[i]] &lt;- sample.int(1000, 10) seeds[[26]] &lt;- sample.int(1000,1) Ten values of k will be evaluated using 5 repeats of 5-fold cross-validation. knnTune &lt;- train(descrTrain, concRatioTrain, method=&quot;knn&quot;, tuneGrid = data.frame(.k=1:10), trControl = trainControl(method=&quot;repeatedcv&quot;, number = 5, repeats = 5, seeds=seeds, preProcOptions=list(cutoff=0.75)) ) knnTune ## k-Nearest Neighbors ## ## 168 samples ## 61 predictors ## ## No pre-processing ## Resampling: Cross-Validated (5 fold, repeated 5 times) ## Summary of sample sizes: 135, 135, 134, 135, 133, 135, ... ## Resampling results across tuning parameters: ## ## k RMSE Rsquared MAE ## 1 0.6413320 0.4132466 0.4642574 ## 2 0.5957949 0.4374605 0.4500493 ## 3 0.5888289 0.4382933 0.4410306 ## 4 0.5841354 0.4364264 0.4409199 ## 5 0.5867094 0.4292670 0.4494250 ## 6 0.5988751 0.4038387 0.4593885 ## 7 0.6015944 0.3971522 0.4554279 ## 8 0.6059279 0.3899379 0.4587389 ## 9 0.6108446 0.3813605 0.4622043 ## 10 0.6145065 0.3769979 0.4647581 ## ## RMSE was used to select the optimal model using the smallest value. ## The final value used for the model was k = 4. The Root Mean Squared Error (RMSE) measures the differences between the values predicted by the model and the values actually observed. More specifically, it represents the sample standard deviation of the difference between the predicted values and observed values. plot(knnTune) Figure 7.13: Root Mean Squared Error as a function of neighbourhood size. 7.4.4 Use model to make predictions Before attempting to predict the blood/brain concentration ratios of the test samples, the descriptors in the test set must be transformed using the same pre-processing procedure that was applied to the descriptors in the training set. descrTest &lt;- predict(transformations, descrTest) Use model to predict outcomes (concentration ratios) of the test set. test_pred &lt;- predict(knnTune, descrTest) Prediction performance can be visualized in a scatterplot. qplot(concRatioTest, test_pred) + xlab(&quot;observed&quot;) + ylab(&quot;predicted&quot;) + theme_bw() Figure 7.14: Concordance between observed concentration ratios and those predicted by k-nn regression. We can also measure correlation between observed and predicted values. cor(concRatioTest, test_pred) ## [1] 0.7278034 7.5 Exercises 7.5.1 Exercise 1 The seeds data set https://archive.ics.uci.edu/ml/datasets/seeds contains morphological measurements on the kernels of three varieties of wheat: Kama, Rosa and Canadian. Load the data into your R session using: load(&quot;data/wheat_seeds/wheat_seeds.Rda&quot;) The data are split into two objects. morphometrics is a data.frame containing the morphological measurements: str(morphometrics) ## &#39;data.frame&#39;: 210 obs. of 7 variables: ## $ area : num 15.3 14.9 14.3 13.8 16.1 ... ## $ perimeter : num 14.8 14.6 14.1 13.9 15 ... ## $ compactness : num 0.871 0.881 0.905 0.895 0.903 ... ## $ kernLength : num 5.76 5.55 5.29 5.32 5.66 ... ## $ kernWidth : num 3.31 3.33 3.34 3.38 3.56 ... ## $ asymCoef : num 2.22 1.02 2.7 2.26 1.35 ... ## $ grooveLength: num 5.22 4.96 4.83 4.8 5.17 ... variety is a factor containing the corresponding classes: str(variety) ## Factor w/ 3 levels &quot;Canadian&quot;,&quot;Kama&quot;,..: 2 2 2 2 2 2 2 2 2 2 ... Your task is to build a k-nn classifier which will predict the variety of wheat from a seeds morphological measurements. You do not need to perform feature selection, but you will want to pre-process the data. Solutions to exercises can be found in appendix F. References "],
["svm.html", "8 Support vector machines 8.1 Introduction 8.2 Support vector classifier 8.3 Support Vector Machine 8.4 Example - training a classifier 8.5 Example - regression 8.6 Further reading 8.7 Exercises", " 8 Support vector machines 8.1 Introduction Support vector machines (SVMs) are models of supervised learning, applicable to both classification and regression problems. The SVM is an extension of the support vector classifier (SVC), which is turn is an extension of the maximum margin classifier. 8.1.1 Maximum margin classifier Let’s start by definining a hyperplane. In p-dimensional space a hyperplane is a flat affine subspace of p-1. Figure 8.1 shows three separating hyperplanes and objects of two different classes. A separating hyperplane forms a natural linear decision boundary, classifying new objects according to which side of the line they are located. Figure 8.1: Left: two classes of observations (blue, purple) and three separating hyperplanes. Right: separating hyperplane shown as black line and grid indicates decision rule. Source: http://www-bcf.usc.edu/~gareth/ISL/ If the classes of observations can be separated by a hyperplane, then there will in fact be an infinite number of hyperplanes. So which of the possible hyperplanes do we choose to be our decision boundary? The maximal margin hyperplane is the separating hyperplane that is farthest from the training observations. The perpendicular distance from a given hyperplane to the nearest training observation is known as the margin. The maximal margin hyperplane is the separating hyperplane for which the margin is largest. Figure 8.2: Maximal margin hyperplane shown as solid line. Margin is the distance from the solid line to either of the dashed lines. The support vectors are the points on the dashed line. Source: http://www-bcf.usc.edu/~gareth/ISL/ Figure 8.2 shows three training observations that are equidistant from the maximal margin hyperplane and lie on the dashed lines indicating the margin. These are the support vectors. If these points were moved slightly, the maximal margin hyperplane would also move, hence the term support. The maximal margin hyperplane is set by the support vectors alone; it is not influenced by any other observations. The maximal margin hyperplane is a natural decision boundary, but only if a separating hyperplane exists. In practice there may be non separable cases which prevent the use of the maximal margin classifier. Figure 8.3: The two classes cannot be separated by a hyperplane and so the maximal margin classifier cannot be used. Source: http://www-bcf.usc.edu/~gareth/ISL/ 8.2 Support vector classifier Even if a separating hyperplane exists, it may not be the best decision boundary. The maximal margin classifier is extremely sensitive to individual observations, so may overfit the training data. Figure 8.4: Left: two classes of observations and a maximum margin hyperplane (solid line). Right: Hyperplane (solid line) moves after the addition of a new observation (original hyperplane is dashed line). Source: http://www-bcf.usc.edu/~gareth/ISL/ It would be better to choose a classifier based on a hyperplane that: is more robust to individual observations provides better classification of most of the training variables In other words, we might tolerate some misclassifications if the prediction of the remaining observations is more reliable. The support vector classifier does this by allowing some observations to be on the wrong side of the margin or even on the wrong side of the hyperplane. Observations on the wrong side of the hyperplane are misclassifications. Figure 8.5: Left: observations on the wrong side of the margin. Right: observations on the wrong side of the margin and observations on the wrong side of the hyperplane. Source: http://www-bcf.usc.edu/~gareth/ISL/ The support vector classifier has a tuning parameter, C, that determines the number and severity of the violations to the margin. If C = 0, then no violations to the margin will be tolerated, which is equivalent to the maximal margin classifier. As C increases, the classifier becomes more tolerant of violations to the margin, and so the margin widens. The optimal value of C is chosen through cross-validation. C is described as a tuning parameter, because it controls the bias-variance trade-off: a small C results in narrow margins that are rarely violated; the model will have low bias, but high variance. as C increases the margins widen allowing more violations; the bias of the model will increase, but its variance will decrease. The support vectors are the observations that lie directly on the margin, or on the wrong side of the margin for their class. The only observations that affect the classifier are the support vectors. As C increases, the margin widens and the number of support vectors increases. In other words, when C increases more observations are involved in determining the decision boundary of the classifier. Figure 8.6: Margin of a support vector classifier changing with tuning parameter C. Largest value of C was used in the top left panel, and smaller values in the top right, bottom left and bottom right panels. Source: http://www-bcf.usc.edu/~gareth/ISL/ 8.3 Support Vector Machine The support vector classifier performs well if we have linearly separable classes, however this isn’t always the case. Figure 8.7: Two classes of observations with a non-linear boundary between them. The SVM uses the kernel trick to operate in a higher dimensional space, without ever computing the coordinates of the data in that space. Figure 8.8: Kernel machine. By Alisneaky - Own work, CC0, https://commons.wikimedia.org/w/index.php?curid=14941564 Figure 8.9: Left: SVM with polynomial kernel of degree 3. Right: SVM with radial kernel. Source: http://www-bcf.usc.edu/~gareth/ISL/ 8.4 Example - training a classifier Training of an SVM will be demonstrated on a 2-dimensional simulated data set, with a non-linear decision boundary. 8.4.1 Setup environment Load required libraries library(caret) ## Loading required package: lattice ## Loading required package: ggplot2 library(RColorBrewer) library(ggplot2) library(doMC) ## Loading required package: foreach ## Loading required package: iterators ## Loading required package: parallel library(pROC) ## Type &#39;citation(&quot;pROC&quot;)&#39; for a citation. ## ## Attaching package: &#39;pROC&#39; ## The following objects are masked from &#39;package:stats&#39;: ## ## cov, smooth, var library(e1071) Initialize parallel processing registerDoMC(detectCores()) getDoParWorkers() ## [1] 8 8.4.2 Partition data Load data moons &lt;- read.csv(&quot;data/sim_data_svm/moons.csv&quot;, header=F) str(moons) ## &#39;data.frame&#39;: 400 obs. of 3 variables: ## $ V1: num -0.496 1.827 1.322 -1.138 -0.21 ... ## $ V2: num 0.985 -0.501 -0.397 0.192 -0.145 ... ## $ V3: Factor w/ 2 levels &quot;A&quot;,&quot;B&quot;: 1 2 2 1 2 1 1 2 1 2 ... V1 and V2 are the predictors; V3 is the class. Partition data into training and test set set.seed(42) trainIndex &lt;- createDataPartition(y=moons$V3, times=1, p=0.7, list=F) moonsTrain &lt;- moons[trainIndex,] moonsTest &lt;- moons[-trainIndex,] summary(moonsTrain$V3) ## A B ## 140 140 summary(moonsTest$V3) ## A B ## 60 60 8.4.3 Visualize training data point_shapes &lt;- c(15,17) bp &lt;- brewer.pal(3,&quot;Dark2&quot;) point_colours &lt;- ifelse(moonsTrain$V3==&quot;A&quot;, bp[1], bp[2]) point_shapes &lt;- ifelse(moonsTrain$V3==&quot;A&quot;, 15, 17) point_size = 2 ggplot(moonsTrain, aes(V1,V2)) + geom_point(col=point_colours, shape=point_shapes, size=point_size) + theme_bw() + theme(plot.title = element_text(size=25, face=&quot;bold&quot;), axis.text=element_text(size=15), axis.title=element_text(size=20,face=&quot;bold&quot;)) Figure 8.10: Scatterplot of the training data 8.4.4 Define a custom model Caret has over two hundred built in models, including several support vector machines: https://topepo.github.io/caret/available-models.html However, despite this wide range of options, you may occasionally need to define your own model. Caret does not currently have a radial SVM implemented using the e1071 library, so we will define one here. svmRadialE1071 &lt;- list( label = &quot;Support Vector Machines with Radial Kernel - e1071&quot;, library = &quot;e1071&quot;, type = c(&quot;Regression&quot;, &quot;Classification&quot;), parameters = data.frame(parameter=&quot;cost&quot;, class=&quot;numeric&quot;, label=&quot;Cost&quot;), grid = function (x, y, len = NULL, search = &quot;grid&quot;) { if (search == &quot;grid&quot;) { out &lt;- expand.grid(cost = 2^((1:len) - 3)) } else { out &lt;- data.frame(cost = 2^runif(len, min = -5, max = 10)) } out }, loop=NULL, fit=function (x, y, wts, param, lev, last, classProbs, ...) { if (any(names(list(...)) == &quot;probability&quot;) | is.numeric(y)) { out &lt;- e1071::svm(x = as.matrix(x), y = y, kernel = &quot;radial&quot;, cost = param$cost, ...) } else { out &lt;- e1071::svm(x = as.matrix(x), y = y, kernel = &quot;radial&quot;, cost = param$cost, probability = classProbs, ...) } out }, predict = function (modelFit, newdata, submodels = NULL) { predict(modelFit, newdata) }, prob = function (modelFit, newdata, submodels = NULL) { out &lt;- predict(modelFit, newdata, probability = TRUE) attr(out, &quot;probabilities&quot;) }, predictors = function (x, ...) { out &lt;- if (!is.null(x$terms)) predictors.terms(x$terms) else x$xNames if (is.null(out)) out &lt;- names(attr(x, &quot;scaling&quot;)$x.scale$`scaled:center`) if (is.null(out)) out &lt;- NA out }, tags = c(&quot;Kernel Methods&quot;, &quot;Support Vector Machines&quot;, &quot;Regression&quot;, &quot;Classifier&quot;, &quot;Robust Methods&quot;), levels = function(x) x$levels, sort = function(x) { x[order(x$cost), ] } ) Note that the radial SVM model we have defined has only one tuning parameter, cost (C). If we do not define the kernel parameter gamma, e1071 will automatically calculate it as 1/(data dimension); i.e. if we have 58 predictors, gamma will be 1/58 = 0.01724. 8.4.5 Model cross-validation and tuning Set seeds for reproducibility. We will be trying 9 values of the tuning parameter with 10 repeats of 10 fold cross-validation, so we need the following list of seeds. set.seed(42) seeds &lt;- vector(mode = &quot;list&quot;, length = 26) for(i in 1:25) seeds[[i]] &lt;- sample.int(1000, 9) seeds[[26]] &lt;- sample.int(1000,1) We will pass the twoClassSummary function into model training through trainControl. Additionally we would like the model to predict class probabilities so that we can calculate the ROC curve, so we use the classProbs option. cvCtrl &lt;- trainControl(method = &quot;repeatedcv&quot;, repeats = 5, number = 5, summaryFunction = twoClassSummary, classProbs = TRUE, seeds=seeds) We set the method of the train function to svmRadial to specify a radial kernel SVM. In this implementation we only have to tune one parameter, cost. The default grid of cost parameters start at 0.25 and double at each iteration. Choosing tuneLength = 9 will give us cost parameters of 0.25, 0.5, 1, 2, 4, 8, 16, 32 and 64. svmTune &lt;- train(x = moonsTrain[,c(1:2)], y = moonsTrain[,3], method = svmRadialE1071, tuneLength = 9, preProc = c(&quot;center&quot;, &quot;scale&quot;), metric = &quot;ROC&quot;, trControl = cvCtrl) svmTune ## Support Vector Machines with Radial Kernel - e1071 ## ## 280 samples ## 2 predictors ## 2 classes: &#39;A&#39;, &#39;B&#39; ## ## Pre-processing: centered (2), scaled (2) ## Resampling: Cross-Validated (5 fold, repeated 5 times) ## Summary of sample sizes: 224, 224, 224, 224, 224, 224, ... ## Resampling results across tuning parameters: ## ## cost ROC Sens Spec ## 0.25 0.9337245 0.8485714 0.8900000 ## 0.50 0.9437755 0.8571429 0.8942857 ## 1.00 0.9517857 0.8542857 0.8971429 ## 2.00 0.9584184 0.8685714 0.9014286 ## 4.00 0.9611224 0.8785714 0.9014286 ## 8.00 0.9633673 0.8842857 0.8985714 ## 16.00 0.9633673 0.8900000 0.8957143 ## 32.00 0.9629592 0.8914286 0.8914286 ## 64.00 0.9609184 0.8771429 0.8871429 ## ## ROC was used to select the optimal model using the largest value. ## The final value used for the model was cost = 8. svmTune$finalModel ## ## Call: ## svm.default(x = as.matrix(x), y = y, kernel = &quot;radial&quot;, cost = param$cost, ## probability = classProbs) ## ## ## Parameters: ## SVM-Type: C-classification ## SVM-Kernel: radial ## cost: 8 ## gamma: 0.5 ## ## Number of Support Vectors: 81 8.4.6 Prediction performance measures SVM accuracy profile plot(svmTune, metric = &quot;ROC&quot;, scales = list(x = list(log =2))) Figure 8.11: SVM accuracy profile for moons data set. Predictions on test set. svmPred &lt;- predict(svmTune, moonsTest[,c(1:2)]) confusionMatrix(svmPred, moonsTest[,3]) ## Confusion Matrix and Statistics ## ## Reference ## Prediction A B ## A 56 6 ## B 4 54 ## ## Accuracy : 0.9167 ## 95% CI : (0.8521, 0.9593) ## No Information Rate : 0.5 ## P-Value [Acc &gt; NIR] : &lt;2e-16 ## ## Kappa : 0.8333 ## Mcnemar&#39;s Test P-Value : 0.7518 ## ## Sensitivity : 0.9333 ## Specificity : 0.9000 ## Pos Pred Value : 0.9032 ## Neg Pred Value : 0.9310 ## Prevalence : 0.5000 ## Detection Rate : 0.4667 ## Detection Prevalence : 0.5167 ## Balanced Accuracy : 0.9167 ## ## &#39;Positive&#39; Class : A ## Get predicted class probabilities so we can build ROC curve. svmProbs &lt;- predict(svmTune, moonsTest[,c(1:2)], type=&quot;prob&quot;) head(svmProbs) ## A B ## 2 0.01529839 0.984701610 ## 5 0.05702411 0.942975894 ## 6 0.97385690 0.026143097 ## 7 0.99341309 0.006586907 ## 8 0.03357317 0.966426827 ## 9 0.94400432 0.055995678 Build a ROC curve. svmROC &lt;- roc(moonsTest[,3], svmProbs[,&quot;A&quot;]) auc(svmROC) ## Area under the curve: 0.9578 Plot ROC curve, including the threshold with the highest sum sensitivity + specificity. plot(svmROC, type = &quot;S&quot;, print.thres = 0.5, print.thres.col = &quot;blue&quot;, print.thres.pch = 19, print.thres.cex=1.5) Figure 8.12: SVM accuracy profile. Sensitivity (true positive rate) TPR = TP/P = TP/(TP+FN) Specificity (true negative rate) SPC = TN/N = TN/(TN+FP) Calculate area under ROC curve. auc(svmROC) ## Area under the curve: 0.9578 8.4.7 Plot decision boundary Create a grid so we can predict across the full range of our variables V1 and V2. gridSize &lt;- 150 v1limits &lt;- c(min(moons$V1),max(moons$V1)) tmpV1 &lt;- seq(v1limits[1],v1limits[2],len=gridSize) v2limits &lt;- c(min(moons$V2), max(moons$V2)) tmpV2 &lt;- seq(v2limits[1],v2limits[2],len=gridSize) xgrid &lt;- expand.grid(tmpV1,tmpV2) names(xgrid) &lt;- names(moons)[1:2] Predict values of all elements of grid. V3 &lt;- as.numeric(predict(svmTune, xgrid)) xgrid &lt;- cbind(xgrid, V3) Plot point_shapes &lt;- c(15,17) point_colours &lt;- brewer.pal(3,&quot;Dark2&quot;) point_size = 2 trainClassNumeric &lt;- ifelse(moonsTrain$V3==&quot;A&quot;, 1, 2) testClassNumeric &lt;- ifelse(moonsTest$V3==&quot;A&quot;, 1, 2) ggplot(xgrid, aes(V1,V2)) + geom_point(col=point_colours[V3], shape=16, size=0.3) + geom_point(data=moonsTrain, aes(V1,V2), col=point_colours[trainClassNumeric], shape=point_shapes[trainClassNumeric], size=point_size) + geom_contour(data=xgrid, aes(x=V1, y=V2, z=V3), breaks=1.5, col=&quot;grey30&quot;) + ggtitle(&quot;train&quot;) + theme_bw() + theme(plot.title = element_text(size=25, face=&quot;bold&quot;), axis.text=element_text(size=15), axis.title=element_text(size=20,face=&quot;bold&quot;)) ggplot(xgrid, aes(V1,V2)) + geom_point(col=point_colours[V3], shape=16, size=0.3) + geom_point(data=moonsTest, aes(V1,V2), col=point_colours[testClassNumeric], shape=point_shapes[testClassNumeric], size=point_size) + geom_contour(data=xgrid, aes(x=V1, y=V2, z=V3), breaks=1.5, col=&quot;grey30&quot;) + ggtitle(&quot;test&quot;) + theme_bw() + theme(plot.title = element_text(size=25, face=&quot;bold&quot;), axis.text=element_text(size=15), axis.title=element_text(size=20,face=&quot;bold&quot;)) Figure 8.13: Decision boundary created by radial kernel SVM. 8.5 Example - regression This example serves to demonstrate the use of SVMs in regression, but perhaps more importantly, it highlights the power and flexibility of the caret package. Earlier we used k-NN for a regression analysis of the BloodBrain dataset (see section 7.4). We will repeat the regression analysis, but this time we will fit a radial kernel SVM. Remarkably, a re-run of this analysis using a completely different type of model, requires changes to only two lines of code. The pre-processing steps and generation of seeds are identical, therefore if the data were still in memory, we could skip this next block of code: data(BloodBrain) set.seed(42) trainIndex &lt;- createDataPartition(y=logBBB, times=1, p=0.8, list=F) descrTrain &lt;- bbbDescr[trainIndex,] concRatioTrain &lt;- logBBB[trainIndex] descrTest &lt;- bbbDescr[-trainIndex,] concRatioTest &lt;- logBBB[-trainIndex] transformations &lt;- preProcess(descrTrain, method=c(&quot;center&quot;, &quot;scale&quot;, &quot;corr&quot;, &quot;nzv&quot;), cutoff=0.75) descrTrain &lt;- predict(transformations, descrTrain) set.seed(42) seeds &lt;- vector(mode = &quot;list&quot;, length = 26) for(i in 1:25) seeds[[i]] &lt;- sample.int(1000, 50) seeds[[26]] &lt;- sample.int(1000,1) In the arguments to the train function we change method from knn to svmRadialE1071. The tunegrid parameter is replaced with tuneLength = 9. Now we are ready to fit an SVM model. svmTune2 &lt;- train(descrTrain, concRatioTrain, method=svmRadialE1071, tuneLength = 9, trControl = trainControl(method=&quot;repeatedcv&quot;, number = 5, repeats = 5, seeds=seeds, preProcOptions=list(cutoff=0.75) ) ) svmTune2 ## Support Vector Machines with Radial Kernel - e1071 ## ## 168 samples ## 61 predictors ## ## No pre-processing ## Resampling: Cross-Validated (5 fold, repeated 5 times) ## Summary of sample sizes: 134, 135, 135, 134, 134, 135, ... ## Resampling results across tuning parameters: ## ## cost RMSE Rsquared MAE ## 0.25 0.5971200 0.4824548 0.4414628 ## 0.50 0.5626587 0.5069825 0.4131150 ## 1.00 0.5471035 0.5087939 0.4074223 ## 2.00 0.5399000 0.5129105 0.4055802 ## 4.00 0.5356158 0.5197415 0.4029454 ## 8.00 0.5327136 0.5240834 0.4034633 ## 16.00 0.5325896 0.5242726 0.4035651 ## 32.00 0.5325896 0.5242726 0.4035651 ## 64.00 0.5325896 0.5242726 0.4035651 ## ## RMSE was used to select the optimal model using the smallest value. ## The final value used for the model was cost = 16. plot(svmTune2) Figure 8.14: Root Mean Squared Error as a function of cost. Use model to predict outcomes, after first pre-processing the test set. descrTest &lt;- predict(transformations, descrTest) test_pred &lt;- predict(svmTune2, descrTest) Prediction performance can be visualized in a scatterplot. qplot(concRatioTest, test_pred) + xlab(&quot;observed&quot;) + ylab(&quot;predicted&quot;) + theme_bw() Figure 8.15: Concordance between observed concentration ratios and those predicted by radial kernel SVM. We can also measure correlation between observed and predicted values. cor(concRatioTest, test_pred) ## [1] 0.8078836 8.6 Further reading An Introduction to Statistical Learning 8.7 Exercises 8.7.1 Exercise 1 In this exercise we will return to the cell segmentation data set that we attempted to classify using k-nn in section 7.3 of the nearest neighbours chapter. data(segmentationData) The aim of the exercise is to build a binary classifier to predict the quality of segmentation (poorly segmented or well segmented) based on the various morphological features. Do not worry about feature selection, but you may want to pre-process the data. Use a radial SVM model and tune over the cost function C. Produce a ROC curve to show the performance of the classifier on the test set. Solutions to exercises can be found in appendix G "],
["decision-trees.html", "9 Decision trees and random forests 9.1 Decision Trees 9.2 Random Forest 9.3 Exercises", " 9 Decision trees and random forests 9.1 Decision Trees What is a Decision Tree? Decision tree or recursive partitioning is a supervised graph based algorithm to represent choices and the results of the choices in the form of a tree. The nodes in the graph represent an event or choice and it is referred to as a leaf and the set of decisions made at the node is reffered to as branches. Decision trees map non-linear relationships and the hierarchial leaves and branches makes a Tree. It is one of the most widely used tool in ML for predictive analytics. Examples of use of decision tress are − predicting an email as spam or not spam, predicting whether a tumor is cancerous or not. Figure 9.1: Decision Tree Image source: analyticsvidhya.com How does it work? A model is first created with training data and then a set of validation data is used to verify and improve the model. R has many packages, such as ctree, rpart, tree, and so on, which are used to create and visualize decision trees. Figure 9.2: Example of a decision Tree Image source: analyticsvidhya.com Example of a decision tree In this problem (Figure 6.2), we need to segregate students who play cricket in their leisure time based on highly significant input variable among all three. The decision tree algorithm will initially segregate the students based on all values of three variable (Gender, Class, and Height) and identify the variable, which creates the best homogeneous sets of students (which are heterogeneous to each other). In the snapshot above, you can see that variable Gender is able to identify best homogeneous sets compared to the other two variables. There are a number of decision tree algorithms. We have to choose them based on our dataset. If the dependent variable is categorical, then we have to use a categorical variable decision tree. If the dependent variable is continuous, then we have to use a continuos variable deicsion tree. The above example is of the categorical variable decision tree type. A simple R code for decision tree looks like this: library(rpart) x &lt;- cbind(x_train,y_train) # grow tree fit &lt;- rpart(y_train ~ ., data = x,method=“class”) summary(fit) #Predict Output predicted= predict(fit,x_test) Where: y_train – represents dependent variable. x_train – represents independent variable x – represents training data. Terminologies related to decision trees Root nodule: the entire population that can get further divided into homogenous sets Splitting: process of diving a node into two or more sub-nodes Decision node: When a sub-node splits into further sub-nodes Leaf or terminal node: when a node does not split further it is called a terminal node. Prunning: A loose stopping crieteria is used to contruct the tree and then the tree is cut back by removing branches that do not contribute to the generalisation accuracy. Branch: a sub-section of an entire tree How does a tree decide where to split? The classification tree searches through each dependent variable to find a single variable that splits the data into two or more groups and this process is repeated until the stopping criteria is invoked. The decision of making strategic splits heavily affects a tree’s accuracy. The decision criteria is different for classification and regression trees. Decision trees use multiple algorithms to decide to split a node in two or more sub-nodes. The common goal for these algorithms is the creation of sub-nodes with increased homogeneity. In other words, we can say that purity of the node increases with respect to the target variable. Decision tree splits the nodes on all available variables and then selects the split which results in most homogeneous sub-nodes. Commonly used algorithms to decide where to split Gini Index If we select two items from a population at random then they must be of same class and the probability for this is 1 if population is pure. It works with categorical target variable “Success” or “Failure”. It performs only Binary splits Higher the value of Gini higher the homogeneity. CART (Classification and Regression Tree) uses Gini method to create binary splits. Step 1: Calculate Gini for sub-nodes, using formula sum of square of probability for success and failure  \\[p^2+q^2\\]. Step 2: Calculate Gini for split using weighted Gini score of each node of that split. Chi-Square It is an algorithm to find out the statistical significance between the differences between sub-nodes and parent node. We measure it by sum of squares of standardized differences between observed and expected frequencies of target variable. It works with categorical target variable “Success” or “Failure”. It can perform two or more splits. Higher the value of Chi-Square higher the statistical significance of differences between sub-node and Parent node. Chi-Square of each node is calculated using formula, Chi-square = \\[\\sum(Actual – Expected)^2 / Expected\\] Steps to Calculate Chi-square for a split: Calculate Chi-square for individual node by calculating the deviation for Success and Failure both Calculated Chi-square of Split using Sum of all Chi-square of success and Failure of each node of the split Information Gain The more homogenous something is the less information is needed to describe it and hence it has gained information. Information theory has a measure to define this degree of disorganization in a system and it is known as Entropy. If a sample is completely homogeneous, then the entropy is zero and if it is equally divided (50% – 50%), it has entropy of one. Entropy can be calculated using formula \\[Entropy = -plog_2p - qlog_2q\\] Where p and q are probablity of success and failure Reduction in Variance Reduction in variance is an algorithm used for continuous target variables (regression problems). This algorithm uses the standard formula of variance to choose the best split. The split with lower variance is selected as the criteria to split the population: Advantages of decision tree Simple to understand and use Algorithms are robust to noisy data Useful in data exploration decision tree is ‘non parametric’ in nature i.e. does not have any assumptions about the distribution of the variables Disadvantages of decision tree 1.Overfitting is the common disadvantage of decision trees. It is taken care of partially by constraining the model parameter and by prunning. 2. It is not ideal for continuous variables as in it looses information Some parameters used to defining a tree and constrain overfitting Minimum sample for a node split Minimum sample for a terminal node Maximum depth of a tree Maximum number of terminal nodes Maximum features considered for split Acknowledgement: some aspects of this explanation can be read from www.analyticsvidhya.com Example code with categorical data We are going to plot a car evaulation data with 7 attributes, 6 as feature attributes and 1 as the target attribute. This is to evaluate what kinds of cars people purchase. All the attributes are categorical. We will try to build a classifier for predicting the Class attribute. The index of target attribute is 7th. instaling packages and downloading data R package caret helps to perform various machine learning tasks including decision tree classification. The rplot.plot package will help to get a visual plot of the decision tree. library(caret) ## Loading required package: lattice ## Loading required package: ggplot2 library(rpart.plot) ## Loading required package: rpart data_url &lt;- c(&quot;https://archive.ics.uci.edu/ml/machine-learning-databases/car/car.data&quot;) download.file(url = data_url, destfile = &quot;car.data&quot;) car_df &lt;- read.csv(&quot;car.data&quot;, sep = &#39;,&#39;, header = FALSE) str(car_df) ## &#39;data.frame&#39;: 1728 obs. of 7 variables: ## $ V1: Factor w/ 4 levels &quot;high&quot;,&quot;low&quot;,&quot;med&quot;,..: 4 4 4 4 4 4 4 4 4 4 ... ## $ V2: Factor w/ 4 levels &quot;high&quot;,&quot;low&quot;,&quot;med&quot;,..: 4 4 4 4 4 4 4 4 4 4 ... ## $ V3: Factor w/ 4 levels &quot;2&quot;,&quot;3&quot;,&quot;4&quot;,&quot;5more&quot;: 1 1 1 1 1 1 1 1 1 1 ... ## $ V4: Factor w/ 3 levels &quot;2&quot;,&quot;4&quot;,&quot;more&quot;: 1 1 1 1 1 1 1 1 1 2 ... ## $ V5: Factor w/ 3 levels &quot;big&quot;,&quot;med&quot;,&quot;small&quot;: 3 3 3 2 2 2 1 1 1 3 ... ## $ V6: Factor w/ 3 levels &quot;high&quot;,&quot;low&quot;,&quot;med&quot;: 2 3 1 2 3 1 2 3 1 2 ... ## $ V7: Factor w/ 4 levels &quot;acc&quot;,&quot;good&quot;,&quot;unacc&quot;,..: 3 3 3 3 3 3 3 3 3 3 ... The output of this will show us that our dataset consists of 1728 observations each with 7 attributes. head(car_df) ## V1 V2 V3 V4 V5 V6 V7 ## 1 vhigh vhigh 2 2 small low unacc ## 2 vhigh vhigh 2 2 small med unacc ## 3 vhigh vhigh 2 2 small high unacc ## 4 vhigh vhigh 2 2 med low unacc ## 5 vhigh vhigh 2 2 med med unacc ## 6 vhigh vhigh 2 2 med high unacc All the features are categorical, so normalization of data is not needed. Data Slicing Data slicing is a step to split data into train and test set. Training data set can be used specifically for our model building. Test dataset should not be mixed up while building model. Even during standardization, we should not standardize our test set. set.seed(3033) intrain &lt;- createDataPartition(y = car_df$V1, p= 0.7, list = FALSE) training &lt;- car_df[intrain,] testing &lt;- car_df[-intrain,] The “p” parameter holds a decimal value in the range of 0-1. It’s to show that percentage of the split. We are using p=0.7. It means that data split should be done in 70:30 ratio. Data Preprocessing #check dimensions of train &amp; test set dim(training); dim(testing); ## [1] 1212 7 ## [1] 516 7 anyNA(car_df) ## [1] FALSE summary(car_df) ## V1 V2 V3 V4 V5 V6 ## high :432 high :432 2 :432 2 :576 big :576 high:576 ## low :432 low :432 3 :432 4 :576 med :576 low :576 ## med :432 med :432 4 :432 more:576 small:576 med :576 ## vhigh:432 vhigh:432 5more:432 ## V7 ## acc : 384 ## good : 69 ## unacc:1210 ## vgood: 65 Training the Decision Tree classifier with criterion as INFORMATION GAIN trctrl &lt;- trainControl(method = &quot;repeatedcv&quot;, number = 10, repeats = 3) set.seed(3333) dtree_fit &lt;- train(V7 ~., data = training, method = &quot;rpart&quot;, parms = list(split = &quot;information&quot;), trControl=trctrl, tuneLength = 10) Trained Decision Tree classifier results dtree_fit ## CART ## ## 1212 samples ## 6 predictors ## 4 classes: &#39;acc&#39;, &#39;good&#39;, &#39;unacc&#39;, &#39;vgood&#39; ## ## No pre-processing ## Resampling: Cross-Validated (10 fold, repeated 3 times) ## Summary of sample sizes: 1091, 1090, 1091, 1092, 1090, 1091, ... ## Resampling results across tuning parameters: ## ## cp Accuracy Kappa ## 0.01123596 0.8655218 0.7104756 ## 0.01404494 0.8547504 0.6850294 ## 0.01896067 0.8380018 0.6444074 ## 0.01966292 0.8352468 0.6390441 ## 0.02247191 0.8168367 0.6035238 ## 0.02387640 0.8151860 0.6002291 ## 0.05337079 0.7802250 0.5541888 ## 0.06179775 0.7741710 0.5466629 ## 0.07584270 0.7524613 0.4213615 ## 0.08426966 0.7164441 0.1425639 ## ## Accuracy was used to select the optimal model using the largest value. ## The final value used for the model was cp = 0.01123596. Plotting the decision tress prp(dtree_fit$finalModel, box.palette = &quot;Reds&quot;, tweak = 1.2) Prediction The model is trained with cp = 0.01123596. cp is complexity parameter for our dtree. We are ready to predict classes for our test set. We can use predict() method. Let’s try to predict target variable for test set’s 1st record. testing[1,] ## V1 V2 V3 V4 V5 V6 V7 ## 2 vhigh vhigh 2 2 small med unacc predict(dtree_fit, newdata = testing[1,]) ## [1] unacc ## Levels: acc good unacc vgood For our 1st record of testing data classifier is predicting class variable as “unacc”. Now, its time to predict target variable for the whole test set. test_pred &lt;- predict(dtree_fit, newdata = testing) confusionMatrix(test_pred, testing$V7 ) #check accuracy ## Confusion Matrix and Statistics ## ## Reference ## Prediction acc good unacc vgood ## acc 102 19 36 3 ## good 6 4 0 3 ## unacc 5 0 318 0 ## vgood 11 1 0 8 ## ## Overall Statistics ## ## Accuracy : 0.8372 ## 95% CI : (0.8025, 0.868) ## No Information Rate : 0.686 ## P-Value [Acc &gt; NIR] : 3.262e-15 ## ## Kappa : 0.6703 ## Mcnemar&#39;s Test P-Value : NA ## ## Statistics by Class: ## ## Class: acc Class: good Class: unacc Class: vgood ## Sensitivity 0.8226 0.166667 0.8983 0.57143 ## Specificity 0.8520 0.981707 0.9691 0.97610 ## Pos Pred Value 0.6375 0.307692 0.9845 0.40000 ## Neg Pred Value 0.9382 0.960239 0.8135 0.98790 ## Prevalence 0.2403 0.046512 0.6860 0.02713 ## Detection Rate 0.1977 0.007752 0.6163 0.01550 ## Detection Prevalence 0.3101 0.025194 0.6260 0.03876 ## Balanced Accuracy 0.8373 0.574187 0.9337 0.77376 The above results show that the classifier with the criterion as information gain is giving 83.72% of accuracy for the test set. Training the Decision Tree classifier with criterion as GINI INDEX Let’s try to program a decision tree classifier using splitting criterion as gini index. set.seed(3333) dtree_fit_gini &lt;- train(V7 ~., data = training, method = &quot;rpart&quot;, parms = list(split = &quot;gini&quot;), trControl=trctrl, tuneLength = 10) dtree_fit_gini ## CART ## ## 1212 samples ## 6 predictors ## 4 classes: &#39;acc&#39;, &#39;good&#39;, &#39;unacc&#39;, &#39;vgood&#39; ## ## No pre-processing ## Resampling: Cross-Validated (10 fold, repeated 3 times) ## Summary of sample sizes: 1091, 1090, 1091, 1092, 1090, 1091, ... ## Resampling results across tuning parameters: ## ## cp Accuracy Kappa ## 0.01123596 0.8603075 0.6980952 ## 0.01404494 0.8528534 0.6797153 ## 0.01896067 0.8176967 0.5885188 ## 0.01966292 0.8143977 0.5818006 ## 0.02247191 0.7939998 0.5391064 ## 0.02387640 0.7923469 0.5378897 ## 0.05337079 0.7824472 0.5363813 ## 0.06179775 0.7744555 0.5368891 ## 0.07584270 0.7524613 0.4213615 ## 0.08426966 0.7164441 0.1425639 ## ## Accuracy was used to select the optimal model using the largest value. ## The final value used for the model was cp = 0.01123596. It is showing us the accuracy metrics for different values of cp. Plotting decision tree prp(dtree_fit_gini$finalModel, box.palette = &quot;Blues&quot;, tweak = 1.2) Prediction Our model is trained with cp = 0.01123596. Now, it’s time to predict target variable for the whole test set. test_pred_gini &lt;- predict(dtree_fit_gini, newdata = testing) confusionMatrix(test_pred_gini, testing$V7 ) #check accuracy ## Confusion Matrix and Statistics ## ## Reference ## Prediction acc good unacc vgood ## acc 109 16 34 6 ## good 5 7 0 0 ## unacc 7 0 320 0 ## vgood 3 1 0 8 ## ## Overall Statistics ## ## Accuracy : 0.8605 ## 95% CI : (0.8275, 0.8892) ## No Information Rate : 0.686 ## P-Value [Acc &gt; NIR] : &lt; 2.2e-16 ## ## Kappa : 0.7133 ## Mcnemar&#39;s Test P-Value : NA ## ## Statistics by Class: ## ## Class: acc Class: good Class: unacc Class: vgood ## Sensitivity 0.8790 0.29167 0.9040 0.57143 ## Specificity 0.8571 0.98984 0.9568 0.99203 ## Pos Pred Value 0.6606 0.58333 0.9786 0.66667 ## Neg Pred Value 0.9573 0.96627 0.8201 0.98810 ## Prevalence 0.2403 0.04651 0.6860 0.02713 ## Detection Rate 0.2112 0.01357 0.6202 0.01550 ## Detection Prevalence 0.3198 0.02326 0.6337 0.02326 ## Balanced Accuracy 0.8681 0.64075 0.9304 0.78173 The above results show that the classifier with the criterion as gini index is giving 86.05% of accuracy for the test set. In this case, our classifier with criterion gini index is giving better results. Acknowledgement: the above data comes from a machine learning database and the codes are discussed at: http://dataaspirant.com/2017/02/03/decision-tree-classifier-implementation-in-r/ Methods used in Decision Trees for trade-off balance Ensemble methods involve group of predictive models to achieve a better accuracy and model stability. Ensemble methods are known to impart supreme boost to tree based models. Bagging is a technique used to reduce the variance of predictions by combining the result of multiple classifiers modeled on different sub-samples of the same data set. Boosting refers to a family of algorithms which converts weak learner to strong learner by combing the prediction of each weak learner using methods like average/ weighted average or by considering a prediction that has a higher vote. Gradient boosting and XGboost are examples of boosting algorithms. 9.2 Random Forest What is a Random Forest? It is a kind of ensemble learning method that combines a set of weak models to form a powerful model. In the process it reduces dimensionality, removes outliers, treats missing values, and more importantly it is both a regression and classification machine learning approach. How does it work? In Random Forest, multiple trees are grown as opposed to a single tree in a decision tree model. Assume number of cases in the training set is N. Then, sample of these N cases is taken at random but with replacement. This sample will be the training set for growing the tree. Each tree is grown to the largest extent possible and without pruning. To classify a new object based on attributes, each tree gives a classification i.e. “votes” for that class. The forest chooses the classification having the most votes (over all the trees in the forest) and in case of regression, it takes the average of outputs by different trees. Key differences between decision trees and random forest Decision trees proceed by searching for a split on every variable in every node random forest searches for a split only on one variable in a node - the variable that has the largest association with the target among all other explanatory variables but only on a subset of randomly selected explanatory variables that is tested for that node. At every node a new list is selected. Therefore, eligible variable set will be different from node to node but the important ones will eventually be “voted in” based on their success in predicting the targert variable. This random selection of explanatory variables at each node and which are different at each treee is known as bagging. For each tree the ratio between bagging and out of bagging is 60/40. The important thing to note is that the trees are themselves not intpreted but they are used to collectively rank the importance of each variable. Example Random Forest code for binary classification In this example, a bank wanted to cross-sell term deposit product to its customers and hence it wanted to build a predictive model, which will identify customers who are more likely to respond to term deport cross-sell campaign. Install and load randomForest library # Load library library(randomForest) ## randomForest 4.6-14 ## Type rfNews() to see new features/changes/bug fixes. ## ## Attaching package: &#39;randomForest&#39; ## The following object is masked from &#39;package:ggplot2&#39;: ## ## margin ## Read data Example&lt;-read.csv(file=&quot;data/Decision_tree_and_RF/bank.csv&quot;,header = T) Input dataset has 20 independent variables and a target variable. The target variable y is binary. names(Example) ## [1] &quot;age&quot; &quot;job&quot; &quot;marital&quot; &quot;education&quot; ## [5] &quot;default&quot; &quot;housing&quot; &quot;loan&quot; &quot;contact&quot; ## [9] &quot;month&quot; &quot;day_of_week&quot; &quot;duration&quot; &quot;campaign&quot; ## [13] &quot;pdays&quot; &quot;previous&quot; &quot;poutcome&quot; &quot;emp.var.rate&quot; ## [17] &quot;cons.price.idx&quot; &quot;cons.conf.idx&quot; &quot;euribor3m&quot; &quot;nr.employed&quot; ## [21] &quot;y&quot; table(Example$y)/nrow(Example) ## ## no yes ## 0.8905074 0.1094926 11% of the observations has target variable “yes” and remaining 89% observations take value “no”. We will split the data sample into development and validation samples. sample.ind &lt;- sample(2, nrow(Example), replace = T, prob = c(0.6,0.4)) Example.dev &lt;- Example[sample.ind==1,] Example.val &lt;- Example[sample.ind==2,] table(Example.dev$y)/nrow(Example.dev) ## ## no yes ## 0.8881469 0.1118531 Both development and validation samples have similar target variable distribution. This is just a sample validation. class(Example.dev$y) ## [1] &quot;factor&quot; Class of target or response variable is factor, so a classification Random Forest will be built. The current data frame has a list of independent variables, so we can make it formula and then pass as a parameter value for randomForest. Make Formula varNames &lt;- names(Example.dev) # Exclude ID or Response variable varNames &lt;- varNames[!varNames %in% c(&quot;y&quot;)] # add + sign between exploratory variables varNames1 &lt;- paste(varNames, collapse = &quot;+&quot;) # Add response variable and convert to a formula object rf.form &lt;- as.formula(paste(&quot;y&quot;, varNames1, sep = &quot; ~ &quot;)) Building Random Forest model We will build 500 decision trees using Random Forest. Example.rf &lt;- randomForest(rf.form, Example.dev, ntree=500, importance=T) plot(Example.rf) 500 decision trees or a forest has been built using the Random Forest algorithm based learning. We can plot the error rate across decision trees. The plot seems to indicate that after 100 decision trees, there is not a significant reduction in error rate. # Variable Importance Plot varImpPlot(Example.rf, sort = T, main=&quot;Variable Importance&quot;, n.var=5) Variable importance plot is also a useful tool and can be plotted using varImpPlot function. Top 5 variables are selected and plotted based on Model Accuracy and Gini value. We can also get a table with decreasing order of importance based on a measure (1 for model accuracy and 2 node impurity) # Variable Importance Table var.imp &lt;- data.frame(importance(Example.rf, type=2)) # make row names as columns var.imp$Variables &lt;- row.names(var.imp) var.imp[order(var.imp$MeanDecreaseGini,decreasing = T),] ## MeanDecreaseGini Variables ## duration 131.064241 duration ## euribor3m 51.767706 euribor3m ## nr.employed 33.282654 nr.employed ## job 33.202606 job ## age 30.542239 age ## education 21.817563 education ## month 21.481160 month ## day_of_week 21.449264 day_of_week ## pdays 19.153022 pdays ## cons.conf.idx 15.695240 cons.conf.idx ## campaign 15.283873 campaign ## poutcome 13.860486 poutcome ## cons.price.idx 13.175296 cons.price.idx ## emp.var.rate 10.957252 emp.var.rate ## marital 8.270943 marital ## previous 7.556246 previous ## housing 7.012238 housing ## loan 5.045711 loan ## contact 4.187090 contact ## default 3.104346 default Based on Random Forest variable importance, the variables could be selected for any other predictive modelling techniques or machine learning. Predict Response Variable Value using Random Forest Generic predict function can be used for predicting response variable using Random Forest object. # Predicting response variable Example.dev$predicted.response &lt;- predict(Example.rf ,Example.dev) confusionMatrix function from caret package can be used for creating confusion matrix based on actual response variable and predicted value. # Load Library or packages library(e1071) library(caret) ## Loading required package: lattice ## Loading required package: ggplot2 # Create Confusion Matrix confusionMatrix(data=Example.dev$predicted.response, reference=Example.dev$y, positive=&#39;yes&#39;) ## Confusion Matrix and Statistics ## ## Reference ## Prediction no yes ## no 2128 0 ## yes 0 268 ## ## Accuracy : 1 ## 95% CI : (0.9985, 1) ## No Information Rate : 0.8881 ## P-Value [Acc &gt; NIR] : &lt; 2.2e-16 ## ## Kappa : 1 ## Mcnemar&#39;s Test P-Value : NA ## ## Sensitivity : 1.0000 ## Specificity : 1.0000 ## Pos Pred Value : 1.0000 ## Neg Pred Value : 1.0000 ## Prevalence : 0.1119 ## Detection Rate : 0.1119 ## Detection Prevalence : 0.1119 ## Balanced Accuracy : 1.0000 ## ## &#39;Positive&#39; Class : yes ## It has accuracy of 99.81%. Now we can predict response for the validation sample and calculate model accuracy for the sample. # Predicting response variable Example.val$predicted.response &lt;- predict(Example.rf ,Example.val) # Create Confusion Matrix confusionMatrix(data=Example.val$predicted.response, reference=Example.val$y, positive=&#39;yes&#39;) ## Confusion Matrix and Statistics ## ## Reference ## Prediction no yes ## no 1476 98 ## yes 64 85 ## ## Accuracy : 0.906 ## 95% CI : (0.8912, 0.9193) ## No Information Rate : 0.8938 ## P-Value [Acc &gt; NIR] : 0.052603 ## ## Kappa : 0.4606 ## Mcnemar&#39;s Test P-Value : 0.009522 ## ## Sensitivity : 0.46448 ## Specificity : 0.95844 ## Pos Pred Value : 0.57047 ## Neg Pred Value : 0.93774 ## Prevalence : 0.10621 ## Detection Rate : 0.04933 ## Detection Prevalence : 0.08648 ## Balanced Accuracy : 0.71146 ## ## &#39;Positive&#39; Class : yes ## Accuracy level has dropped to 91.8% but still significantly higher. Acknowledgement: the above data is from a machine-learning database and the code is discusses: http://dni-institute.in/blogs/random-forest-using-r-step-by-step-tutorial/* 9.3 Exercises Titanic Data  One of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew. Although there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others, such as women, children, and the upper-class. In this excerise, try to complete the analysis of what sorts of people were likely to survive. The data can be downloaded from https://goo.gl/At238b. Hint: Use decision tree. Solutions to exercises can be found in appendix H. "],
["ann.html", "10 Artificial neural networks 10.1 Neural Networks", " 10 Artificial neural networks 10.1 Neural Networks What are artificial neural networks (ANNs)? ANN is actually an old idea but it came back into vogue recently and it is the state of the art technique for machine learning. The goal of ANN algorithms is to mimmick the functions of a neuron (Figure 10.1) and neuronal networks. Figure 10.1: Neuronal computation Computational representation of a neuron (Figure 10.2) aims to mimmick the biological input-and-activation architecture of a neuron (Figure 10.1). A single unit of a computational neuron is also called a perceptron or ptrons. Ptrons have a nonlinear activation function (e.g a logistic function) which determines their output value based upon the values of their inputs. Figure 10.2: Perceptron Architecture of ANNs ANNs are built from ptrons. Ptrons have one or more inputs, an activation function and an output (Figure Perceptron). An ANN model is built up by combining ptrons in structured layers. Ptrons in a given layer are independent of each other, but each of them connect to all the ptrons in the next layer (Figure Neural Network Modeling). The input layer contains a ptron for each input variable. One or more hidden layers contain a user defined number of ptrons. Each ptron in the first hidden layer receives an input from the each ptron in the input layer. If there is a second hidden layer, each ptron in this layer receives an input from each ptron in the first hidden layer, and so on with additional layers. The output layer contains a ptron for each response variable (usually one, sometimes more in multivariate response situations). Each output ptron receives one input from each ptron in the final hidden layer Important: The connections between ptrons are weighted. The magnitude of the weight controls the strength of the influence of that input on the receiving ptron. The sign of the weight controls whether the influence is stimulating or inhibiting the signal to the next layer. The weights are somewhat analogous to the parameters of a linear model. There is also a bias adjustment that represents the base value of a ptron and is analogous to the intercept in a linear model. If the inputs are near zero, the bias ensures that the output is near average. Due to the network-like nature of the ANN a complex, non-linear relationship exist between the predictors and response. Acknowledgement: aspects of the above discussion are from: https://rpubs.com/julianhatwell/annr Forward propagation Figure 10.3 represents a simple ANN, where we have an input later (layer 1) with three ptrons and a base unit, one hidden layer (layer 2) again with three prtons and a base unit, and an output layer (layer 3) where the \\(h_{\\theta}\\)(x) is computed. This method of computing \\(h_{\\theta}\\)(x) is called Forward Propagation. Figure 10.3: Neural Network Modeling where \\(a_i^{(j)}\\)= activation of i in layer j \\(\\theta^i\\) = matrix of weights controlling function mapping from layer j to layer j+1 \\[\\begin{align} a_1^{(2)} &amp;= g (\\theta_{10}^{(1)}x_0 + \\theta_{11}^{(1)}x_1 + \\theta_{12}^{(1)}x_2+ \\theta_{13}^{(1)}x_3)\\\\ a_2^{(2)} &amp;= g (\\theta_{20}^{(1)}x_0 + \\theta_{21}^{(1)}x_1 + \\theta_{22}^{(1)}x_2+ \\theta_{23}^{(1)}x_3)\\\\ a_3^{(2)} &amp;= g (\\theta_{30}^{(1)}x_0 + \\theta_{31}^{(1)}x_1 + \\theta_{32}^{(1)}x_2+ \\theta_{33}^{(1)}x_3)\\\\ h_{\\theta}(x) &amp;= a_1^{(3)}=g (\\theta_{10}^{(2)}a_0^{(2)} + \\theta_{11}^{(2)}a_1^{(2)} + \\theta_{12}^{(1)}a_2^{(2)} + \\theta_{13}^{(2)}a_3^{(2)})\\\\ \\end{align}\\] Vectorized notations of inputs and activations. \\[\\begin{align} x &amp;= \\begin{bmatrix}x_o \\\\ x_1\\\\ x_2\\\\ x_3 \\end{bmatrix}\\\\ z^{(2)} &amp;= \\begin{bmatrix} z_1^{(2)}\\\\ z_2^{(2)}\\\\ z_3^{(2)} \\end{bmatrix} \\end{align}\\] Vectorized representation of activation of hidden layer and activation layer. \\[\\begin{align} z^{(2)} &amp;= \\Theta^{(1)} a^{(1)}\\\\ a^{(2)} &amp;= g(z^{(2)}) \\end{align}\\] \\[\\begin{align} z^{(3)} &amp;= \\Theta^{(2)} a^{(2)}\\\\ h_\\Theta(x) &amp;= a^{(3)} = g(z^{(3)}) \\end{align}\\] Why ANN? Consider the supervised learning problems below (Figure 10.4). The first two are straight forward cases. Figure 10.4: Why ANN Whereas for the third we could probably apply a logistic regression with a lot of nonlinear features like this \\[ Y_i = g(\\theta_0 + \\theta_1 x_1 + \\theta_2 x_2+ \\theta_3 x_1x_2+ \\theta_4 x_i^2x_2 + \\theta_5 x_i^3x_2+ \\theta_6 x_i^3x_2^2 \\dots)\\] i.e. if we include enough polynomials we could arrive at an hypothesis that will separate the two classes. This could perfectly work well if we just have two features, such as x1 and x2 but for almost all machine learning problems we usually have more than two features. Importantly if the number of features increase the number of quadratic terms increase as a function of \\(n^2/2\\); where n is the number of features. This would result in overfitting if the number of features increase. Because ANN has felixibility to derive complex features from each layer of ptrons, it can be applied to any complex functional relationship and more importantly unlike generalized linear models (GLMs) it is not necessary to prespecify the type of relationship between covariates and response variables as for instance as linear combination. This makes ANN a valuable statistical tool. Observed data are used to train the neural network and the neural network learns an approximation of the relationship by iteratively adapting its parameters. Figure 10.5 shows an example of a simple logical AND ptron architecture. Figure 10.5: Simple Logical AND ANN Cost function and back propagation Cost function of linear models \\[ \\begin{equation*} CF(\\theta_{0},\\theta_{1}) = 1/2n\\sum_{i=1}^{n} (h_{\\theta}(x^i) - y^i)^2 \\end{equation*} \\] Matrix solution to minimise the cost function in linear models \\[ \\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^\\top \\mathbf{X})^{-1} \\mathbf{X}^\\top \\mathbf{Y} \\] Cost funciton in ANN (it is a pretty scarry equation) \\[ \\begin{equation*} CF(\\Theta) = -1/m \\Bigg[\\sum_{i=1}^{m}\\sum_{k=1}^{K} y_k^{(i)}log(h_\\Theta(x^{(i)}))_k+(1-y_k^{(i)})log(1-(h_\\Theta(x^{(i)}))_k)\\Bigg] \\end{equation*} \\] \\[ \\begin{equation*} +\\lambda/2m\\sum_{l=1}^{L-1}\\sum_{i=1}^{s_l}\\sum_{j=1}^{S_{l+1}}(\\Theta_{ji}^{(l)})^2 \\end{equation*} \\] We have to minimise this ANN cost function and it is done by back propogation. It is termed back propogation because of the fact that we compute the error from the outer most layer and go backwards. Acknowledgement: The above example and discussion is from Prof. Andrew Ng’s coursera session on ANN Example model This example uses the Boston data from the MASS package which contains a number of predictors of median property values in suburbs of Boston, MA, USA. The code used is based on Alice, (2015) The Boston dataset is a collection of data about housing values in the suburbs of Boston. Our goal is to predict the median value of owner-occupied homes (medv) using all the other continuous variables available. library(neuralnet) library(nnet) library(NeuralNetTools) library(MASS) library(ISLR) library(caTools) # sample.split library(boot) # cv.glm library(faraway) # compact lm summary &quot;sumary&quot; function ## ## Attaching package: &#39;faraway&#39; ## The following objects are masked from &#39;package:boot&#39;: ## ## logit, melanoma library(caret) # useful tools for machine learning ## Loading required package: lattice ## ## Attaching package: &#39;lattice&#39; ## The following object is masked from &#39;package:faraway&#39;: ## ## melanoma ## The following object is masked from &#39;package:boot&#39;: ## ## melanoma ## Loading required package: ggplot2 library(corrplot) ## corrplot 0.84 loaded set.seed(500) library(MASS) data &lt;- Boston Checking whether there are any missing data. apply(data,2,function(x) sum(is.na(x))) ## crim zn indus chas nox rm age dis rad ## 0 0 0 0 0 0 0 0 0 ## tax ptratio black lstat medv ## 0 0 0 0 0 We randomly splitt the data into a train and a test set and then we fit a linear regression model and test it on the test set. index &lt;- sample(1:nrow(data),round(0.75*nrow(data))) train &lt;- data[index,] test &lt;- data[-index,] lm.fit &lt;- glm(medv~., data=train) summary(lm.fit) ## ## Call: ## glm(formula = medv ~ ., data = train) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -14.9143 -2.8607 -0.5244 1.5242 25.0004 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 43.469681 6.099347 7.127 5.50e-12 *** ## crim -0.105439 0.057095 -1.847 0.065596 . ## zn 0.044347 0.015974 2.776 0.005782 ** ## indus 0.024034 0.071107 0.338 0.735556 ## chas 2.596028 1.089369 2.383 0.017679 * ## nox -22.336623 4.572254 -4.885 1.55e-06 *** ## rm 3.538957 0.472374 7.492 5.15e-13 *** ## age 0.016976 0.015088 1.125 0.261291 ## dis -1.570970 0.235280 -6.677 9.07e-11 *** ## rad 0.400502 0.085475 4.686 3.94e-06 *** ## tax -0.015165 0.004599 -3.297 0.001072 ** ## ptratio -1.147046 0.155702 -7.367 1.17e-12 *** ## black 0.010338 0.003077 3.360 0.000862 *** ## lstat -0.524957 0.056899 -9.226 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for gaussian family taken to be 23.26491) ## ## Null deviance: 33642 on 379 degrees of freedom ## Residual deviance: 8515 on 366 degrees of freedom ## AIC: 2290 ## ## Number of Fisher Scoring iterations: 2 pr.lm &lt;- predict(lm.fit,test) MSE.lm &lt;- sum((pr.lm - test$medv)^2)/nrow(test) The sample(x,size) function simply outputs a vector of the specified size of randomly selected samples from the vector x. By default the sampling is without replacement: index is essentially a random vector of indeces. Since we are dealing with a regression problem, we are going to use the mean squared error (MSE) as a measure of how much our predictions are far away from the real data. Before fitting a neural network, we need to be prepare them to train and tune. maxs &lt;- apply(data, 2, max) mins &lt;- apply(data, 2, min) It is important to normalize the data before training a neural network. Avoiding normalization may lead to useless results or to a very difficult training process (most of the times the algorithm will not converge before the number of maximum iterations are allowed). There are different methods to choose to scale the data (z-normalization, min-max scale, etc…). Here we have chosen to use the min-max method and scale the data in the interval [0,1]. Usually scaling in the intervals [0,1] or [-1,1] tends to give better results. We therefore scale and split the data before moving on: scaled &lt;- as.data.frame(scale(data, center = mins, scale = maxs - mins)) train_ &lt;- scaled[index,] test_ &lt;- scaled[-index,] Scale returns a matrix that needs to be coerced into a data.frame. library(neuralnet) n &lt;- names(train_) f &lt;- as.formula(paste(&quot;medv ~&quot;, paste(n[!n %in% &quot;medv&quot;], collapse = &quot; + &quot;))) nn &lt;- neuralnet(f,data=train_,hidden=c(5,3),linear.output=T) There are no fixed rules as to how many layers and neurons to use although there are several more or less accepted rules of thumb. Usually, one hidden layer is enough for a vast numbers of applications. As far as the number of neurons is concerned, it should be between the input layer size and the output layer size, usually 2/3 of the input size. Since this is a toy example, we are going to use 2 hidden layers with this configuration: 13:5:3:1. The input layer has 13 inputs, the two hidden layers have 5 and 3 neurons and the output layer has a single output since we are doing regression. The formula y~. is not accepted in the neuralnet() function. You need to first write the formula and then pass it as an argument in the fitting function. The hidden argument accepts a vector with the number of neurons for each hidden layer, while the argument linear.output is used to specify whether we want to do regression linear.output=TRUE or classification linear.output=FALSE This is the graphical representation of the model with the weights on each connection: plot(nn) The black lines show the connections between each layer and the weights on each connection while the blue lines show the bias term added in each step. The bias can be thought as the intercept of a linear model. The net is essentially a black box so we cannot say that much about the fitting, the weights and the model. Suffice to say that the training algorithm has converged and therefore the model is ready to be used. Now we can try to predict the values for the test set and calculate the MSE. Remember that the net will output a normalized prediction, so we need to scale it back in order to make a meaningful comparison (or just a simple prediction). pr.nn &lt;- compute(nn,test_[,1:13]) pr.nn_ &lt;- pr.nn$net.result*(max(data$medv)-min(data$medv))+min(data$medv) test.r &lt;- (test_$medv)*(max(data$medv)-min(data$medv))+min(data$medv) MSE.nn &lt;- sum((test.r - pr.nn_)^2)/nrow(test_) we then compare the two MSEs print(paste(MSE.lm,MSE.nn)) ## [1] &quot;21.6297593507225 15.7518370200153&quot; par(mfrow=c(1,2)) plot(test$medv,pr.nn_,col=&#39;red&#39;,main=&#39;Real vs predicted NN&#39;,pch=18,cex=0.7) abline(0,1,lwd=2) legend(&#39;bottomright&#39;,legend=&#39;NN&#39;,pch=18,col=&#39;red&#39;, bty=&#39;n&#39;) plot(test$medv,pr.lm,col=&#39;blue&#39;,main=&#39;Real vs predicted lm&#39;,pch=18, cex=0.7) abline(0,1,lwd=2) legend(&#39;bottomright&#39;,legend=&#39;LM&#39;,pch=18,col=&#39;blue&#39;, bty=&#39;n&#39;, cex=.95) The net is doing a better work than the linear model at predicting medv. Once again, be cautious because this result depends on the train-test split performed above. Below, after the visual plot, we are going to perform a fast cross validation in order to be more confident about the results. A first visual approach to the performance of the network and the linear model on the test set is plotted. By visually inspecting the plot we can see that the predictions made by the neural network are (in general) more concetrated around the line (a perfect alignment with the line would indicate a MSE of 0 and thus an ideal perfect prediction) than those made by the linear model. A perhaps more useful visual comparison is plotted. plot(test$medv,pr.nn_,col=&#39;red&#39;,main=&#39;Real vs predicted NN&#39;,pch=18,cex=0.7) points(test$medv,pr.lm,col=&#39;blue&#39;,pch=18,cex=0.7) abline(0,1,lwd=2) legend(&#39;bottomright&#39;,legend=c(&#39;NN&#39;,&#39;LM&#39;),pch=18,col=c(&#39;red&#39;,&#39;blue&#39;)) Cross validation is another very important step of building predictive models. While there are different kind of cross validation methods, the basic idea is repeating the following process a number of time: Train-test split Do the train-test split Fit the model to the train set Test the model on the test set Calculate the prediction error Repeat the process K times Then by calculating the average error we can get a grasp of how the model is doing. We are going to implement a fast cross validation using a for loop for the neural network and the cv.glm() function in the boot package for the linear model. Here is the 10 fold cross validated MSE for the linear model library(boot) set.seed(200) lm.fit &lt;- glm(medv~.,data=data) cv.glm(data,lm.fit,K=10)$delta[1] ## [1] 23.83560156 set.seed(450) cv.error &lt;- NULL k &lt;- 10 Note that we are splitting the data in this way: 90% train set and 10% test set in a random way for 10 times. We are also initializing a progress bar using the plyr library. library(plyr) ## ## Attaching package: &#39;plyr&#39; ## The following object is masked from &#39;package:faraway&#39;: ## ## ozone pbar &lt;- create_progress_bar(&#39;text&#39;) pbar$init(k) ## | | | 0% for(i in 1:k){ index &lt;- sample(1:nrow(data),round(0.9*nrow(data))) train.cv &lt;- scaled[index,] test.cv &lt;- scaled[-index,] nn &lt;- neuralnet(f,data=train.cv,hidden=c(5,2),linear.output=T) pr.nn &lt;- compute(nn,test.cv[,1:13]) pr.nn &lt;- pr.nn$net.result*(max(data$medv)-min(data$medv))+min(data$medv) test.cv.r &lt;- (test.cv$medv)*(max(data$medv)-min(data$medv))+min(data$medv) cv.error[i] &lt;- sum((test.cv.r - pr.nn)^2)/nrow(test.cv) pbar$step() } ## | |====== | 10% | |============= | 20% | |==================== | 30% | |========================== | 40% | |================================ | 50% | |======================================= | 60% | |============================================== | 70% | |==================================================== | 80% | |========================================================== | 90% | |=================================================================| 100% We calculate the average MSE and plot the results as a boxplot. mean(cv.error) ## [1] 10.32697995 boxplot(cv.error,xlab=&#39;MSE CV&#39;,col=&#39;cyan&#39;, border=&#39;blue&#39;,names=&#39;CV error (MSE)&#39;, main=&#39;CV error (MSE) for NN&#39;,horizontal=TRUE) The average MSE for the neural network (10.33) is lower than the one of the linear model although there seems to be a certain degree of variation in the MSEs of the cross validation. This may depend on the splitting of the data or the random initialization of the weights in the net. By running the simulation different times with different seeds you can get a more precise point estimate for the average MSE. Acknowledgement: the above example is from https://www.r-bloggers.com/fitting-a-neural-network-in-r-neuralnet-package/ ## Exercises Using a ANN, take a number and calculate its square root. Solutions to exercises can be found in appendix I. "],
["mlnn.html", "11 Deep Learning 11.1 Multilayer Neural Networks 11.2 Convolutional neural networks 11.3 Further reading", " 11 Deep Learning 11.1 Multilayer Neural Networks Neural networks with multiple layers are increasingly used to attack a variety of complex problems under the umberella of Deep Learning (Angermueller and Stegle 2016). In this final section we will explore the basics of Deep Learning for image classification using a set of images taken from the animated TV series Rick and Morty. For those unfamiliar with Rick and Morty, the series revolves around the adventures of Rick Sanchez, an alcoholic, arguably sociopathic scientist, and his neurotic grandson, Morty Smith. Although many scientists aspire to be like Rick, they’re usually more like a Jerry. Our motivating goal in this section is to develop a image classification algorithm capable of telling us whether a given image contains Rick or not: a binary classification task with two classes, Rick or not Rick. For training purposes we have downloaded \\(1000\\) random images of Rick and \\(1000\\) random images without Rick from the website Master of All Science. We have also downloaded \\(200\\) validation images each of the Rick versus not Rick classes. These images can be found in the appropriate subdirectories in the folder {/RickandMorty/data/}. 11.1.1 Constructing layers in kerasR A user friendly package for Deep Learning is available via keras, an application programming interface (API) written in Python, which uses either theano or tensorflow as a back-end. An R interface for keras is available in the form of kerasR. Before we can use kerasR we first need to load the kerasR library in R (we also need to install keras and either theano and tensorflow). library(kerasR) ## successfully loaded keras library(reticulate) library(grid) library(jpeg) set.seed(12345) Now we come to specifying the model itself. Keras has an simple and intuitive way of specifying layers of a neural network, and kerasR makes good use of this. We first initialie the model: mod &lt;- Sequential() This tells keras that we’re using the Sequential API i.e., with the first layer connected to the second, the second to the thrid and so forth, which duistuinguishes it from more complex networks possible using the Model API. Once we’ve specified a sequential model, we have to specifiy the layers of the neural network. A standard layer of neurons can be specified using the {Dense} command; the first layer of our network must also include the dimension of the input. So, for example, if our input data was a vector of dimension \\(1 \\times 40\\), we could add an input layer via: mod$add(Dense(100, input_shape = c(1,40))) We also need to specfy the activation function to the next level. This can be done via {Activation()}, so our snippet of code using a Rectified Linear Unit (relu) activation would look something like: mod$add(Dense(100, input_shape = c(1,40))) mod$add(Activation(&quot;relu&quot;)) We could add another layer of 120 neurons in (adding activation functions): mod$add(Dense(100, input_shape = c(1,40))) mod$add(Activation(&quot;relu&quot;)) mod$add(Dense(120)) mod$add(Activation(&quot;relu&quot;)) Finally, we should add the output neurons. If we had, for example, a binary classification algorithm, we could have two nodes, with a sigmoid activation function. Our final model would look like: mod$add(Dense(100, input_shape = c(1,40))) mod$add(Activation(&quot;relu&quot;)) mod$add(Dense(120)) mod$add(Activation(&quot;relu&quot;)) mod$add(Dense(2)) mod$add(Activation(&quot;sigmoid&quot;)) That’s it. Simple! 11.1.2 Reading in images We can load images and plot them in R using the {readJPEG} and {grid.raster} functions respectively. im &lt;- readJPEG(&quot;data/RickandMorty/data/train/Rick/Rick_001.jpg&quot;) grid.raster(im, interpolate=FALSE) Each image is stored as a jpeg file with \\(90 \\times 160\\) pixel resolution and \\(3\\) colour channels (RGB). The input data is therefore a tensor/array of dimension \\(90 \\times 160 \\times 3\\). Keras expects inputs in the form of Numpy arrays, and we can construct the training dataset by loading all \\(1000\\) Rick and all \\(1000\\) not Rick images. We first get a list of all the Rick images in the directory {train/Rick}: files &lt;- list.files(path = &quot;data/RickandMorty/data/train/Rick/&quot;, pattern = &quot;jpg&quot;) We next preallocate an empty array to store these training images for the Rick and not Rick images (an array of dimension \\(2000 \\times 90 \\times 160 \\times 3\\)): trainX &lt;- array(0, dim=c(2000,90,160,3)) We can load images using the {readJPEG} function: for (i in 1:1000){ trainX[i,1:90,1:160,1:3] &lt;- readJPEG(paste(&quot;data/RickandMorty/data/train/Rick/&quot;, files[i], sep=&quot;&quot;)) } Similarly, we can load the not Rick images and store in the same array: files &lt;- list.files(path = &quot;data/RickandMorty/data/train/Morty/&quot;,pattern = &quot;jpg&quot;) for (i in 1001:2000){ trainX[i,1:90,1:160,1:3] &lt;- readJPEG(paste(&quot;data/RickandMorty/data/train/Morty/&quot;, files[i-1000], sep=&quot;&quot;)) } Next we can construct a vector of length \\(2000\\) containing the classification for each of these \\(2000\\) images e.g., \\(0\\) if the image is a Rick and \\(1\\) if it is not Rick. This is simple enough using the function {rbind}, as we know the first \\(1000\\) images were Rick and the second \\(1000\\) images not Rick. Since we are dealing with a classification algorithm, we next convert the data to binary categorical output (that is, a Rick is now represented as \\([1, 0]\\) and a not Rick is a \\([0, 1]\\)), which we can do using the {to_categorical} conversion function: trainY &lt;- to_categorical(rbind(matrix(0, 1000, 1), matrix(1, 1000, 1)), 2) Obviously the \\(2\\) in the code snippet above informs us that we have \\(2\\) classes; we could just as easily perform classificaiton with more than \\(2\\) classes, for example if we wanted to classify Ricky, Morty or neither Rick or Morty. Next we will load in the validation sets: files &lt;- list.files(path = &quot;data/RickandMorty/data/validation/Rick/&quot;,pattern = &quot;jpg&quot;) validateX &lt;- array(0, dim=c(400,90,160,3)) for (i in 1:200){ validateX[i,1:90,1:160,1:3] &lt;- readJPEG(paste(&quot;data/RickandMorty/data/validation/Rick/&quot;, files[i], sep=&quot;&quot;)) } files &lt;- list.files(path = &quot;data/RickandMorty/data/validation/Morty/&quot;,pattern = &quot;jpg&quot;) for (i in 201:400){ validateX[i,1:90,1:160,1:3] &lt;- readJPEG(paste(&quot;data/RickandMorty/data/validation/Morty/&quot;, files[i-200], sep=&quot;&quot;)) } validateY &lt;- to_categorical(rbind(matrix(0, 200, 1),matrix(1, 200, 1)),2) 11.1.3 Rick and Morty classifier using Deep Learning Let us return to our example of image classification. Our data is slightly different to the usual inputs we’ve been dealing with: that is, we’re not dealing with an input vector, but instead have an image. In this case each image is a \\(90 \\times 160 \\time 3\\) array, so for our first layer we have to flatten this down. This can be done using {Flatten()}: mod$add(Flatten(input_shape = c(90, 160, 3))) This should turn our \\(90 \\times \\160 \\times 3\\) input into a \\(1 \\times 43200\\) node input. We now add an intermediate layer containing \\(100\\) neurons, connected to the input layer with rectified linear units ({relu}): mod$add(Activation(&quot;relu&quot;)) mod$add(Dense(100)) Finally we connect this layer over the final output layer (two neurons) with sigmoid activation: activation mod$add(Activation(&quot;relu&quot;)) mod$add(Dense(2)) mod$add(Activation(&quot;sigmoid&quot;)) The complete model should look something like: mod &lt;- Sequential() mod$add(Flatten(input_shape = c(90, 160, 3))) mod$add(Activation(&quot;relu&quot;)) mod$add(Dense(100)) mod$add(Activation(&quot;relu&quot;)) mod$add(Dense(2)) mod$add(Activation(&quot;sigmoid&quot;)) We can visualise this model using the {plot_model} function (Figure 11.1). plot_model(mod,&#39;images/DNN1.png&#39;) Figure 11.1: Example of a multilayer convolutional neural network We can also print a summary of the network, for example to see how many parameters it has, using the {summary} function: summary(mod) ## &lt;keras.engine.sequential.Sequential&gt; In this case we see a total of \\(4320302\\) parameters. Next we need to compile and run the model. In this case we need to specify three things: A loss function, which specifies the objective function that the model will try to minimise. A number of existing loss functions are built into keras, including mean squared error (mean_squared_error) and categorical cross entropy (categorical_crossentropy). Since we are dealing with binary classification, we will use binary cross entropy (binary_crossentropy). An optimiser, which determines how the loss functin is optimised. Possible examples include stochastic gradient descent ({SGD()}) and Root Mean Square Propagation ({RMSprop()}). A list of metrics to return. These are additional summary statistics that keras evaluates and prints. For classification, a good choice would be accuracy. We can compile this using {keras_compile}: keras_compile(mod, loss = &#39;binary_crossentropy&#39;, metrics = c(&#39;accuracy&#39;), optimizer = RMSprop()) Finally the model can be fitted to the data. When doing so we additionally need to specify the validation set (if we have one), the batch size and the number of epochs, where an epoch is one forward pass and one backward pass of all the training examples and the batch size is the number of training examples in one forward/backward pass. You may want to go and get a tea whilst this is running! keras_fit(mod, trainX, trainY, validation_data = list(validateX, validateY), batch_size = 32, epochs = 25, verbose = 1) For this model we achieved an accuracy of \\(0.5913\\) on the validation dataset at epoch \\(23\\) (which had a corresponding accuracy of \\(0.5938\\) on the training set). Not great is an understatement. In fact it’s barely better than random (which would be \\(0.5\\), with \\(1\\) being perfect)! Let’s try adding in another layer to the network. In this case we add in a layer containing \\(70\\) neurons, connected with {relu} activations: mod &lt;- Sequential() mod$add(Flatten(input_shape = c(90, 160, 3))) mod$add(Activation(&quot;relu&quot;)) mod$add(Dense(100)) mod$add(Activation(&quot;relu&quot;)) mod$add(Dense(70)) mod$add(Activation(&quot;relu&quot;)) mod$add(Dense(2)) mod$add(Activation(&quot;sigmoid&quot;)) keras_compile(mod, loss = &#39;binary_crossentropy&#39;, metrics = c(&#39;accuracy&#39;), optimizer = RMSprop()) keras_fit(mod, trainX, trainY, validation_data = list(validateX, validateY), batch_size = 32, epochs = 25, verbose = 1) We can again visualise the model: plot_model(mod,&#39;images/DNN2.png&#39;) Figure 11.2: Example of a multilayer convolutional neural network We get now get a validation accuracy of \\(0.6238\\) at epoch \\(25\\), with corresponding training accuracy of \\(0.5767\\). It’s an improvement, but it’s still pretty bad. We could try adding in extra layers, but it seems we’re getting nowhere fast, and will need to change tactic. We need to think a little about what the data actually is. In this case, we’re looking at a set of images. As Rick Sanchez can appear almost anywhere in the image, there’s no reason to think that a given input node should correspond in two different images, so it’s not surprising that the network did so badly. We need something that can extract out features from the image irregardless of where Rick is in the image. There are approaches build precicesly for image analysis that do just this: convolutional neural networks. 11.2 Convolutional neural networks Convolutional neural networks essentially scan through an image and extract out a set of features. In multilayer neural networks, these features might then be passed on to deeper layers (other convolutional layers or standard neurons) as shown in Figure 11.3. Figure 11.3: Example of a multilayer convolutional neural network In kerasR we can add a convolutional layer using {Conv2D}. A multilayer convolutional neural network might look something like: mod &lt;- Sequential() mod$add(Conv2D(filters = 20, kernel_size = c(5, 5),input_shape = c(90, 160, 3))) mod$add(Activation(&quot;relu&quot;)) mod$add(MaxPooling2D(pool_size=c(3, 3))) mod$add(Conv2D(filters = 20, kernel_size = c(5, 5))) mod$add(Activation(&quot;relu&quot;)) mod$add(MaxPooling2D(pool_size=c(3, 3))) mod$add(Conv2D(filters = 64, kernel_size = c(5, 5))) mod$add(Activation(&quot;relu&quot;)) mod$add(MaxPooling2D(pool_size=c(3, 3))) mod$add(Flatten()) mod$add(Dense(100)) mod$add(Activation(&quot;relu&quot;)) mod$add(Dropout(0.6)) mod$add(Dense(2)) mod$add(Activation(&quot;sigmoid&quot;)) keras_compile(mod, loss = &#39;binary_crossentropy&#39;, metrics = c(&#39;accuracy&#39;), optimizer = RMSprop()) keras_fit(mod, trainX, trainY, validation_data = list(validateX, validateY), batch_size = 32, epochs = 25, verbose = 1) Again we can visualise this network: plot_model(mod,&#39;images/DNN3.png&#39;) Figure 11.4: Example of a multilayer convolutional neural network Okay, so now we have achieved a better accuracy: we have an accuracy of \\(0.8462\\) on the validation dataset at epoch \\(23\\), with a training accuracy of \\(0.9688\\). Whilst this is still not great, it’s accurate enough to begin useuflly making predictions and visualising the results. We have a trained model for classification of Rick, we can use it to make predictions for images not present in either the training or validation datasets. First load in the new set of images, which can be found in the {predictions} subfolder: files &lt;- list.files(path = &quot;data/RickandMorty/data/predictions/&quot;,pattern = &quot;jpg&quot;) predictX &lt;- array(0,dim=c(length(files),90,160,3)) for (i in 1:length(files)){ x &lt;- readJPEG(paste(&quot;data/RickandMorty/data/predictions/&quot;, files[i],sep=&quot;&quot;)) predictX[i,1:90,1:160,1:3] &lt;- x[1:90,1:160,1:3] } A hard classification can be assigned using the {keras_predict_classes} function, whilst the probability of assignment to either class can be evaluated using {keras_predict_proba} (this can be useful for images that might be ambiguous). probY &lt;- keras_predict_proba(mod, predictX) predictY &lt;- keras_predict_classes(mod, predictX) We can plot an example: choice = 13 if (predictY[choice]==0) { grid.raster(predictX[choice,1:90,1:160,1:3], interpolate=FALSE) grid.text(label=&#39;Rick&#39;,x = 0.4, y = 0.77,just = c(&quot;left&quot;, &quot;top&quot;), gp=gpar(fontsize=15, col=&quot;grey&quot;)) } else { grid.raster(predictX[choice,1:90,1:160,1:3], interpolate=FALSE) grid.text(label=&#39;Not Rick&#39;,x = 0.4, y = 0.77,just = c(&quot;left&quot;, &quot;top&quot;), gp=gpar(fontsize=15, col=&quot;grey&quot;)) } choice = 1 if (predictY[choice]==0) { grid.raster(predictX[choice,1:90,1:160,1:3], interpolate=FALSE) grid.text(label=&#39;Rick&#39;,x = 0.4, y = 0.77,just = c(&quot;left&quot;, &quot;top&quot;), gp=gpar(fontsize=15, col=&quot;grey&quot;)) } else { grid.raster(predictX[choice,1:90,1:160,1:3], interpolate=FALSE) grid.text(label=&#39;Not Rick&#39;,x = 0.4, y = 0.77,just = c(&quot;left&quot;, &quot;top&quot;), gp=gpar(fontsize=15, col=&quot;grey&quot;)) } choice = 6 if (predictY[choice]==0) { grid.raster(predictX[choice,1:90,1:160,1:3], interpolate=FALSE) grid.text(label=&#39;Rick&#39;,x = 0.4, y = 0.77,just = c(&quot;left&quot;, &quot;top&quot;), gp=gpar(fontsize=15, col=&quot;grey&quot;)) } else { grid.raster(predictX[choice,1:90,1:160,1:3], interpolate=FALSE) grid.text(label=&#39;Not Rick&#39;,x = 0.4, y = 0.77,just = c(&quot;left&quot;, &quot;top&quot;), gp=gpar(fontsize=15, col=&quot;grey&quot;)) } choice = 16 if (predictY[choice]==0) { grid.raster(predictX[choice,1:90,1:160,1:3], interpolate=FALSE) grid.text(label=&#39;Rick&#39;,x = 0.4, y = 0.77,just = c(&quot;left&quot;, &quot;top&quot;), gp=gpar(fontsize=15, col=&quot;grey&quot;)) } else { grid.raster(predictX[choice,1:90,1:160,1:3], interpolate=FALSE) grid.text(label=&#39;Not Rick: must be a Jerry&#39;,x = 0.2, y = 0.77,just = c(&quot;left&quot;, &quot;top&quot;), gp=gpar(fontsize=15, col=&quot;grey&quot;)) } 11.2.1 Data augmentation Although we saw some imporovements in the previous section using convolutional neural networks, the end results were not particularly convincing. After all, previous applications in the recognition of handwritten digits (0-9) showed above human accuracy, see e.g., Neural Networks and Deep Learning. Our accuracy of approximately \\(80\\) percent is nowhere near human levels of accuracy. So where are we gong wrong? We should, of course, start by considering the number of parameters versus the size of the training dataset. In our final model we had \\(69506\\) parameters, and only \\(2000\\) training images, so it is perhaps not surprising that our model is doing relatively poorly. In previous examples of digit recognition more than \\(10000\\) images were used, whilst better known examples of Deep Learning for image classification make use of millions of images. Our task is also, arguably, a lot harder than digit recognition. After all, a handwritten \\(0\\) is relatively similar regardless of who wrote it. Rick Sanchez, on the other hand, can come in a diverse range of guises, with different postures, facial expressions, clothing, and even in pickle-Rick form. We may well need a vastly increased number of training; with more training data, we can begin to learn more robustly what features define a Rick. Whilst we could simply download more data from Master of All Science, an alternative approach is to atrificially increase our pool of training data by manipulating the images. For example, we could shear, warp or rotate some of the images in our training set; we could add noise and we could manipulate the colouring. For example, when we artifically increase the training size in a Python implementation to \\(10000\\) we achieve an accuracy to \\(0.8770\\) on the validation data; when we artifically increase the training dataset to \\(30000\\) we pushed this above \\(0.9\\). Again, not quite human-level, but a reasonable accuracy, all things considered. 11.2.2 Asking more precise questions Another way we could improve our accuracy is to ask more precise questions. In our application we have focused on what makes a Rick, and what makes a not Rick. Whilst there may be definable features for Rick, such as his hair and his white coat, the class not Rick is an amalgamation of all other characters and scenes in the series. A better approach would be to develop algorithms that classify Rick versus Morty. In this case we would need to tweak our training and validation datasets. 11.2.3 More complex networks More complex learning algorithms can easily be built using keras using the Model class API rather than the Sequential API. This allows, for example, learning from multiple inputs and/or outputs, with more interconnection between different layers. We might, for example, want to include additional contextual information about the image that could serve to augment the predictions. Another approach is to use transfer learning. This is where we make use of existing neural networks to make predictions on our specific datasets, usually fixing the top layers in place and fine tuning the lower layers to our dataset. For example, for image recognition we could make use of top perfoming neural networks on the ImageNet database. Whilst none of these networks would have been designed to identify Rick they would have been trained on millions of images, and the top levels would have been able to extract useful general features of an image. 11.3 Further reading A particularly comprehensive introduction to Deep Learning can be found in the e-book Neural Networks and Deep Learning, written by Michael Nielsen. Useful examples can also be found in the keras documentation. Installing Python Linux Installing Python for Mac Python install via conda Installing tensorflow Installing keras Solutions to exercises can be found in appendix ??. References "],
["resources.html", "A Resources A.1 Python A.2 Machine learning data set repositories", " A Resources A.1 Python scikit-learn A.2 Machine learning data set repositories A.2.1 MLDATA mldata.org This repository manages the following types of objects: Data Sets - Raw data as a collection of similarily structured objects. Material and Methods - Descriptions of the computational pipeline. Learning Tasks - Learning tasks defined on raw data. Challenges - Collections of tasks which have a particular theme. A.2.2 UCI Machine Learning Repository Machine learning database at the University of California, Irvine, School of Information and Computer Sciences (Lichman 2013). References "],
["solutions-linear-models.html", "B Solutions ch. 3 - Linear models and matrix algebra B.1 Example 2 B.2 Example 2", " B Solutions ch. 3 - Linear models and matrix algebra Solutions to exercises of chapter 3. B.1 Example 2 We already know the equation that describes the data very well from high school physics. \\[d = h_0 + v_0 t - 0.5 \\times 9.8 t^2\\] with \\(h_0\\) and \\(v_0\\) the starting height and velocity respectively. The data we simulated above followed this equation and added measurement error to simulate n observations for dropping the ball \\((v_0=0)\\) from from height \\((h_0=56.67)\\) Here is what the data looks like with the solid line representing the true trajectory: g &lt;- 9.8 ##meters per second n &lt;- 25 tt &lt;- seq(0,3.4,len=n) ##time in secs, t is a base function f &lt;- 56.67 - 0.5*g*tt^2 y &lt;- f + rnorm(n,sd=1) plot(tt,y,ylab=&quot;Distance in meters&quot;,xlab=&quot;Time in seconds&quot;) lines(tt,f,col=2) In R we can fit this model by simply using the lm function. tt2 &lt;-tt^2 fit &lt;- lm(y~tt+tt2) summary(fit)$coef ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 56.6593133 0.5659753 100.1091642 9.375216e-31 ## tt -0.5847194 0.7709777 -0.7584129 4.562524e-01 ## tt2 -4.6891430 0.2190235 -21.4093096 3.195505e-16 B.2 Example 2 data(father.son,package=&quot;UsingR&quot;) x=father.son$fheight y=father.son$sheight X &lt;- cbind(1,x) thetahat &lt;- solve( t(X) %*% X ) %*% t(X) %*% y ###or thetahat &lt;- solve( crossprod(X) ) %*% crossprod( X, y ) We can see the results of this by computing the estimated \\(\\hat{\\theta}_0+\\hat{\\theta}_1 x\\) for any value of \\(x\\): newx &lt;- seq(min(x),max(x),len=100) X &lt;- cbind(1,newx) fitted &lt;- X%*%thetahat plot(x,y,xlab=&quot;Father&#39;s height&quot;,ylab=&quot;Son&#39;s height&quot;) lines(newx,fitted,col=2) This \\(\\hat{\\boldsymbol{\\theta}}=(\\mathbf{X}^\\top \\mathbf{X})^{-1} \\mathbf{X}^\\top \\mathbf{Y}\\) is one of the most widely used results in data analysis. "],
["solutions-logistic-regression.html", "C Solutions ch. 4 - Linear and non-linear (logistic) regression", " C Solutions ch. 4 - Linear and non-linear (logistic) regression Solutions to exercises of chapter 4. Exercise 3.1. options(warn=-1) geneindex &lt;- 36 D &lt;- read.csv(file = &quot;data/Arabidopsis/Arabidopsis_Botrytis_transpose_2.csv&quot;, header = TRUE, sep = &quot;,&quot;, row.names=1) genenames &lt;- colnames(D) Xs &lt;- D$Time[1:24] plot(Xs,(D[1:24,geneindex]),type=&quot;p&quot;,col=&quot;black&quot;,ylim=c(min(D[,geneindex])-0.2, max(D[,geneindex]+0.2)),main=genenames[geneindex],xlab = &quot;Time&quot;, ylab = &quot;log_2 expression&quot;) points(Xs,(D[25:nrow(D),geneindex]),type=&quot;p&quot;,col=&quot;red&quot;) Exercise 3.2. The caret package has a variety of features that are of use in ML. In the previous example, above, we fitted a linear model to a gene to identify parameters and make predictions using all the data. A better approach would be to partition the data into a test We can make use of the caret functionality to split our data into training and test sets, which should allow us to gauge uncertainty in our parameters and the strength of the model. Exercise 3.3. Linear regression can generally be applied for any number of variables. A notable example, would be to regress the expression pattern of a gene against putative regulators. library(caret) ## Loading required package: lattice ## Loading required package: ggplot2 geneindex &lt;- 10 lrfit3 &lt;- train(y~., data=data.frame(x=D[1:24,3:10],y=D[1:24,geneindex]), method = &quot;lm&quot;) Excercise 3.4. Compare the RMSE for various polynomial models versus that of the linear models. lrfit2 &lt;- train(y~poly(x,degree=1), data=data.frame(x=D[25:nrow(D),1],y=D[25:nrow(D),geneindex]), method = &quot;lm&quot;) lrfit3 &lt;- train(y~poly(x,degree=3), data=data.frame(x=D[25:nrow(D),1],y=D[25:nrow(D),geneindex]), method = &quot;lm&quot;) lrfit4 &lt;- train(y~poly(x,degree=20), data=data.frame(x=D[25:nrow(D),1],y=D[25:nrow(D),geneindex]), method = &quot;lm&quot;) plot(Xs,D[25:nrow(D),geneindex],type=&quot;p&quot;,col=&quot;black&quot;,ylim=c(min(D[,geneindex])-0.2, max(D[,geneindex]+0.2)),main=genenames[geneindex]) lines(Xs,fitted(lrfit2),type=&quot;l&quot;,col=&quot;blue&quot;) lines(Xs,fitted(lrfit3),type=&quot;l&quot;,col=&quot;red&quot;) lines(Xs,fitted(lrfit4),type=&quot;l&quot;,col=&quot;black&quot;) We can look at the RMSE: barplot(c(lrfit2$results$RMSE,lrfit3$results$RMSE,lrfit4$results$RMSE)) Excercise 3.4 (optional): Example covariance functions implemented from the Kernel Cookbook. Here we implement a rational quadratic covariance function: covRQ &lt;- function(X1,X2,l=1,sig=1,a=2) { K &lt;- matrix(rep(0, length(X1)*length(X2)), nrow=length(X1)) for (i in 1:nrow(K)) { for (j in 1:ncol(K)) { K[i,j] &lt;- sig^2*(1 + (abs(X1[i]-X2[j])^2/(2*a*l^2)) )^a } } return(K) } Here we implement a periodic covariance function: covPer &lt;- function(X1,X2,l=1,sig=1,p=1) { K &lt;- matrix(rep(0, length(X1)*length(X2)), nrow=length(X1)) for (i in 1:nrow(K)) { for (j in 1:ncol(K)) { K[i,j] &lt;- sig^2*exp(sin(pi*abs(X1[i]-X2[j])/p)^2 / l^2) } } return(K) } Exercise 3.5: Try fitting plotting the GP for the optimised values of the hyperparameters. We need to borrow the following snippets of code from the main text. require(MASS) ## Loading required package: MASS require(plyr) ## Loading required package: plyr require(reshape2) ## Loading required package: reshape2 require(ggplot2) covSE &lt;- function(X1,X2,l=1,sig=1) { K &lt;- matrix(rep(0, length(X1)*length(X2)), nrow=length(X1)) for (i in 1:nrow(K)) { for (j in 1:ncol(K)) { K[i,j] &lt;- sig^2*exp(-0.5*(abs(X1[i]-X2[j]))^2 /l^2) } } return(K) } x.star &lt;- seq(-5,5,len=500) f &lt;- data.frame(x=c(-4,-3,-2,-1,0,1,2), y=sin(c(-4,-3,-2,-1,0,1,2))) x &lt;- f$x k.xx &lt;- covSE(x,x) k.xxs &lt;- covSE(x,x.star) k.xsx &lt;- covSE(x.star,x) k.xsxs &lt;- covSE(x.star,x.star) f.star.bar &lt;- k.xsx%*%solve(k.xx)%*%f$y #Mean cov.f.star &lt;- k.xsxs - k.xsx%*%solve(k.xx)%*%k.xxs #Var y1 &lt;- mvrnorm(1, f.star.bar, cov.f.star) y2 &lt;- mvrnorm(1, f.star.bar, cov.f.star) y3 &lt;- mvrnorm(1, f.star.bar, cov.f.star) plot(x.star,sin(x.star),type = &#39;l&#39;,col=&quot;red&quot;,ylim=c(-2.2, 2.2)) points(f,type = &#39;p&#39;,col=&quot;blue&quot;) lines(x.star,y1,type = &#39;l&#39;,col=&quot;blue&quot;) lines(x.star,y2,type = &#39;l&#39;,col=&quot;blue&quot;) lines(x.star,y3,type = &#39;l&#39;,col=&quot;blue&quot;) Exercise 3.6: Try fitting plotting the GP for the optimised values of the hyperparameters. calcML &lt;- function(f,l=1,sig=1) { f2 &lt;- t(f) yt &lt;- f2[2,] y &lt;- f[,2] K &lt;- covSE(f[,1],f[,1],l,sig) ML &lt;- -0.5*yt%*%ginv(K+0.1^2*diag(length(y)))%*%y -0.5*log(det(K)) -(length(f[,1])/2)*log(2*pi); return(ML) } #install.packages(&quot;plot3D&quot;) library(plot3D) par &lt;- seq(.1,10,by=0.1) ML &lt;- matrix(rep(0, length(par)^2), nrow=length(par), ncol=length(par)) for(i in 1:length(par)) { for(j in 1:length(par)) { ML[i,j] &lt;- calcML(f,par[i],par[j]) } } ind&lt;-which(ML==max(ML), arr.ind=TRUE) lmap&lt;-par[ind[1]] varmap&lt;-par[ind[2]] x.star &lt;- seq(-5,5,len=500) f &lt;- data.frame(x=c(-4,-3,-2,-1,0,1,2), y=sin(c(-4,-3,-2,-1,0,1,2))) x &lt;- f$x k.xx &lt;- covSE(x,x,lmap,varmap) k.xxs &lt;- covSE(x,x.star,lmap,varmap) k.xsx &lt;- covSE(x.star,x,lmap,varmap) k.xsxs &lt;- covSE(x.star,x.star,lmap,varmap) f.star.bar &lt;- k.xsx%*%solve(k.xx)%*%f$y #Mean cov.f.star &lt;- k.xsxs - k.xsx%*%solve(k.xx)%*%k.xxs #Var plot(x.star,sin(x.star),type = &#39;l&#39;,col=&quot;red&quot;,ylim=c(-2.2, 2.2)) points(f,type=&#39;o&#39;) lines(x.star,f.star.bar,type = &#39;l&#39;) lines(x.star,f.star.bar+2*sqrt(diag(cov.f.star)),type = &#39;l&#39;,pch=22, lty=2, col=&quot;black&quot;) lines(x.star,f.star.bar-2*sqrt(diag(cov.f.star)),type = &#39;l&#39;,pch=22, lty=2, col=&quot;black&quot;) Excercise 3.7: Now try fitting a Gaussian process to one of the gene expression profiles in the Botrytis dataset. covSEn &lt;- function(X1,X2,l=1,sig=1,sigman=0.1) { K &lt;- matrix(rep(0, length(X1)*length(X2)), nrow=length(X1)) for (i in 1:nrow(K)) { for (j in 1:ncol(K)) { K[i,j] &lt;- sig^2*exp(-0.5*(abs(X1[i]-X2[j]))^2 /l^2) if (i==j){ K[i,j] &lt;- K[i,j] + sigman^2 } } } return(K) } geneindex &lt;- 36 lmap &lt;- 0.1 varmap &lt;- 5 x.star &lt;- seq(0,1,len=500) f &lt;- data.frame(x=D[25:nrow(D),1]/48, y=D[25:nrow(D),geneindex]) x &lt;- f$x k.xx &lt;- covSEn(x,x,lmap,varmap,0.2) k.xxs &lt;- covSEn(x,x.star,lmap,varmap,0.2) k.xsx &lt;- covSEn(x.star,x,lmap,varmap,0.2) k.xsxs &lt;- covSEn(x.star,x.star,lmap,varmap,0.2) f.star.bar &lt;- k.xsx%*%solve(k.xx)%*%f$y #Mean cov.f.star &lt;- k.xsxs - k.xsx%*%solve(k.xx)%*%k.xxs #Var plot(f,type = &#39;l&#39;,col=&quot;red&quot;) points(f,type=&#39;o&#39;) lines(x.star,f.star.bar,type = &#39;l&#39;) lines(x.star,f.star.bar+2*sqrt(diag(cov.f.star)),type = &#39;l&#39;,pch=22, lty=2, col=&quot;black&quot;) lines(x.star,f.star.bar-2*sqrt(diag(cov.f.star)),type = &#39;l&#39;,pch=22, lty=2, col=&quot;black&quot;) calcMLn &lt;- function(f,l=1,sig=1,sigman=0.1) { f2 &lt;- t(f) yt &lt;- f2[2,] y &lt;- f[,2] K &lt;- covSE(f[,1],f[,1],l,sig) ML &lt;- -0.5*yt%*%ginv(K+diag(length(y))*sigman^2)%*%y -0.5*log(det(K+diag(length(y))*sigman^2)) -(length(f[,1])/2)*log(2*pi); return(ML) } Exercise 3.8 (optional): Write a function for determining differential expression for two genes. Hint: we are interested in comparing two models, and using Bayes’ Factor to determine if the genes are differentially expressed. f &lt;- data.frame(x=D[25:nrow(D),1]/48, y=D[25:nrow(D),geneindex]) par &lt;- seq(.1,10,by=0.1) ML &lt;- matrix(rep(0, length(par)^2), nrow=length(par), ncol=length(par)) for(i in 1:length(par)) { for(j in 1:length(par)) { ML[i,j] &lt;- calcMLn(f,par[i],par[j],0.05) } } persp3D(z = ML,theta = 120) ind&lt;-which(ML==max(ML), arr.ind=TRUE) Now let’s calculate the BF. lmap &lt;- par[ind[1]] varmap &lt;- par[ind[2]] f1 &lt;- data.frame(x=D[1:24,1]/48, y=D[1:24,geneindex]) f2 &lt;- data.frame(x=D[25:nrow(D),1]/48, y=D[25:nrow(D),geneindex]) f3 &lt;- data.frame(x=D[,1]/48, y=D[,geneindex]) MLs &lt;- matrix(rep(0, 3, nrow=3)) MLs[1] &lt;- calcMLn(f1,lmap,varmap,0.05) MLs[2] &lt;- calcMLn(f2,lmap,varmap,0.05) MLs[3] &lt;- calcMLn(f3,lmap,varmap,0.05) BF &lt;- (MLs[1]+MLs[2]) -MLs[3] BF ## [1] 2749.534 So from the Bayes’ Factor there’s some slight evidence for model 1 (differential expression) over model 2 (non-differential expression). "],
["solutions-dimensionality-reduction.html", "D Solutions ch. 5 - Dimensionality reduction D.1 Exercise 5.1 D.2 Exercise 5.2 D.3 Exercise 5.3. D.4 Exercise 5.4. D.5 Exercise 5.5 D.6 Exercise 5.6. D.7 Exercise 5.7. D.8 Exercise 5.8.", " D Solutions ch. 5 - Dimensionality reduction Solutions to exercises of chapter 5. D.1 Exercise 5.1 We can read the data in using the following command: D &lt;- read.csv(file = &quot;data/PGC_transcriptomics/PGC_transcriptomics.csv&quot;, header = TRUE, sep = &quot;,&quot;, row.names=1) genenames &lt;- rownames(D) genenames &lt;- genenames[4:nrow(D)] This reads in the corresponding spreadsheet into the R environment as a data frame variable. D.2 Exercise 5.2 We will first run PCA on the data. Recall that the data is already log_2 normalised, with expression values beginning from row 4. Within R we would run: pcaresult &lt;- prcomp(t(D[4:nrow(D),1:ncol(D)]), center = TRUE, scale. = FALSE) Here we have opted to centre the data, but have not normalised each gene to be zero-mean. This is beacuse we are dealing entirely with gene expression, rather than a variety of variables that may exist on different scales. We can extract the positions of individual cells from the variable. In the snipped below we index the different cells types (ESC, pre-implantation cells, primordial germ cells and somatic cells) for easier plotting. y1 &lt;- pcaresult$x[which(D[1,]==-1),1:2] # PCA y2 &lt;- pcaresult$x[which(D[1,]==0),1:2] # y3 &lt;- pcaresult$x[which(D[1,]==1),1:2] # y4 &lt;- pcaresult$x[which(D[1,]==2),1:2] # Finally, we can plot the data as follows: plot(y1,type=&quot;p&quot;,col=&quot;red&quot;,xlim=c(-100, 100),ylim=c(-50, 50)) points(y2,type=&quot;p&quot;,col=&quot;black&quot;) points(y3,type=&quot;p&quot;,col=&quot;blue&quot;) points(y4,type=&quot;p&quot;,col=&quot;green&quot;) legend(-95, 50, legend=c(&quot;ESC&quot;, &quot;preimp&quot;, &quot;PGC&quot;, &quot;soma&quot;), col=c(&quot;red&quot;, &quot;black&quot;, &quot;blue&quot;, &quot;green&quot;), pch=&quot;o&quot;, bty=&quot;n&quot;, cex=0.8) From the plot, we can see PCA has done a reasonable job of separating out various cells. For example, a cluster of PGCs appears at the top of the plot, with somatic cells towards the lower right hand side. Pre-implantation embryos and ESCs appear to cluster together: perhaps this is not surprising as the ESCs are derived from blastocyst cells. Loosely, we can interpret the PC1 as dividing pre-implantation cells from somatic cells, with PC2 separating out PGCs. D.3 Exercise 5.3. In the previous exercise we used PCA to reduce the dimensionality of our data from thousands of genes down to two principle components. By eye, PCA appeared to do a reasonable job separating out different cell types. A useful next step might therefore be to perform clustering on the reduced dimensional space, which can be done using: clust &lt;- kmeans(pcaresult$x[,1:2], 4, iter.max = 1000) We can now compare the cluster assignment to the known cell types: Labels &lt;- vector(&quot;character&quot;, ncol(D)) Labels[which(D[1,]==-1)] = &quot;ESC&quot; Labels[which(D[1,]==0)] = &quot;preimp&quot; Labels[which(D[1,]==1)] = &quot;PGC&quot; Labels[which(D[1,]==2)] = &quot;soma&quot; clusterresults &lt;- rbind(Labels,clust$cluster) We note that, in general PGCs fall into one or more separate clusters, with soma also separating out well. ESCs and pre-implantation tend to fall into identical clusters. We can take a look at what cell types fall into a specific cluster: clusterresults[1,which(clusterresults[2,]==1)] ## PGC.109 PGC.111 PGC.144 PGC.145 PGC.202 PGC.203 PGC.204 PGC.206 PGC.208 ## &quot;PGC&quot; &quot;PGC&quot; &quot;PGC&quot; &quot;PGC&quot; &quot;PGC&quot; &quot;PGC&quot; &quot;PGC&quot; &quot;PGC&quot; &quot;PGC&quot; ## PGC.209 PGC.210 PGC.211 PGC.212 PGC.213 PGC.214 PGC.215 PGC.216 PGC.218 ## &quot;PGC&quot; &quot;PGC&quot; &quot;PGC&quot; &quot;PGC&quot; &quot;PGC&quot; &quot;PGC&quot; &quot;PGC&quot; &quot;PGC&quot; &quot;PGC&quot; ## PGC.219 PGC.220 PGC.221 PGC.222 PGC.223 PGC.224 PGC.225 PGC.226 PGC.227 ## &quot;PGC&quot; &quot;PGC&quot; &quot;PGC&quot; &quot;PGC&quot; &quot;PGC&quot; &quot;PGC&quot; &quot;PGC&quot; &quot;PGC&quot; &quot;PGC&quot; ## PGC.228 PGC.229 PGC.230 PGC.231 PGC.232 ## &quot;PGC&quot; &quot;PGC&quot; &quot;PGC&quot; &quot;PGC&quot; &quot;PGC&quot; clusterresults[1,which(clusterresults[2,]==2)] ## preimp.51 ESC.31 PGC.192 PGC.193 PGC.194 PGC.196 PGC.197 ## &quot;preimp&quot; &quot;ESC&quot; &quot;PGC&quot; &quot;PGC&quot; &quot;PGC&quot; &quot;PGC&quot; &quot;PGC&quot; ## PGC.199 PGC.200 PGC.201 soma soma.1 soma.2 soma.3 ## &quot;PGC&quot; &quot;PGC&quot; &quot;PGC&quot; &quot;soma&quot; &quot;soma&quot; &quot;soma&quot; &quot;soma&quot; ## soma.4 soma.5 soma.6 soma.7 soma.8 soma.9 soma.10 ## &quot;soma&quot; &quot;soma&quot; &quot;soma&quot; &quot;soma&quot; &quot;soma&quot; &quot;soma&quot; &quot;soma&quot; ## soma.11 soma.12 soma.13 soma.14 soma.15 soma.16 soma.17 ## &quot;soma&quot; &quot;soma&quot; &quot;soma&quot; &quot;soma&quot; &quot;soma&quot; &quot;soma&quot; &quot;soma&quot; ## soma.18 soma.19 soma.20 soma.21 soma.22 soma.23 soma.24 ## &quot;soma&quot; &quot;soma&quot; &quot;soma&quot; &quot;soma&quot; &quot;soma&quot; &quot;soma&quot; &quot;soma&quot; ## soma.25 soma.26 soma.27 soma.28 soma.29 soma.30 soma.31 ## &quot;soma&quot; &quot;soma&quot; &quot;soma&quot; &quot;soma&quot; &quot;soma&quot; &quot;soma&quot; &quot;soma&quot; ## soma.32 soma.33 soma.34 soma.35 soma.36 soma.37 soma.38 ## &quot;soma&quot; &quot;soma&quot; &quot;soma&quot; &quot;soma&quot; &quot;soma&quot; &quot;soma&quot; &quot;soma&quot; ## soma.39 soma.40 soma.41 soma.42 soma.43 soma.44 soma.45 ## &quot;soma&quot; &quot;soma&quot; &quot;soma&quot; &quot;soma&quot; &quot;soma&quot; &quot;soma&quot; &quot;soma&quot; ## soma.46 soma.47 soma.50 soma.53 soma.54 soma.55 soma.56 ## &quot;soma&quot; &quot;soma&quot; &quot;soma&quot; &quot;soma&quot; &quot;soma&quot; &quot;soma&quot; &quot;soma&quot; ## soma.57 soma.58 soma.59 soma.60 soma.61 soma.62 soma.63 ## &quot;soma&quot; &quot;soma&quot; &quot;soma&quot; &quot;soma&quot; &quot;soma&quot; &quot;soma&quot; &quot;soma&quot; ## soma.64 soma.65 soma.66 soma.67 soma.68 soma.69 soma.70 ## &quot;soma&quot; &quot;soma&quot; &quot;soma&quot; &quot;soma&quot; &quot;soma&quot; &quot;soma&quot; &quot;soma&quot; ## soma.71 soma.72 soma.73 soma.74 soma.75 soma.76 soma.77 ## &quot;soma&quot; &quot;soma&quot; &quot;soma&quot; &quot;soma&quot; &quot;soma&quot; &quot;soma&quot; &quot;soma&quot; ## soma.78 soma.79 soma.80 soma.81 soma.82 soma.83 soma.84 ## &quot;soma&quot; &quot;soma&quot; &quot;soma&quot; &quot;soma&quot; &quot;soma&quot; &quot;soma&quot; &quot;soma&quot; ## soma.85 ## &quot;soma&quot; clusterresults[1,which(clusterresults[2,]==3)] ## preimp preimp.1 preimp.2 preimp.3 preimp.4 preimp.5 preimp.6 ## &quot;preimp&quot; &quot;preimp&quot; &quot;preimp&quot; &quot;preimp&quot; &quot;preimp&quot; &quot;preimp&quot; &quot;preimp&quot; ## preimp.7 preimp.8 preimp.9 preimp.10 preimp.11 ESC ESC.1 ## &quot;preimp&quot; &quot;preimp&quot; &quot;preimp&quot; &quot;preimp&quot; &quot;preimp&quot; &quot;ESC&quot; &quot;ESC&quot; ## preimp.12 preimp.13 preimp.14 preimp.15 preimp.16 preimp.17 preimp.18 ## &quot;preimp&quot; &quot;preimp&quot; &quot;preimp&quot; &quot;preimp&quot; &quot;preimp&quot; &quot;preimp&quot; &quot;preimp&quot; ## preimp.19 preimp.20 preimp.21 preimp.22 preimp.23 preimp.24 preimp.25 ## &quot;preimp&quot; &quot;preimp&quot; &quot;preimp&quot; &quot;preimp&quot; &quot;preimp&quot; &quot;preimp&quot; &quot;preimp&quot; ## preimp.26 preimp.27 preimp.28 preimp.29 preimp.30 preimp.31 preimp.32 ## &quot;preimp&quot; &quot;preimp&quot; &quot;preimp&quot; &quot;preimp&quot; &quot;preimp&quot; &quot;preimp&quot; &quot;preimp&quot; ## preimp.33 preimp.34 preimp.35 preimp.36 preimp.37 preimp.38 preimp.39 ## &quot;preimp&quot; &quot;preimp&quot; &quot;preimp&quot; &quot;preimp&quot; &quot;preimp&quot; &quot;preimp&quot; &quot;preimp&quot; ## preimp.40 preimp.41 preimp.42 preimp.43 preimp.44 preimp.45 preimp.46 ## &quot;preimp&quot; &quot;preimp&quot; &quot;preimp&quot; &quot;preimp&quot; &quot;preimp&quot; &quot;preimp&quot; &quot;preimp&quot; ## preimp.47 preimp.48 preimp.49 preimp.50 preimp.52 preimp.53 preimp.54 ## &quot;preimp&quot; &quot;preimp&quot; &quot;preimp&quot; &quot;preimp&quot; &quot;preimp&quot; &quot;preimp&quot; &quot;preimp&quot; ## preimp.55 preimp.56 preimp.57 preimp.58 preimp.59 preimp.60 preimp.61 ## &quot;preimp&quot; &quot;preimp&quot; &quot;preimp&quot; &quot;preimp&quot; &quot;preimp&quot; &quot;preimp&quot; &quot;preimp&quot; ## preimp.62 preimp.63 preimp.64 preimp.65 preimp.66 preimp.67 preimp.68 ## &quot;preimp&quot; &quot;preimp&quot; &quot;preimp&quot; &quot;preimp&quot; &quot;preimp&quot; &quot;preimp&quot; &quot;preimp&quot; ## preimp.69 preimp.70 preimp.71 preimp.72 preimp.73 preimp.74 preimp.75 ## &quot;preimp&quot; &quot;preimp&quot; &quot;preimp&quot; &quot;preimp&quot; &quot;preimp&quot; &quot;preimp&quot; &quot;preimp&quot; ## preimp.76 preimp.77 preimp.78 preimp.79 preimp.80 preimp.81 preimp.82 ## &quot;preimp&quot; &quot;preimp&quot; &quot;preimp&quot; &quot;preimp&quot; &quot;preimp&quot; &quot;preimp&quot; &quot;preimp&quot; ## preimp.83 preimp.84 preimp.85 preimp.86 preimp.87 preimp.88 preimp.89 ## &quot;preimp&quot; &quot;preimp&quot; &quot;preimp&quot; &quot;preimp&quot; &quot;preimp&quot; &quot;preimp&quot; &quot;preimp&quot; ## ESC.2 ESC.3 ESC.4 ESC.5 ESC.6 ESC.7 ESC.8 ## &quot;ESC&quot; &quot;ESC&quot; &quot;ESC&quot; &quot;ESC&quot; &quot;ESC&quot; &quot;ESC&quot; &quot;ESC&quot; ## ESC.9 ESC.10 ESC.11 ESC.12 ESC.13 ESC.14 ESC.15 ## &quot;ESC&quot; &quot;ESC&quot; &quot;ESC&quot; &quot;ESC&quot; &quot;ESC&quot; &quot;ESC&quot; &quot;ESC&quot; ## ESC.16 ESC.17 ESC.18 ESC.19 ESC.20 ESC.21 ESC.22 ## &quot;ESC&quot; &quot;ESC&quot; &quot;ESC&quot; &quot;ESC&quot; &quot;ESC&quot; &quot;ESC&quot; &quot;ESC&quot; ## ESC.23 ESC.24 ESC.25 ESC.26 ESC.27 ESC.28 ESC.29 ## &quot;ESC&quot; &quot;ESC&quot; &quot;ESC&quot; &quot;ESC&quot; &quot;ESC&quot; &quot;ESC&quot; &quot;ESC&quot; ## ESC.30 ESC.32 ESC.33 soma.48 soma.49 soma.51 soma.52 ## &quot;ESC&quot; &quot;ESC&quot; &quot;ESC&quot; &quot;soma&quot; &quot;soma&quot; &quot;soma&quot; &quot;soma&quot; clusterresults[1,which(clusterresults[2,]==4)] ## PGC PGC.1 PGC.2 PGC.3 PGC.4 PGC.5 PGC.6 PGC.7 PGC.8 ## &quot;PGC&quot; &quot;PGC&quot; &quot;PGC&quot; &quot;PGC&quot; &quot;PGC&quot; &quot;PGC&quot; &quot;PGC&quot; &quot;PGC&quot; &quot;PGC&quot; ## PGC.9 PGC.10 PGC.11 PGC.12 PGC.13 PGC.14 PGC.15 PGC.16 PGC.17 ## &quot;PGC&quot; &quot;PGC&quot; &quot;PGC&quot; &quot;PGC&quot; &quot;PGC&quot; &quot;PGC&quot; &quot;PGC&quot; &quot;PGC&quot; &quot;PGC&quot; ## PGC.18 PGC.19 PGC.20 PGC.21 PGC.22 PGC.23 PGC.24 PGC.25 PGC.26 ## &quot;PGC&quot; &quot;PGC&quot; &quot;PGC&quot; &quot;PGC&quot; &quot;PGC&quot; &quot;PGC&quot; &quot;PGC&quot; &quot;PGC&quot; &quot;PGC&quot; ## PGC.27 PGC.28 PGC.29 PGC.30 PGC.31 PGC.32 PGC.33 PGC.34 PGC.35 ## &quot;PGC&quot; &quot;PGC&quot; &quot;PGC&quot; &quot;PGC&quot; &quot;PGC&quot; &quot;PGC&quot; &quot;PGC&quot; &quot;PGC&quot; &quot;PGC&quot; ## PGC.36 PGC.37 PGC.38 PGC.39 PGC.40 PGC.41 PGC.42 PGC.43 PGC.44 ## &quot;PGC&quot; &quot;PGC&quot; &quot;PGC&quot; &quot;PGC&quot; &quot;PGC&quot; &quot;PGC&quot; &quot;PGC&quot; &quot;PGC&quot; &quot;PGC&quot; ## PGC.45 PGC.46 PGC.47 PGC.48 PGC.49 PGC.50 PGC.51 PGC.52 PGC.53 ## &quot;PGC&quot; &quot;PGC&quot; &quot;PGC&quot; &quot;PGC&quot; &quot;PGC&quot; &quot;PGC&quot; &quot;PGC&quot; &quot;PGC&quot; &quot;PGC&quot; ## PGC.54 PGC.55 PGC.56 PGC.57 PGC.58 PGC.59 PGC.60 PGC.61 PGC.62 ## &quot;PGC&quot; &quot;PGC&quot; &quot;PGC&quot; &quot;PGC&quot; &quot;PGC&quot; &quot;PGC&quot; &quot;PGC&quot; &quot;PGC&quot; &quot;PGC&quot; ## PGC.63 PGC.64 PGC.65 PGC.66 PGC.67 PGC.68 PGC.69 PGC.70 PGC.71 ## &quot;PGC&quot; &quot;PGC&quot; &quot;PGC&quot; &quot;PGC&quot; &quot;PGC&quot; &quot;PGC&quot; &quot;PGC&quot; &quot;PGC&quot; &quot;PGC&quot; ## PGC.72 PGC.73 PGC.74 PGC.75 PGC.76 PGC.77 PGC.78 PGC.79 PGC.80 ## &quot;PGC&quot; &quot;PGC&quot; &quot;PGC&quot; &quot;PGC&quot; &quot;PGC&quot; &quot;PGC&quot; &quot;PGC&quot; &quot;PGC&quot; &quot;PGC&quot; ## PGC.81 PGC.82 PGC.83 PGC.84 PGC.85 PGC.86 PGC.87 PGC.88 PGC.89 ## &quot;PGC&quot; &quot;PGC&quot; &quot;PGC&quot; &quot;PGC&quot; &quot;PGC&quot; &quot;PGC&quot; &quot;PGC&quot; &quot;PGC&quot; &quot;PGC&quot; ## PGC.90 PGC.91 PGC.92 PGC.93 PGC.94 PGC.95 PGC.96 PGC.97 PGC.98 ## &quot;PGC&quot; &quot;PGC&quot; &quot;PGC&quot; &quot;PGC&quot; &quot;PGC&quot; &quot;PGC&quot; &quot;PGC&quot; &quot;PGC&quot; &quot;PGC&quot; ## PGC.99 PGC.100 PGC.101 PGC.102 PGC.103 PGC.104 PGC.105 PGC.106 PGC.107 ## &quot;PGC&quot; &quot;PGC&quot; &quot;PGC&quot; &quot;PGC&quot; &quot;PGC&quot; &quot;PGC&quot; &quot;PGC&quot; &quot;PGC&quot; &quot;PGC&quot; ## PGC.108 PGC.110 PGC.112 PGC.113 PGC.114 PGC.115 PGC.116 PGC.117 PGC.118 ## &quot;PGC&quot; &quot;PGC&quot; &quot;PGC&quot; &quot;PGC&quot; &quot;PGC&quot; &quot;PGC&quot; &quot;PGC&quot; &quot;PGC&quot; &quot;PGC&quot; ## PGC.119 PGC.120 PGC.121 PGC.122 PGC.123 PGC.124 PGC.125 PGC.126 PGC.127 ## &quot;PGC&quot; &quot;PGC&quot; &quot;PGC&quot; &quot;PGC&quot; &quot;PGC&quot; &quot;PGC&quot; &quot;PGC&quot; &quot;PGC&quot; &quot;PGC&quot; ## PGC.128 PGC.129 PGC.130 PGC.131 PGC.132 PGC.133 PGC.134 PGC.135 PGC.136 ## &quot;PGC&quot; &quot;PGC&quot; &quot;PGC&quot; &quot;PGC&quot; &quot;PGC&quot; &quot;PGC&quot; &quot;PGC&quot; &quot;PGC&quot; &quot;PGC&quot; ## PGC.137 PGC.138 PGC.139 PGC.140 PGC.141 PGC.142 PGC.143 PGC.146 PGC.147 ## &quot;PGC&quot; &quot;PGC&quot; &quot;PGC&quot; &quot;PGC&quot; &quot;PGC&quot; &quot;PGC&quot; &quot;PGC&quot; &quot;PGC&quot; &quot;PGC&quot; ## PGC.148 PGC.149 PGC.150 PGC.151 PGC.152 PGC.153 PGC.154 PGC.155 PGC.156 ## &quot;PGC&quot; &quot;PGC&quot; &quot;PGC&quot; &quot;PGC&quot; &quot;PGC&quot; &quot;PGC&quot; &quot;PGC&quot; &quot;PGC&quot; &quot;PGC&quot; ## PGC.157 PGC.158 PGC.159 PGC.160 PGC.161 PGC.162 PGC.163 PGC.164 PGC.165 ## &quot;PGC&quot; &quot;PGC&quot; &quot;PGC&quot; &quot;PGC&quot; &quot;PGC&quot; &quot;PGC&quot; &quot;PGC&quot; &quot;PGC&quot; &quot;PGC&quot; ## PGC.166 PGC.167 PGC.168 PGC.169 PGC.170 PGC.171 PGC.172 PGC.173 PGC.174 ## &quot;PGC&quot; &quot;PGC&quot; &quot;PGC&quot; &quot;PGC&quot; &quot;PGC&quot; &quot;PGC&quot; &quot;PGC&quot; &quot;PGC&quot; &quot;PGC&quot; ## PGC.175 PGC.176 PGC.177 PGC.178 PGC.179 PGC.180 PGC.181 PGC.182 PGC.183 ## &quot;PGC&quot; &quot;PGC&quot; &quot;PGC&quot; &quot;PGC&quot; &quot;PGC&quot; &quot;PGC&quot; &quot;PGC&quot; &quot;PGC&quot; &quot;PGC&quot; ## PGC.184 PGC.185 PGC.186 PGC.187 PGC.188 PGC.189 PGC.190 PGC.191 PGC.195 ## &quot;PGC&quot; &quot;PGC&quot; &quot;PGC&quot; &quot;PGC&quot; &quot;PGC&quot; &quot;PGC&quot; &quot;PGC&quot; &quot;PGC&quot; &quot;PGC&quot; ## PGC.198 PGC.205 PGC.207 PGC.217 PGC.233 PGC.234 PGC.235 PGC.236 PGC.237 ## &quot;PGC&quot; &quot;PGC&quot; &quot;PGC&quot; &quot;PGC&quot; &quot;PGC&quot; &quot;PGC&quot; &quot;PGC&quot; &quot;PGC&quot; &quot;PGC&quot; ## PGC.238 PGC.239 PGC.240 PGC.241 ## &quot;PGC&quot; &quot;PGC&quot; &quot;PGC&quot; &quot;PGC&quot; D.4 Exercise 5.4. In our previous section we identified clusters associated with various groups. In our application cluster 1 was associated primarily with pre-implantation cells, with cluster 3 associated with PGCs. We could therefore empirically look for genes that are differentially expressed. Since we know SOX17 is associated with PGC specification (Irie et al. 2015,Tang et al. (2015)) let’s first compare the expression levels of SOX17 in the two groups: t.test(D[which(genenames==&quot;SOX17&quot;)+3, which(clusterresults[2,]==1)],D[which(genenames==&quot;SOX17&quot;)+3, which(clusterresults[2,]==3)]) ## ## Welch Two Sample t-test ## ## data: D[which(genenames == &quot;SOX17&quot;) + 3, which(clusterresults[2, ] == and D[which(genenames == &quot;SOX17&quot;) + 3, which(clusterresults[2, ] == 1)] and 3)] ## t = 3.3819, df = 33.189, p-value = 0.001859 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## 0.5740331 2.3066516 ## sample estimates: ## mean of x mean of y ## 1.6785760 0.2382337 Typically we won’t always know the important genes, but can perform an unbiased analysis by testing all genes. pvalstore &lt;- vector(mode=&quot;numeric&quot;, length=length(genenames)) for (i in c(1:length(genenames))){ pvals &lt;- t.test(D[which(genenames==genenames[i])+3, which(clusterresults[2,]==1)],D[which(genenames==genenames[i])+3, which(clusterresults[2,]==3)]) pvalstore[i] &lt;- pvals$p.value } sortedgenes &lt;- genenames[order(pvalstore)] D.5 Exercise 5.5 Within our example, the original axes of our data have very obvious solutions: the axes represent the expression levels of individual genes. The PCs, however, represent linear combinations of various genes, and do not have obvious interpretations. To find an intuition, we can project the original axes (genes) into the new co-ordinate system. This is stored in variable. plot(pcaresult$rotation[,1:2],type=&quot;n&quot;) text(pcaresult$rotation[,1:2], genenames, cex = .4) Okay, this plot is a little busy, so let’s focus in on a particular region. Recall that PGCs seemed to lie towards the upper section of the plot (that is PC2 separated out PGCs from other cell types), so we’ll take a look at the top section: plot(pcaresult$rotation[,1:2],type=&quot;n&quot;,xlim=c(-0.07, 0.07),ylim=c(0.04, 0.1)) genenames &lt;- rownames(D) genenames &lt;- genenames[4:nrow(D)] text(pcaresult$rotation[,1:2], genenames, , cex = .4) We now see a number of genes that are potentially associated with PGCs. These include a number of known PGCs, for example, both SOX17 and PRDM1 (which can be found at co-ordinates PC1=0, PC2= 0.04) represent two key specifiers of human PGC fate (Irie et al. 2015,Tang et al. (2015),Kobayashi et al. (2017)). We further note a number of other key regulators, such as DAZL, have been implicated in germ cell development, with DAZL over expressed ESCs forming spermatogonia-like colonies in a rare instance upon xenotransplantation (Panula et al. 2016). We can similarly look at regions associated with early embryogenesis by concentrating on the lower half of the plot: plot(pcaresult$rotation[,1:2],type=&quot;n&quot;,xlim=c(0.0, 0.07),ylim=c(-0.07, -0.03)) genenames &lt;- rownames(D) genenames &lt;- genenames[4:nrow(D)] text(pcaresult$rotation[,1:2], genenames, , cex = .4) This appears to identify a number of genes associated with embryogenesis, for example, DPPA3, which encodes for a maternally inherited factor, Stella, required for normal pre-implantation development (Bortvin et al. 2004,Payer et al. (2003)) as well as regulation of transcriptional and endogenous retrovirus programs during maternal-to-zygotic transition (Huang et al. 2017). D.6 Exercise 5.6. We can run tSNE using the following command: library(Rtsne) set.seed(1) tsne_model_1 = Rtsne(as.matrix(t(D)), check_duplicates=FALSE, pca=TRUE, perplexity=100, theta=0.5, dims=2) As we did previously, we can plot the results using: y1 &lt;- tsne_model_1$Y[which(D[1,]==-1),1:2] y2 &lt;- tsne_model_1$Y[which(D[1,]==0),1:2] y3 &lt;- tsne_model_1$Y[which(D[1,]==1),1:2] y4 &lt;- tsne_model_1$Y[which(D[1,]==2),1:2] plot(y1,type=&quot;p&quot;,col=&quot;red&quot;,xlim=c(-45, 45),ylim=c(-45, 45)) points(y2,type=&quot;p&quot;,col=&quot;black&quot;) points(y3,type=&quot;p&quot;,col=&quot;blue&quot;) points(y4,type=&quot;p&quot;,col=&quot;green&quot;) legend(-40, 40, legend=c(&quot;ESC&quot;, &quot;preimp&quot;, &quot;PGC&quot;, &quot;soma&quot;), col=c(&quot;red&quot;, &quot;black&quot;, &quot;blue&quot;, &quot;green&quot;),pch=&quot;o&quot;, bty=&quot;n&quot;, cex=0.8) D.7 Exercise 5.7. We can plot the expression patterns for pre-implantation embryos: y2_0 &lt;- tsne_model_1$Y[which(D[1,]==0 &amp; D[3,]==0),1:2] y2_1 &lt;- tsne_model_1$Y[which(D[1,]==0 &amp; D[3,]==1),1:2] y2_2 &lt;- tsne_model_1$Y[which(D[1,]==0 &amp; D[3,]==2),1:2] y2_3 &lt;- tsne_model_1$Y[which(D[1,]==0 &amp; D[3,]==3),1:2] y2_4 &lt;- tsne_model_1$Y[which(D[1,]==0 &amp; D[3,]==4),1:2] y2_5 &lt;- tsne_model_1$Y[which(D[1,]==0 &amp; D[3,]==5),1:2] y2_6 &lt;- tsne_model_1$Y[which(D[1,]==0 &amp; D[3,]==6),1:2] plot(y2_0,type=&quot;p&quot;,col=&quot;tomato&quot;,xlim=c(-10, 0),ylim=c(-10, 10)) points(y2_2,type=&quot;p&quot;,col=&quot;tomato&quot;) points(y2_2,type=&quot;p&quot;,col=&quot;tomato1&quot;) points(y2_3,type=&quot;p&quot;,col=&quot;tomato1&quot;) points(y2_4,type=&quot;p&quot;,col=&quot;tomato2&quot;) points(y2_5,type=&quot;p&quot;,col=&quot;tomato3&quot;) points(y2_6,type=&quot;p&quot;,col=&quot;tomato4&quot;) legend(-10, 10, legend=c(&quot;Ooc&quot;, &quot;Zyg&quot;, &quot;2C&quot;, &quot;4C&quot;,&quot;8C&quot;,&quot;Mor&quot;,&quot;Blast&quot;), col=c(&quot;tomato&quot;, &quot;tomato&quot;, &quot;tomato1&quot;, &quot;tomato1&quot;, &quot;tomato2&quot;,&quot;tomato3&quot;,&quot;tomato4&quot;),pch=&quot;o&quot;, bty=&quot;n&quot;, cex=0.8) D.8 Exercise 5.8. Before we perform any clustering let’s generate labels for the pre-implantation data. Type &lt;- D[1,which(D[1,]==0 &amp; D[3,]&lt;7 &amp; D[3,]&gt;-1)] TPs &lt;- D[3,which(D[1,]==0 &amp; D[3,]&lt;7 &amp; D[3,]&gt;-1)] Labels &lt;- vector(&quot;character&quot;, length(which(D[1,]==0 &amp; D[3,]&lt;7 &amp; D[3,]&gt;-1))) Labels[which(Type==0 &amp; TPs==0)] = &quot;Oocyte&quot; Labels[which(Type==0 &amp; TPs==1)] = &quot;Zygote&quot; Labels[which(Type==0 &amp; TPs==2)] = &quot;2C&quot; Labels[which(Type==0 &amp; TPs==3)] = &quot;4C&quot; Labels[which(Type==0 &amp; TPs==4)] = &quot;8C&quot; Labels[which(Type==0 &amp; TPs==5)] = &quot;morula&quot; Labels[which(Type==0 &amp; TPs==6)] = &quot;blast&quot; Now generate some clusters: clust &lt;- kmeans(tsne_model_1$Y[which(D[1,]==0 &amp; D[3,]&lt;7 &amp; D[3,]&gt;-1),1:2], 4, iter.max = 1000) rbind(Labels,clust$cluster) ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] ## Labels &quot;Oocyte&quot; &quot;Oocyte&quot; &quot;Oocyte&quot; &quot;Zygote&quot; &quot;Zygote&quot; &quot;Zygote&quot; &quot;2C&quot; &quot;2C&quot; ## &quot;1&quot; &quot;1&quot; &quot;1&quot; &quot;1&quot; &quot;1&quot; &quot;1&quot; &quot;1&quot; &quot;1&quot; ## [,9] [,10] [,11] [,12] [,13] [,14] [,15] [,16] [,17] [,18] [,19] ## Labels &quot;2C&quot; &quot;2C&quot; &quot;2C&quot; &quot;2C&quot; &quot;4C&quot; &quot;4C&quot; &quot;4C&quot; &quot;4C&quot; &quot;4C&quot; &quot;4C&quot; &quot;4C&quot; ## &quot;1&quot; &quot;1&quot; &quot;1&quot; &quot;1&quot; &quot;1&quot; &quot;1&quot; &quot;1&quot; &quot;1&quot; &quot;1&quot; &quot;1&quot; &quot;1&quot; ## [,20] [,21] [,22] [,23] [,24] [,25] [,26] [,27] [,28] [,29] [,30] ## Labels &quot;4C&quot; &quot;4C&quot; &quot;4C&quot; &quot;4C&quot; &quot;4C&quot; &quot;8C&quot; &quot;8C&quot; &quot;8C&quot; &quot;8C&quot; &quot;8C&quot; &quot;8C&quot; ## &quot;1&quot; &quot;1&quot; &quot;1&quot; &quot;1&quot; &quot;1&quot; &quot;1&quot; &quot;1&quot; &quot;1&quot; &quot;1&quot; &quot;1&quot; &quot;1&quot; ## [,31] [,32] [,33] [,34] [,35] [,36] [,37] [,38] [,39] [,40] [,41] ## Labels &quot;8C&quot; &quot;8C&quot; &quot;8C&quot; &quot;8C&quot; &quot;8C&quot; &quot;8C&quot; &quot;8C&quot; &quot;8C&quot; &quot;8C&quot; &quot;8C&quot; &quot;8C&quot; ## &quot;1&quot; &quot;1&quot; &quot;1&quot; &quot;1&quot; &quot;1&quot; &quot;1&quot; &quot;1&quot; &quot;1&quot; &quot;1&quot; &quot;1&quot; &quot;1&quot; ## [,42] [,43] [,44] [,45] [,46] [,47] [,48] [,49] ## Labels &quot;8C&quot; &quot;8C&quot; &quot;8C&quot; &quot;morula&quot; &quot;morula&quot; &quot;morula&quot; &quot;morula&quot; &quot;morula&quot; ## &quot;1&quot; &quot;1&quot; &quot;1&quot; &quot;1&quot; &quot;1&quot; &quot;1&quot; &quot;1&quot; &quot;1&quot; ## [,50] [,51] [,52] [,53] [,54] [,55] [,56] ## Labels &quot;morula&quot; &quot;morula&quot; &quot;morula&quot; &quot;morula&quot; &quot;morula&quot; &quot;morula&quot; &quot;morula&quot; ## &quot;1&quot; &quot;1&quot; &quot;1&quot; &quot;1&quot; &quot;1&quot; &quot;1&quot; &quot;1&quot; ## [,57] [,58] [,59] [,60] [,61] [,62] [,63] [,64] ## Labels &quot;morula&quot; &quot;morula&quot; &quot;morula&quot; &quot;morula&quot; &quot;blast&quot; &quot;blast&quot; &quot;blast&quot; &quot;blast&quot; ## &quot;1&quot; &quot;1&quot; &quot;1&quot; &quot;1&quot; &quot;3&quot; &quot;3&quot; &quot;3&quot; &quot;3&quot; ## [,65] [,66] [,67] [,68] [,69] [,70] [,71] [,72] ## Labels &quot;blast&quot; &quot;blast&quot; &quot;blast&quot; &quot;blast&quot; &quot;blast&quot; &quot;blast&quot; &quot;blast&quot; &quot;blast&quot; ## &quot;3&quot; &quot;3&quot; &quot;3&quot; &quot;3&quot; &quot;3&quot; &quot;3&quot; &quot;3&quot; &quot;3&quot; ## [,73] [,74] [,75] [,76] [,77] [,78] [,79] [,80] ## Labels &quot;blast&quot; &quot;blast&quot; &quot;blast&quot; &quot;blast&quot; &quot;blast&quot; &quot;blast&quot; &quot;blast&quot; &quot;blast&quot; ## &quot;4&quot; &quot;2&quot; &quot;2&quot; &quot;3&quot; &quot;3&quot; &quot;3&quot; &quot;3&quot; &quot;4&quot; ## [,81] [,82] [,83] [,84] [,85] [,86] [,87] [,88] ## Labels &quot;blast&quot; &quot;blast&quot; &quot;blast&quot; &quot;blast&quot; &quot;blast&quot; &quot;blast&quot; &quot;blast&quot; &quot;blast&quot; ## &quot;4&quot; &quot;3&quot; &quot;2&quot; &quot;4&quot; &quot;3&quot; &quot;2&quot; &quot;2&quot; &quot;4&quot; ## [,89] [,90] ## Labels &quot;blast&quot; &quot;blast&quot; ## &quot;3&quot; &quot;4&quot; Note that, using tSNE we appear to have identified clear structure separating out the majority of blastocyst cells from other. Separate clusters appear to exist for other cell types as well, that were not immediately obvious in the PCA analyses of these datasets. In general, the PCA analysis separated out early-embryogenesis from soma and PGCs, due to large differences in the expression patterns of those cell types; comparatively, the expression patterns for the pre-implantation embryos were close together in the reduced dimensionality space. Due to its nonlinear nature, tSNE was able, in this case to more accurately separate out different cells types. References "],
["solutions-clustering.html", "E Solutions ch. 6 - Clustering E.1 Exercise 1", " E Solutions ch. 6 - Clustering Solutions to exercises of chapter 6. E.1 Exercise 1 First we need to read the image data and transform it into a suitable format for analysis: library(EBImage) library(ggplot2) img &lt;- readImage(&quot;data/histology/Emphysema_H_and_E.jpg&quot;) imgDim &lt;- dim(img) imgDF &lt;- data.frame( x = rep(1:imgDim[1], imgDim[2]), y = rep(imgDim[2]:1, each=imgDim[1]), r = as.vector(img[,,1]), g = as.vector(img[,,2]), b = as.vector(img[,,3]) ) Next we will perform kmeans clustering for k in the range 1:9. This is computationally quite intensive, so we’ll use parallel processing: library(doMC) ## Loading required package: foreach ## Loading required package: iterators ## Loading required package: parallel registerDoMC(detectCores()) k=1:9 set.seed(42) res &lt;- foreach( i=k, .options.multicore=list(set.seed=FALSE)) %dopar% kmeans(imgDF[,c(&quot;r&quot;, &quot;g&quot;, &quot;b&quot;)], i, nstart=50) We can now plot total within-cluster sum of squares against k: plot_tot_withinss &lt;- function(kmeans_output){ tot_withinss &lt;- sapply(k, function(i){kmeans_output[[i]]$tot.withinss}) qplot(k, tot_withinss, geom=c(&quot;point&quot;, &quot;line&quot;), ylab=&quot;Total within-cluster sum of squares&quot;) + theme_bw() } plot_tot_withinss(res) Figure E.1: Variance within the clusters of pixels. Total within-cluster sum of squares plotted against k. The plot of total within-cluster sum of squares against k (figure E.1) shows an elbow at k=2, indicating that most of the variance in the image can be described by just two clusters. Let’s plot the clusters for k=2. clusterColours &lt;- rgb(res[[2]]$centers) ggplot(data = imgDF, aes(x = x, y = y)) + geom_point(colour = clusterColours[res[[2]]$cluster]) + xlab(&quot;x&quot;) + ylab(&quot;y&quot;) + theme_minimal() Figure E.2: Result of k-means clustering of pixels based on colour for k=2. Segmentation of the image with k=2 separates air-spaces from all other objects (figure E.2). Therefore, the difference in pixel colour between the air-spaces and other objects accounts for most of the variance in the data-set (image). Let’s now take a look at a segmentation of the image using k=4. clusterColours &lt;- rgb(res[[4]]$centers) ggplot(data = imgDF, aes(x = x, y = y)) + geom_point(colour = clusterColours[res[[4]]$cluster]) + xlab(&quot;x&quot;) + ylab(&quot;y&quot;) + theme_minimal() Figure E.3: Result of k-means clustering of pixels based on colour for k=4. K-means clustering with k=4 rapidly and effectively segments the image of the histological section into the biological objects we can see by eye. A manual segmentation of the same image would be very laborious. This exercise highlights the importance of using biological insight to choose a sensible value of k. N.B. the cluster centres provide the mean pixel intensities for the red, green and blue channels and we have used this information to colour the pixels belonging to each cluster (figure E.3). "],
["solutions-nearest-neighbours.html", "F Solutions ch. 7 - Nearest neighbours F.1 Exercise 1", " F Solutions ch. 7 - Nearest neighbours Solutions to exercises of chapter 7. F.1 Exercise 1 Load libraries library(caret) ## Loading required package: lattice ## Loading required package: ggplot2 library(RColorBrewer) library(doMC) ## Loading required package: foreach ## Loading required package: iterators ## Loading required package: parallel library(corrplot) ## corrplot 0.84 loaded Prepare for parallel processing registerDoMC(detectCores()) Load data load(&quot;data/wheat_seeds/wheat_seeds.Rda&quot;) Partition data set.seed(42) trainIndex &lt;- createDataPartition(y=variety, times=1, p=0.7, list=F) varietyTrain &lt;- variety[trainIndex] morphTrain &lt;- morphometrics[trainIndex,] varietyTest &lt;- variety[-trainIndex] morphTest &lt;- morphometrics[-trainIndex,] summary(varietyTrain) ## Canadian Kama Rosa ## 49 49 49 summary(varietyTest) ## Canadian Kama Rosa ## 21 21 21 Data check: zero and near-zero predictors nzv &lt;- nearZeroVar(morphTrain, saveMetrics=T) nzv ## freqRatio percentUnique zeroVar nzv ## area 1.5 93.87755 FALSE FALSE ## perimeter 1.0 85.03401 FALSE FALSE ## compactness 1.0 93.19728 FALSE FALSE ## kernLength 1.5 91.83673 FALSE FALSE ## kernWidth 1.5 91.15646 FALSE FALSE ## asymCoef 1.0 98.63946 FALSE FALSE ## grooveLength 1.0 77.55102 FALSE FALSE Data check: are all predictors on same scale? summary(morphTrain) ## area perimeter compactness kernLength ## Min. :10.74 Min. :12.57 Min. :0.8081 Min. :4.902 ## 1st Qu.:12.28 1st Qu.:13.46 1st Qu.:0.8571 1st Qu.:5.253 ## Median :14.29 Median :14.28 Median :0.8735 Median :5.504 ## Mean :14.86 Mean :14.56 Mean :0.8712 Mean :5.632 ## 3rd Qu.:17.45 3rd Qu.:15.74 3rd Qu.:0.8880 3rd Qu.:5.979 ## Max. :21.18 Max. :17.25 Max. :0.9108 Max. :6.675 ## kernWidth asymCoef grooveLength ## Min. :2.630 Min. :0.7651 Min. :4.605 ## 1st Qu.:2.947 1st Qu.:2.5965 1st Qu.:5.028 ## Median :3.212 Median :3.5970 Median :5.222 ## Mean :3.258 Mean :3.6679 Mean :5.406 ## 3rd Qu.:3.563 3rd Qu.:4.6735 3rd Qu.:5.878 ## Max. :4.033 Max. :8.4560 Max. :6.550 featurePlot(x = morphTrain, y = varietyTrain, plot = &quot;box&quot;, ## Pass in options to bwplot() scales = list(y = list(relation=&quot;free&quot;), x = list(rot = 90)), layout = c(3,3)) Figure F.1: Boxplots of the 7 geometric parameters in the wheat data set Data check: pairwise correlations between predictors corMat &lt;- cor(morphTrain) corrplot(corMat, order=&quot;hclust&quot;, tl.cex=1) Figure F.2: Correlogram of the wheat seed data set. highCorr &lt;- findCorrelation(corMat, cutoff=0.75) length(highCorr) ## [1] 4 names(morphTrain)[highCorr] ## [1] &quot;area&quot; &quot;kernWidth&quot; &quot;perimeter&quot; &quot;kernLength&quot; Data check: skewness featurePlot(x = morphTrain, y = varietyTrain, plot = &quot;density&quot;, ## Pass in options to xyplot() to ## make it prettier scales = list(x = list(relation=&quot;free&quot;), y = list(relation=&quot;free&quot;)), adjust = 1.5, pch = &quot;|&quot;, layout = c(3, 3), auto.key = list(columns = 3)) Figure F.3: Density plots of the 7 geometric parameters in the wheat data set Create a ‘grid’ of values of k for evaluation: tuneParam &lt;- data.frame(k=seq(1,50,2)) Generate a list of seeds for reproducibility (optional) based on grid size set.seed(42) seeds &lt;- vector(mode = &quot;list&quot;, length = 101) for(i in 1:100) seeds[[i]] &lt;- sample.int(1000, length(tuneParam$k)) seeds[[101]] &lt;- sample.int(1000,1) Set training parameters. In the example in chapter 7 pre-processing was performed outside the cross-validation process to save time for the purposes of the demonstration. Here we have a relatively small data set, so we can do pre-processing within each iteration of the cross-validation process. We specify the option preProcOptions=list(cutoff=0.75) to set a value for the pairwise correlation coefficient cutoff. train_ctrl &lt;- trainControl(method=&quot;repeatedcv&quot;, number = 10, repeats = 10, preProcOptions=list(cutoff=0.75), seeds = seeds) Run training knnFit &lt;- train(morphTrain, varietyTrain, method=&quot;knn&quot;, preProcess = c(&quot;center&quot;, &quot;scale&quot;, &quot;corr&quot;), tuneGrid=tuneParam, trControl=train_ctrl) knnFit ## k-Nearest Neighbors ## ## 147 samples ## 7 predictors ## 3 classes: &#39;Canadian&#39;, &#39;Kama&#39;, &#39;Rosa&#39; ## ## Pre-processing: centered (3), scaled (3), remove (4) ## Resampling: Cross-Validated (10 fold, repeated 10 times) ## Summary of sample sizes: 133, 132, 132, 132, 132, 132, ... ## Resampling results across tuning parameters: ## ## k Accuracy Kappa ## 1 0.8429963 0.7644190 ## 3 0.9060916 0.8591664 ## 5 0.8809414 0.8214171 ## 7 0.8764249 0.8145913 ## 9 0.8840989 0.8260932 ## 11 0.8900989 0.8350932 ## 13 0.8974799 0.8461701 ## 15 0.8981465 0.8471701 ## 17 0.8981465 0.8471701 ## 19 0.8941465 0.8411868 ## 21 0.8955751 0.8433490 ## 23 0.8934322 0.8400932 ## 25 0.8920989 0.8381099 ## 27 0.8921465 0.8381868 ## 29 0.8928132 0.8391868 ## 31 0.8907656 0.8360598 ## 33 0.8893370 0.8339060 ## 35 0.8819560 0.8228372 ## 37 0.8813370 0.8219221 ## 39 0.8853370 0.8279221 ## 41 0.8880513 0.8319908 ## 43 0.8893846 0.8339908 ## 45 0.8921465 0.8381614 ## 47 0.8934799 0.8401614 ## 49 0.8920513 0.8379992 ## ## Accuracy was used to select the optimal model using the largest value. ## The final value used for the model was k = 3. Plot cross validation accuracy as a function of k plot(knnFit) Figure F.4: Accuracy (repeated cross-validation) as a function of neighbourhood size for the wheat seeds data set. Predict the class (wheat variety) of the observations in the test set. test_pred &lt;- predict(knnFit, morphTest) confusionMatrix(test_pred, varietyTest) ## Confusion Matrix and Statistics ## ## Reference ## Prediction Canadian Kama Rosa ## Canadian 18 4 0 ## Kama 3 16 2 ## Rosa 0 1 19 ## ## Overall Statistics ## ## Accuracy : 0.8413 ## 95% CI : (0.7274, 0.9212) ## No Information Rate : 0.3333 ## P-Value [Acc &gt; NIR] : &lt; 2.2e-16 ## ## Kappa : 0.7619 ## Mcnemar&#39;s Test P-Value : NA ## ## Statistics by Class: ## ## Class: Canadian Class: Kama Class: Rosa ## Sensitivity 0.8571 0.7619 0.9048 ## Specificity 0.9048 0.8810 0.9762 ## Pos Pred Value 0.8182 0.7619 0.9500 ## Neg Pred Value 0.9268 0.8810 0.9535 ## Prevalence 0.3333 0.3333 0.3333 ## Detection Rate 0.2857 0.2540 0.3016 ## Detection Prevalence 0.3492 0.3333 0.3175 ## Balanced Accuracy 0.8810 0.8214 0.9405 "],
["solutions-svm.html", "G Solutions ch. 8 - Support vector machines G.1 Exercise 1", " G Solutions ch. 8 - Support vector machines Solutions to exercises of chapter 8. G.1 Exercise 1 Load required libraries library(caret) ## Loading required package: lattice ## Loading required package: ggplot2 library(doMC) ## Loading required package: foreach ## Loading required package: iterators ## Loading required package: parallel library(pROC) ## Type &#39;citation(&quot;pROC&quot;)&#39; for a citation. ## ## Attaching package: &#39;pROC&#39; ## The following objects are masked from &#39;package:stats&#39;: ## ## cov, smooth, var library(e1071) Define a radial SVM using the e1071 library svmRadialE1071 &lt;- list( label = &quot;Support Vector Machines with Radial Kernel - e1071&quot;, library = &quot;e1071&quot;, type = c(&quot;Regression&quot;, &quot;Classification&quot;), parameters = data.frame(parameter=&quot;cost&quot;, class=&quot;numeric&quot;, label=&quot;Cost&quot;), grid = function (x, y, len = NULL, search = &quot;grid&quot;) { if (search == &quot;grid&quot;) { out &lt;- expand.grid(cost = 2^((1:len) - 3)) } else { out &lt;- data.frame(cost = 2^runif(len, min = -5, max = 10)) } out }, loop=NULL, fit=function (x, y, wts, param, lev, last, classProbs, ...) { if (any(names(list(...)) == &quot;probability&quot;) | is.numeric(y)) { out &lt;- e1071::svm(x = as.matrix(x), y = y, kernel = &quot;radial&quot;, cost = param$cost, ...) } else { out &lt;- e1071::svm(x = as.matrix(x), y = y, kernel = &quot;radial&quot;, cost = param$cost, probability = classProbs, ...) } out }, predict = function (modelFit, newdata, submodels = NULL) { predict(modelFit, newdata) }, prob = function (modelFit, newdata, submodels = NULL) { out &lt;- predict(modelFit, newdata, probability = TRUE) attr(out, &quot;probabilities&quot;) }, predictors = function (x, ...) { out &lt;- if (!is.null(x$terms)) predictors.terms(x$terms) else x$xNames if (is.null(out)) out &lt;- names(attr(x, &quot;scaling&quot;)$x.scale$`scaled:center`) if (is.null(out)) out &lt;- NA out }, tags = c(&quot;Kernel Methods&quot;, &quot;Support Vector Machines&quot;, &quot;Regression&quot;, &quot;Classifier&quot;, &quot;Robust Methods&quot;), levels = function(x) x$levels, sort = function(x) { x[order(x$cost), ] } ) Setup parallel processing registerDoMC(detectCores()) getDoParWorkers() ## [1] 8 Load data data(segmentationData) segClass &lt;- segmentationData$Class Extract predictors from segmentationData segData &lt;- segmentationData[,4:59] Partition data set.seed(42) trainIndex &lt;- createDataPartition(y=segClass, times=1, p=0.5, list=F) segDataTrain &lt;- segData[trainIndex,] segDataTest &lt;- segData[-trainIndex,] segClassTrain &lt;- segClass[trainIndex] segClassTest &lt;- segClass[-trainIndex] Set seeds for reproducibility (optional). We will be trying 9 values of the tuning parameter with 5 repeats of 10 fold cross-validation, so we need the following list of seeds. set.seed(42) seeds &lt;- vector(mode = &quot;list&quot;, length = 51) for(i in 1:50) seeds[[i]] &lt;- sample.int(1000, 9) seeds[[51]] &lt;- sample.int(1000,1) We will pass the twoClassSummary function into model training through trainControl. Additionally we would like the model to predict class probabilities so that we can calculate the ROC curve, so we use the classProbs option. cvCtrl &lt;- trainControl(method = &quot;repeatedcv&quot;, repeats = 5, number = 10, summaryFunction = twoClassSummary, classProbs = TRUE, seeds=seeds) Tune SVM over the cost parameter. The default grid of cost parameters start at 0.25 and double at each iteration. Choosing tuneLength = 9 will give us cost parameters of 0.25, 0.5, 1, 2, 4, 8, 16, 32 and 64. The train function will calculate an appropriate value of sigma (the kernel parameter) from the data. svmTune &lt;- train(x = segDataTrain, y = segClassTrain, method = svmRadialE1071, tuneLength = 9, preProc = c(&quot;center&quot;, &quot;scale&quot;), metric = &quot;ROC&quot;, trControl = cvCtrl) svmTune ## Support Vector Machines with Radial Kernel - e1071 ## ## 1010 samples ## 56 predictors ## 2 classes: &#39;PS&#39;, &#39;WS&#39; ## ## Pre-processing: centered (56), scaled (56) ## Resampling: Cross-Validated (10 fold, repeated 5 times) ## Summary of sample sizes: 909, 909, 909, 909, 909, 909, ... ## Resampling results across tuning parameters: ## ## cost ROC Sens Spec ## 0.25 0.8822479 0.8744615 0.6788889 ## 0.50 0.8889402 0.8704615 0.7111111 ## 1.00 0.8920256 0.8729231 0.7283333 ## 2.00 0.8908291 0.8630769 0.7494444 ## 4.00 0.8856239 0.8566154 0.7494444 ## 8.00 0.8761282 0.8443077 0.7422222 ## 16.00 0.8627265 0.8372308 0.7200000 ## 32.00 0.8530769 0.8415385 0.6988889 ## 64.00 0.8493846 0.8406154 0.6916667 ## ## ROC was used to select the optimal model using the largest value. ## The final value used for the model was cost = 1. svmTune$finalModel ## ## Call: ## svm.default(x = as.matrix(x), y = y, kernel = &quot;radial&quot;, cost = param$cost, ## probability = classProbs) ## ## ## Parameters: ## SVM-Type: C-classification ## SVM-Kernel: radial ## cost: 1 ## gamma: 0.01785714 ## ## Number of Support Vectors: 531 SVM accuracy profile plot(svmTune, metric = &quot;ROC&quot;, scales = list(x = list(log =2))) Figure G.1: SVM accuracy profile. Test set results #segDataTest &lt;- predict(transformations, segDataTest) svmPred &lt;- predict(svmTune, segDataTest) confusionMatrix(svmPred, segClassTest) ## Confusion Matrix and Statistics ## ## Reference ## Prediction PS WS ## PS 571 103 ## WS 79 256 ## ## Accuracy : 0.8196 ## 95% CI : (0.7945, 0.8429) ## No Information Rate : 0.6442 ## P-Value [Acc &gt; NIR] : &lt; 2e-16 ## ## Kappa : 0.6005 ## Mcnemar&#39;s Test P-Value : 0.08822 ## ## Sensitivity : 0.8785 ## Specificity : 0.7131 ## Pos Pred Value : 0.8472 ## Neg Pred Value : 0.7642 ## Prevalence : 0.6442 ## Detection Rate : 0.5659 ## Detection Prevalence : 0.6680 ## Balanced Accuracy : 0.7958 ## ## &#39;Positive&#39; Class : PS ## Get predicted class probabilities svmProbs &lt;- predict(svmTune, segDataTest, type=&quot;prob&quot;) head(svmProbs) ## PS WS ## 3 0.2304335 0.76956646 ## 5 0.9334686 0.06653138 ## 9 0.7495523 0.25044774 ## 10 0.8312666 0.16873341 ## 13 0.9445697 0.05543032 ## 14 0.7674554 0.23254457 Build a ROC curve svmROC &lt;- roc(segClassTest, svmProbs[,&quot;PS&quot;]) auc(svmROC) ## Area under the curve: 0.8864 Plot ROC curve. plot(svmROC, type = &quot;S&quot;, print.thres = 0.5, print.thres.col = &quot;blue&quot;, print.thres.pch = 19, print.thres.cex=1.5) Figure G.2: SVM ROC curve for cell segmentation data set. Calculate area under ROC curve auc(svmROC) ## Area under the curve: 0.8864 "],
["solutions-decision-trees.html", "H Solutions ch. 9 - Decision trees and random forests H.1 Exercise 1", " H Solutions ch. 9 - Decision trees and random forests Solutions to exercises of chapter 9. H.1 Exercise 1 Load the necessary packages readr to read in the data dplyr to process data party and rpart for the classification tree algorithms library(readr) library(dplyr) ## ## Attaching package: &#39;dplyr&#39; ## The following objects are masked from &#39;package:stats&#39;: ## ## filter, lag ## The following objects are masked from &#39;package:base&#39;: ## ## intersect, setdiff, setequal, union library(party) ## Loading required package: grid ## Loading required package: mvtnorm ## Loading required package: modeltools ## Loading required package: stats4 ## Loading required package: strucchange ## Loading required package: zoo ## ## Attaching package: &#39;zoo&#39; ## The following objects are masked from &#39;package:base&#39;: ## ## as.Date, as.Date.numeric ## Loading required package: sandwich library(rpart) library(rpart.plot) library(ROCR) ## Loading required package: gplots ## ## Attaching package: &#39;gplots&#39; ## The following object is masked from &#39;package:stats&#39;: ## ## lowess set.seed(100) Select features that may explain survival Each row in the data is a passenger. Columns are features: survived: 0 if died, 1 if survived embarked: Port of Embarkation (Cherbourg, Queenstown,Southampton) sex: Gender sibsp: Number of Siblings/Spouses Aboard parch: Number of Parents/Children Aboard fare: Fare Payed Make categorical features should be made into factors titanic3 &lt;- &quot;https://goo.gl/At238b&quot; %&gt;% read_csv %&gt;% # read in the data select(survived, embarked, sex, sibsp, parch, fare) %&gt;% mutate(embarked = factor(embarked), sex = factor(sex)) ## Parsed with column specification: ## cols( ## pclass = col_character(), ## survived = col_integer(), ## name = col_character(), ## sex = col_character(), ## age = col_double(), ## sibsp = col_integer(), ## parch = col_integer(), ## ticket = col_character(), ## fare = col_double(), ## cabin = col_character(), ## embarked = col_character(), ## boat = col_character(), ## body = col_integer(), ## home.dest = col_character() ## ) #load(&quot;/Users/robertness/Downloads/titanic.Rdata&quot;) Split data into training and test sets .data &lt;- c(&quot;training&quot;, &quot;test&quot;) %&gt;% sample(nrow(titanic3), replace = T) %&gt;% split(titanic3, .) Recursive partitioning is implemented in “rpart” package rtree_fit &lt;- rpart(survived ~ ., .data$training) rpart.plot(rtree_fit) ## Warning: Bad &#39;data&#39; field in model &#39;call&#39; (expected a data.frame or a matrix). ## To silence this warning: ## Call rpart.plot with roundint=FALSE, ## or rebuild the rpart model with model=TRUE. Conditional partitioning is implemented in the “ctree” method tree_fit &lt;- ctree(survived ~ ., data = .data$training) plot(tree_fit) Use ROCR package to visualize ROC Curve and compare methods tree_roc &lt;- tree_fit %&gt;% predict(newdata = .data$test) %&gt;% prediction(.data$test$survived) %&gt;% performance(&quot;tpr&quot;, &quot;fpr&quot;) plot(tree_roc) Acknowledgement: the code for this excersise is from http://bit.ly/2fqWKvK "],
["solutions-ann.html", "I Solutions ch. 10 - Artificial neural networks I.1 Exercise 1", " I Solutions ch. 10 - Artificial neural networks Solutions to exercises of chapter 10. I.1 Exercise 1 library(&quot;neuralnet&quot;) #To create a neural network to perform square root #Generate 50 random numbers uniformly distributed between 0 and 100 #And store them as a dataframe traininginput &lt;- as.data.frame(runif(50, min=0, max=100)) trainingoutput &lt;- sqrt(traininginput) #Column bind the data into one variable trainingdata &lt;- cbind(traininginput,trainingoutput) colnames(trainingdata) &lt;- c(&quot;Input&quot;,&quot;Output&quot;) #Train the neural network #Will have 10 hidden layers #Threshold is a numeric value specifying the threshold for the partial #derivatives of the error function as stopping criteria. net.sqrt &lt;- neuralnet(Output~Input,trainingdata, hidden=10, threshold=0.01) print(net.sqrt) ## $call ## neuralnet(formula = Output ~ Input, data = trainingdata, hidden = 10, ## threshold = 0.01) ## ## $response ## Output ## 1 5.519227239 ## 2 7.902907242 ## 3 7.494675786 ## 4 9.646949579 ## 5 6.579184589 ## 6 4.582635404 ## 7 1.777047916 ## 8 6.882721190 ## 9 8.519532611 ## 10 8.550083802 ## 11 8.925858724 ## 12 5.758551186 ## 13 2.751831781 ## 14 4.532574265 ## 15 7.319822149 ## 16 9.230921316 ## 17 9.266570062 ## 18 6.689722959 ## 19 8.184185473 ## 20 3.202338849 ## 21 6.225150465 ## 22 4.912382788 ## 23 8.220251152 ## 24 9.037634872 ## 25 8.678084647 ## 26 6.617461345 ## 27 8.059943112 ## 28 6.600743688 ## 29 6.815436656 ## 30 8.735389548 ## 31 8.453906284 ## 32 8.761491596 ## 33 9.588735514 ## 34 7.422184235 ## 35 7.834412718 ## 36 5.272249390 ## 37 5.898309070 ## 38 9.158202848 ## 39 6.761068055 ## 40 4.782685313 ## 41 9.702739429 ## 42 8.859197856 ## 43 1.470391560 ## 44 3.636398481 ## 45 9.314476130 ## 46 5.423897050 ## 47 2.673099228 ## 48 7.711626079 ## 49 5.935216632 ## 50 7.646074653 ## ## $covariate ## [,1] ## [1,] 30.461869319 ## [2,] 62.455942878 ## [3,] 56.170165143 ## [4,] 93.063636171 ## [5,] 43.285669852 ## [6,] 21.000547241 ## [7,] 3.157899296 ## [8,] 47.371850978 ## [9,] 72.582435911 ## [10,] 73.103933013 ## [11,] 79.670953960 ## [12,] 33.160911757 ## [13,] 7.572578150 ## [14,] 20.544229471 ## [15,] 53.579796292 ## [16,] 85.209908336 ## [17,] 85.869320715 ## [18,] 44.752393267 ## [19,] 66.980891861 ## [20,] 10.254974104 ## [21,] 38.752498315 ## [22,] 24.131504656 ## [23,] 67.572529009 ## [24,] 81.678844080 ## [25,] 75.309153134 ## [26,] 43.790794653 ## [27,] 64.962682966 ## [28,] 43.569817231 ## [29,] 46.450176812 ## [30,] 76.307030558 ## [31,] 71.468531457 ## [32,] 76.763734990 ## [33,] 91.943848762 ## [34,] 55.088818818 ## [35,] 61.378022633 ## [36,] 27.796613635 ## [37,] 34.790049889 ## [38,] 83.872679411 ## [39,] 45.712041249 ## [40,] 22.874078806 ## [41,] 94.143152423 ## [42,] 78.485386656 ## [43,] 2.162051341 ## [44,] 13.223393913 ## [45,] 86.759465584 ## [46,] 29.418659210 ## [47,] 7.145459484 ## [48,] 59.469176782 ## [49,] 35.226796474 ## [50,] 58.462457592 ## ## $model.list ## $model.list$response ## [1] &quot;Output&quot; ## ## $model.list$variables ## [1] &quot;Input&quot; ## ## ## $err.fct ## function (x, y) ## { ## 1/2 * (y - x)^2 ## } ## &lt;bytecode: 0x20a8170&gt; ## &lt;environment: 0x20a9478&gt; ## attr(,&quot;type&quot;) ## [1] &quot;sse&quot; ## ## $act.fct ## function (x) ## { ## 1/(1 + exp(-x)) ## } ## &lt;bytecode: 0x20b84c8&gt; ## &lt;environment: 0x20a9478&gt; ## attr(,&quot;type&quot;) ## [1] &quot;logistic&quot; ## ## $linear.output ## [1] TRUE ## ## $data ## Input Output ## 1 30.461869319 5.519227239 ## 2 62.455942878 7.902907242 ## 3 56.170165143 7.494675786 ## 4 93.063636171 9.646949579 ## 5 43.285669852 6.579184589 ## 6 21.000547241 4.582635404 ## 7 3.157899296 1.777047916 ## 8 47.371850978 6.882721190 ## 9 72.582435911 8.519532611 ## 10 73.103933013 8.550083802 ## 11 79.670953960 8.925858724 ## 12 33.160911757 5.758551186 ## 13 7.572578150 2.751831781 ## 14 20.544229471 4.532574265 ## 15 53.579796292 7.319822149 ## 16 85.209908336 9.230921316 ## 17 85.869320715 9.266570062 ## 18 44.752393267 6.689722959 ## 19 66.980891861 8.184185473 ## 20 10.254974104 3.202338849 ## 21 38.752498315 6.225150465 ## 22 24.131504656 4.912382788 ## 23 67.572529009 8.220251152 ## 24 81.678844080 9.037634872 ## 25 75.309153134 8.678084647 ## 26 43.790794653 6.617461345 ## 27 64.962682966 8.059943112 ## 28 43.569817231 6.600743688 ## 29 46.450176812 6.815436656 ## 30 76.307030558 8.735389548 ## 31 71.468531457 8.453906284 ## 32 76.763734990 8.761491596 ## 33 91.943848762 9.588735514 ## 34 55.088818818 7.422184235 ## 35 61.378022633 7.834412718 ## 36 27.796613635 5.272249390 ## 37 34.790049889 5.898309070 ## 38 83.872679411 9.158202848 ## 39 45.712041249 6.761068055 ## 40 22.874078806 4.782685313 ## 41 94.143152423 9.702739429 ## 42 78.485386656 8.859197856 ## 43 2.162051341 1.470391560 ## 44 13.223393913 3.636398481 ## 45 86.759465584 9.314476130 ## 46 29.418659210 5.423897050 ## 47 7.145459484 2.673099228 ## 48 59.469176782 7.711626079 ## 49 35.226796474 5.935216632 ## 50 58.462457592 7.646074653 ## ## $net.result ## $net.result[[1]] ## [,1] ## 1 5.517169398 ## 2 7.899573434 ## 3 7.493741496 ## 4 9.641570856 ## 5 6.581765992 ## 6 4.583214348 ## 7 1.777910876 ## 8 6.885163582 ## 9 8.517989976 ## 10 8.548846798 ## 11 8.929065970 ## 12 5.757436258 ## 13 2.750873399 ## 14 4.533490638 ## 15 7.320117610 ## 16 9.235886561 ## 17 9.271398693 ## 18 6.692393931 ## 19 8.180608483 ## 20 3.202105919 ## 21 6.226534439 ## 22 4.910962181 ## 23 8.216767394 ## 24 9.041935875 ## 25 8.678284493 ## 26 6.620091375 ## 27 8.056272299 ## 28 6.603354730 ## 29 6.818014959 ## 30 8.736289119 ## 31 8.451767666 ## 32 8.762715272 ## 33 9.586241401 ## 34 7.421766922 ## 35 7.831359571 ## 36 5.269870092 ## 37 5.897923979 ## 38 9.163163806 ## 39 6.763712266 ## 40 4.781970104 ## 41 9.694023815 ## 42 8.861628161 ## 43 1.469962493 ## 44 3.638959940 ## 45 9.318954236 ## 46 5.421624685 ## 47 2.672485693 ## 48 7.709231672 ## 49 5.935035256 ## 50 7.644093624 ## ## ## $weights ## $weights[[1]] ## $weights[[1]][[1]] ## [,1] [,2] [,3] [,4] ## [1,] -1.0143442592 -0.3108134549 4.48147360541 -0.6803028026 ## [2,] -0.5251534515 0.1793417287 -0.05084709319 0.0428372541 ## [,5] [,6] [,7] [,8] ## [1,] 2.03882035742 0.43572772952 0.31855674101 0.13532461807 ## [2,] -0.04266953073 0.04422025402 0.04759056023 0.04796222314 ## [,9] [,10] ## [1,] -0.43644174351 1.53363355573 ## [2,] -0.03847716565 0.03463005836 ## ## $weights[[1]][[2]] ## [,1] ## [1,] 0.9323227234 ## [2,] -2.9985588758 ## [3,] 1.8218821789 ## [4,] -2.7774092409 ## [5,] 3.5400471485 ## [6,] -1.2357288637 ## [7,] 1.2339769529 ## [8,] 1.6586583299 ## [9,] 1.2611211125 ## [10,] -1.7699072451 ## [11,] 0.7682671344 ## ## ## ## $startweights ## $startweights[[1]] ## $startweights[[1]][[1]] ## [,1] [,2] [,3] [,4] [,5] ## [1,] -1.0151710299 -0.5342538799 1.217319457 -0.3785569589 0.5730526014 ## [2,] -0.8857900817 2.2311265794 -1.094952553 -0.0405489265 -0.3162993204 ## [,6] [,7] [,8] [,9] [,10] ## [1,] 1.3251399953 2.4838023968 0.8135800074 -1.657374847 1.9515712911 ## [2,] 0.4585721091 -0.1081072498 -0.1057220534 -1.161110638 0.4302669937 ## ## $startweights[[1]][[2]] ## [,1] ## [1,] -0.179535625899 ## [2,] 1.421304912112 ## [3,] 0.710020721636 ## [4,] -1.330598721355 ## [5,] 0.701840030541 ## [6,] -0.215225888872 ## [7,] -0.030688419065 ## [8,] 0.546797615920 ## [9,] -0.004226540943 ## [10,] -0.555436985997 ## [11,] -0.040842801160 ## ## ## ## $generalized.weights ## $generalized.weights[[1]] ## [,1] ## 1 -0.0036452136764 ## 2 -0.0011566625080 ## 3 -0.0013611829827 ## 4 -0.0005880788349 ## 5 -0.0020717546346 ## 6 -0.0065989122131 ## 7 -0.2053310243372 ## 8 -0.0017884073486 ## 9 -0.0009253764487 ## 10 -0.0009154852311 ## 11 -0.0008000234895 ## 12 -0.0031854391135 ## 13 -0.0375912464845 ## 14 -0.0068400650122 ## 15 -0.0014664847729 ## 16 -0.0007103415454 ## 17 -0.0006999048278 ## 18 -0.0019621287100 ## 19 -0.0010422839336 ## 20 -0.0222724885184 ## 21 -0.0024801316225 ## 22 -0.0052737797123 ## 23 -0.0010288359109 ## 24 -0.0007670035668 ## 25 -0.0008750398564 ## 26 -0.0020329117029 ## 27 -0.0010905996196 ## 28 -0.0020497595097 ## 29 -0.0018465417144 ## 30 -0.0008573725047 ## 31 -0.0009469857961 ## 32 -0.0008493996326 ## 33 -0.0006052509646 ## 34 -0.0014033963232 ## 35 -0.0011874502495 ## 36 -0.0042133725530 ## 37 -0.0029505615258 ## 38 -0.0007316310210 ## 39 -0.0018953883305 ## 40 -0.0057460875669 ## 41 -0.0005716168802 ## 42 -0.0008198997544 ## 43 -0.4881119168066 ## 44 -0.0143921626508 ## 45 -0.0006858731315 ## 46 -0.0038519115348 ## 47 -0.0416313937285 ## 48 -0.0012459860555 ## 49 -0.0028921774440 ## 50 -0.0012791282179 ## ## ## $result.matrix ## 1 ## error 0.000198491342 ## reached.threshold 0.009766732701 ## steps 18838.000000000000 ## Intercept.to.1layhid1 -1.014344259179 ## Input.to.1layhid1 -0.525153451492 ## Intercept.to.1layhid2 -0.310813454855 ## Input.to.1layhid2 0.179341728698 ## Intercept.to.1layhid3 4.481473605407 ## Input.to.1layhid3 -0.050847093193 ## Intercept.to.1layhid4 -0.680302802642 ## Input.to.1layhid4 0.042837254098 ## Intercept.to.1layhid5 2.038820357423 ## Input.to.1layhid5 -0.042669530733 ## Intercept.to.1layhid6 0.435727729516 ## Input.to.1layhid6 0.044220254019 ## Intercept.to.1layhid7 0.318556741011 ## Input.to.1layhid7 0.047590560226 ## Intercept.to.1layhid8 0.135324618069 ## Input.to.1layhid8 0.047962223137 ## Intercept.to.1layhid9 -0.436441743513 ## Input.to.1layhid9 -0.038477165647 ## Intercept.to.1layhid10 1.533633555732 ## Input.to.1layhid10 0.034630058360 ## Intercept.to.Output 0.932322723379 ## 1layhid.1.to.Output -2.998558875821 ## 1layhid.2.to.Output 1.821882178934 ## 1layhid.3.to.Output -2.777409240869 ## 1layhid.4.to.Output 3.540047148500 ## 1layhid.5.to.Output -1.235728863749 ## 1layhid.6.to.Output 1.233976952915 ## 1layhid.7.to.Output 1.658658329933 ## 1layhid.8.to.Output 1.261121112522 ## 1layhid.9.to.Output -1.769907245101 ## 1layhid.10.to.Output 0.768267134439 ## ## attr(,&quot;class&quot;) ## [1] &quot;nn&quot; #Plot the neural network plot(net.sqrt) #Test the neural network on some training data testdata &lt;- as.data.frame((1:10)^2) #Generate some squared numbers net.results &lt;- compute(net.sqrt, testdata) #Run them through the neural network #See what properties net.sqrt has ls(net.results) ## [1] &quot;net.result&quot; &quot;neurons&quot; #see the results print(net.results$net.result) ## [,1] ## [1,] 1.032496298 ## [2,] 2.002426250 ## [3,] 2.998899895 ## [4,] 4.003370235 ## [5,] 4.998197260 ## [6,] 6.000180311 ## [7,] 7.002066164 ## [8,] 7.996402383 ## [9,] 9.003971910 ## [10,] 9.962105141 #Display a better version of the results cleanoutput &lt;- cbind(testdata,sqrt(testdata), as.data.frame(net.results$net.result)) colnames(cleanoutput) &lt;- c(&quot;Input&quot;,&quot;Expected Output&quot;,&quot;Neural Net Output&quot;) print(cleanoutput) ## Input Expected Output Neural Net Output ## 1 1 1 1.032496298 ## 2 4 2 2.002426250 ## 3 9 3 2.998899895 ## 4 16 4 4.003370235 ## 5 25 5 4.998197260 ## 6 36 6 6.000180311 ## 7 49 7 7.002066164 ## 8 64 8 7.996402383 ## 9 81 9 9.003971910 ## 10 100 10 9.962105141 Acknowledgement: this example excercise was from http://gekkoquant.com/2012/05/26/neural-networks-with-r-simple-example/ "],
["references.html", "References", " References "]
]
